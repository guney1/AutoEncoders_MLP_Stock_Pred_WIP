{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from functools import partial\n",
    "import talib\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/8f/0fc2384666f7b67a8821c48ae0a7fdd41515e1c938332b2f0848bbad0225/scikit_learn-0.21.1-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (10.2MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2MB 3.1MB/s eta 0:00:01    |█████████▉                      | 3.1MB 1.0MB/s eta 0:00:08\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from scikit-learn->sklearn) (1.14.6)\n",
      "Collecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "  Using cached https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl\n",
      "Collecting scipy>=0.17.0 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/c1/901d7b80bf7793352fd0b2e6a8688d44bf2cba8d8772b8ca25f71f129f77/scipy-1.3.0-cp35-cp35m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (27.6MB)\n",
      "\u001b[K     |████████████████████████████████| 27.6MB 3.0MB/s eta 0:00:01     |██████████████████████████████  | 25.8MB 5.3MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: joblib, scipy, scikit-learn, sklearn\n",
      "Successfully installed joblib-0.13.2 scikit-learn-0.21.1 scipy-1.3.0 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Using cached https://files.pythonhosted.org/packages/63/12/704382c3081df3ae3f9d96fe6afb62efa2fa9749be20c301cd2797fb0b52/hyperopt-0.1.2-py3-none-any.whl\n",
      "Collecting networkx (from hyperopt)\n",
      "Requirement already satisfied: numpy in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt) (1.14.6)\n",
      "Requirement already satisfied: scipy in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt) (1.3.0)\n",
      "Collecting tqdm (from hyperopt)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/af/685bf3ce889ea191f3b916557f5677cc95a5e87b2fa120d74b5dd6d049d0/tqdm-4.32.1-py2.py3-none-any.whl\n",
      "Collecting future (from hyperopt)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz\n",
      "Requirement already satisfied: six in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt) (1.11.0)\n",
      "Collecting pymongo (from hyperopt)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/97/bb3e1c761d60548dfa1adf9a2941883f1dd014362289ba0ccf852543f475/pymongo-3.8.0-cp35-cp35m-macosx_10_6_intel.whl (352kB)\n",
      "\u001b[K     |████████████████████████████████| 358kB 904kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from networkx->hyperopt) (4.4.0)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/guneykan/Library/Caches/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "Successfully built future\n",
      "Installing collected packages: networkx, tqdm, future, pymongo, hyperopt\n",
      "Successfully installed future-0.17.1 hyperopt-0.1.2 networkx-2.3 pymongo-3.8.0 tqdm-4.32.1\n",
      "Collecting hyperas\n",
      "  Using cached https://files.pythonhosted.org/packages/04/34/87ad6ffb42df9c1fa9c4c906f65813d42ad70d68c66af4ffff048c228cd4/hyperas-0.4.1-py3-none-any.whl\n",
      "Requirement already satisfied: entrypoints in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperas) (0.2.3)\n",
      "Requirement already satisfied: nbformat in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperas) (4.4.0)\n",
      "Requirement already satisfied: jupyter in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperas) (1.0.0)\n",
      "Requirement already satisfied: nbconvert in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperas) (5.5.0)\n",
      "Collecting keras (from hyperas)\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: hyperopt in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperas) (0.1.2)\n",
      "Requirement already satisfied: ipython_genutils in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbformat->hyperas) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbformat->hyperas) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbformat->hyperas) (2.6.0)\n",
      "Requirement already satisfied: jupyter_core in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbformat->hyperas) (4.4.0)\n",
      "Requirement already satisfied: qtconsole in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter->hyperas) (4.4.4)\n",
      "Requirement already satisfied: ipykernel in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter->hyperas) (4.10.0)\n",
      "Requirement already satisfied: ipywidgets in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter->hyperas) (7.4.1)\n",
      "Requirement already satisfied: jupyter-console in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter->hyperas) (5.2.0)\n",
      "Requirement already satisfied: notebook in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter->hyperas) (5.6.0)\n",
      "Requirement already satisfied: jinja2>=2.4 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (2.10)\n",
      "Requirement already satisfied: pygments in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (2.4.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (0.8.3)\n",
      "Requirement already satisfied: testpath in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (0.3.1)\n",
      "Requirement already satisfied: bleach in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (2.1.4)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (1.4.2)\n",
      "Requirement already satisfied: defusedxml in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from nbconvert->hyperas) (0.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (1.0.7)\n",
      "Requirement already satisfied: h5py in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (1.3.0)\n",
      "Collecting pyyaml (from keras->hyperas)\n",
      "  Using cached https://files.pythonhosted.org/packages/9f/2c/9417b5c774792634834e730932745bc09a7d36754ca00acf1ccd1ac2594d/PyYAML-5.1.tar.gz\n",
      "Requirement already satisfied: six>=1.9.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from keras->hyperas) (1.14.6)\n",
      "Requirement already satisfied: pymongo in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt->hyperas) (3.8.0)\n",
      "Requirement already satisfied: networkx in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt->hyperas) (2.3)\n",
      "Requirement already satisfied: tqdm in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt->hyperas) (4.32.1)\n",
      "Requirement already satisfied: future in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from hyperopt->hyperas) (0.17.1)\n",
      "Requirement already satisfied: decorator in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from traitlets>=4.1->nbformat->hyperas) (4.4.0)\n",
      "Requirement already satisfied: jupyter-client>=4.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from qtconsole->jupyter->hyperas) (5.2.3)\n",
      "Requirement already satisfied: ipython>=4.0.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipykernel->jupyter->hyperas) (6.5.0)\n",
      "Requirement already satisfied: tornado>=4.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipykernel->jupyter->hyperas) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipywidgets->jupyter->hyperas) (3.4.1)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter-console->jupyter->hyperas) (1.0.15)\n",
      "Requirement already satisfied: terminado>=0.8.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from notebook->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: prometheus-client in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from notebook->jupyter->hyperas) (0.3.1)\n",
      "Requirement already satisfied: Send2Trash in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from notebook->jupyter->hyperas) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from notebook->jupyter->hyperas) (17.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jinja2>=2.4->nbconvert->hyperas) (1.0)\n",
      "Requirement already satisfied: html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from bleach->nbconvert->hyperas) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jupyter-client>=4.1->qtconsole->jupyter->hyperas) (2.7.3)\n",
      "Requirement already satisfied: simplegeneric>0.8 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.8.1)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.1.0)\n",
      "Requirement already satisfied: jedi>=0.10 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.12.1)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (4.6.0)\n",
      "Requirement already satisfied: pickleshare in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.7.4)\n",
      "Requirement already satisfied: backcall in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from ipython>=4.0.0->ipykernel->jupyter->hyperas) (40.2.0)\n",
      "Requirement already satisfied: wcwidth in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from prompt_toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->hyperas) (0.1.7)\n",
      "Requirement already satisfied: webencodings in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from html5lib!=1.0b1,!=1.0b2,!=1.0b3,!=1.0b4,!=1.0b5,!=1.0b6,!=1.0b7,!=1.0b8,>=0.99999999pre->bleach->nbconvert->hyperas) (0.5.1)\n",
      "Requirement already satisfied: parso>=0.3.0 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from jedi>=0.10->ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.4.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptyprocess>=0.5 in ./anaconda3/envs/myenv/lib/python3.5/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0->ipykernel->jupyter->hyperas) (0.6.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/guneykan/Library/Caches/pip/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: pyyaml, keras, hyperas\n",
      "Successfully installed hyperas-0.4.1 keras-2.2.4 pyyaml-5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install hyperopt\n",
    "!pip install hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/e0/be401c003291b56efc55aeba6a80ab790d3d4cece2778288d65323009420/pip-19.1.1-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.4MB 3.1MB/s ta 0:00:011\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 10.0.1\n",
      "    Uninstalling pip-10.0.1:\n",
      "      Successfully uninstalled pip-10.0.1\n",
      "Successfully installed pip-19.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1970-12-31'\n",
    "end_date = '2019-04-12'\n",
    "df = web.DataReader('AAPL', 'yahoo', start_date, end_date)\n",
    "df = df.drop([\"Adj Close\"], axis=1)\n",
    "df[\"mid\"] = (df[\"High\"]+df[\"Low\"])/2\n",
    "df[\"return_log\"] = df[\"mid\"].pct_change(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mid</th>\n",
       "      <th>return_log</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>0.515625</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>0.513393</td>\n",
       "      <td>117258400.0</td>\n",
       "      <td>0.514509</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>0.488839</td>\n",
       "      <td>0.486607</td>\n",
       "      <td>43971200.0</td>\n",
       "      <td>0.487723</td>\n",
       "      <td>-0.052061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.450893</td>\n",
       "      <td>26432000.0</td>\n",
       "      <td>0.452009</td>\n",
       "      <td>-0.073227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>0.462054</td>\n",
       "      <td>21610400.0</td>\n",
       "      <td>0.463170</td>\n",
       "      <td>0.024691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.477679</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>0.475446</td>\n",
       "      <td>18362400.0</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>0.028916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-19</th>\n",
       "      <td>0.506696</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>0.504464</td>\n",
       "      <td>12157600.0</td>\n",
       "      <td>0.505580</td>\n",
       "      <td>0.060890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-22</th>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>9340800.0</td>\n",
       "      <td>0.530134</td>\n",
       "      <td>0.048565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-23</th>\n",
       "      <td>0.553571</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>0.551339</td>\n",
       "      <td>11737600.0</td>\n",
       "      <td>0.552455</td>\n",
       "      <td>0.042105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-24</th>\n",
       "      <td>0.582589</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>0.580357</td>\n",
       "      <td>12000800.0</td>\n",
       "      <td>0.581473</td>\n",
       "      <td>0.052525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-26</th>\n",
       "      <td>0.636161</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>0.633929</td>\n",
       "      <td>13893600.0</td>\n",
       "      <td>0.635045</td>\n",
       "      <td>0.092131</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open     Close       Volume       mid  \\\n",
       "Date                                                                        \n",
       "1980-12-12  0.515625  0.513393  0.513393  0.513393  117258400.0  0.514509   \n",
       "1980-12-15  0.488839  0.486607  0.488839  0.486607   43971200.0  0.487723   \n",
       "1980-12-16  0.453125  0.450893  0.453125  0.450893   26432000.0  0.452009   \n",
       "1980-12-17  0.464286  0.462054  0.462054  0.462054   21610400.0  0.463170   \n",
       "1980-12-18  0.477679  0.475446  0.475446  0.475446   18362400.0  0.476562   \n",
       "1980-12-19  0.506696  0.504464  0.504464  0.504464   12157600.0  0.505580   \n",
       "1980-12-22  0.531250  0.529018  0.529018  0.529018    9340800.0  0.530134   \n",
       "1980-12-23  0.553571  0.551339  0.551339  0.551339   11737600.0  0.552455   \n",
       "1980-12-24  0.582589  0.580357  0.580357  0.580357   12000800.0  0.581473   \n",
       "1980-12-26  0.636161  0.633929  0.633929  0.633929   13893600.0  0.635045   \n",
       "\n",
       "            return_log  \n",
       "Date                    \n",
       "1980-12-12         NaN  \n",
       "1980-12-15   -0.052061  \n",
       "1980-12-16   -0.073227  \n",
       "1980-12-17    0.024691  \n",
       "1980-12-18    0.028916  \n",
       "1980-12-19    0.060890  \n",
       "1980-12-22    0.048565  \n",
       "1980-12-23    0.042105  \n",
       "1980-12-24    0.052525  \n",
       "1980-12-26    0.092131  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smooth data with Double exponential moving average\n",
    "df[\"Close\"] = talib.DEMA(np.array(df[\"Close\"]), timeperiod=3)\n",
    "df[\"High\"] = talib.DEMA(np.array(df[\"High\"]), timeperiod=3)\n",
    "df[\"Low\"] = talib.DEMA(np.array(df[\"Low\"]), timeperiod=3)\n",
    "df[\"Open\"] = talib.DEMA(np.array(df[\"Open\"]), timeperiod=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"dema\"] = talib.DEMA(np.array(df[\"Close\"]), timeperiod=30) #Double Exponential Moving Average\n",
    "df[\"kama\"] = talib.KAMA(np.array(df[\"Close\"]), timeperiod=30) # Kaufman Adaptive Moving Average\n",
    "df[\"trima\"] = talib.TRIMA(np.array(df[\"Close\"]), timeperiod=30) # Triple exponential\n",
    "df[\"WMA\"] = talib.WMA(np.array(df[\"Close\"]), timeperiod=30) # Weighted moving average\n",
    "df[\"adx\"] = talib.ADX(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14) \n",
    "df[\"adxr\"] = talib.ADXR(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14) \n",
    "df[\"apo\"] = talib.APO(np.array(df[\"Close\"]), fastperiod=12, slowperiod=26)\n",
    "df[\"aroondown\"], df[\"aroonup\"] = talib.AROON(np.array(df[\"High\"]), np.array(df[\"Low\"]), timeperiod=14) \n",
    "df[\"aroonosc\"] = talib.AROONOSC(np.array(df[\"High\"]), np.array(df[\"Low\"]), timeperiod=14)\n",
    "df[\"bop\"] = talib.BOP(np.array(df[\"Open\"]), np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"])) \n",
    "df[\"cmo\"] = talib.CMO(np.array(df[\"Close\"]), timeperiod=14) \n",
    "df[\"dx\"] = talib.DX(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14) \n",
    "df[\"mfi\"] = talib.MFI(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), np.array(df[\"Volume\"]), timeperiod=14) \n",
    "df[\"minus_di\"] = talib.MINUS_DI(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14) \n",
    "df[\"minus_dm\"] = talib.MINUS_DM(np.array(df[\"High\"]), np.array(df[\"Low\"]), timeperiod=14)\n",
    "df[\"plus_di\"] = talib.PLUS_DI(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14)\n",
    "df[\"plus_dm\"] = talib.PLUS_DM(np.array(df[\"High\"]), np.array(df[\"Low\"]), timeperiod=14)\n",
    "df[\"ppo\"] = talib.PPO(np.array(df[\"Close\"]), fastperiod=10, slowperiod=20)\n",
    "df[\"rsi\"] = talib.RSI(np.array(df[\"Close\"]), timeperiod=14)\n",
    "df[\"slowk\"], df[\"slowd\"] = talib.STOCH(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), fastk_period=5, slowk_period=3, slowk_matype=0, slowd_period=3, slowd_matype=0)\n",
    "df[\"macd\"], df[\"macdsignal\"], df[\"macdhist\"] = talib.MACD(np.array(df[\"Close\"]), fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "df[\"cci\"] = talib.CCI(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14)\n",
    "df[\"mom20\"] = talib.MOM(np.array(df[\"Close\"]), timeperiod=20)\n",
    "df[\"mom10\"] = talib.MOM(np.array(df[\"Close\"]), timeperiod=10)\n",
    "df[\"ma20\"] = talib.SMA(np.array(df[\"Close\"]), timeperiod=20)\n",
    "df[\"ma10\"] = talib.SMA(np.array(df[\"Close\"]), timeperiod=10)\n",
    "df[\"roc\"] = talib.ROC(np.array(df[\"Close\"]), timeperiod=10)\n",
    "df[\"ult\"] = talib.ULTOSC(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "df[\"will\"] = talib.WILLR(np.array(df[\"High\"]), np.array(df[\"Low\"]), np.array(df[\"Close\"]), timeperiod=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"upperband\"], df[\"middleband\"], df[\"lowerband\"] = talib.BBANDS(np.array(df[\"Close\"]), timeperiod=5, nbdevup=2, nbdevdn=2, matype=0)\n",
    "df[\"hilbert\"] = talib.HT_TRENDLINE(np.array(df[\"Close\"]))\n",
    "\n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mid</th>\n",
       "      <th>return_log</th>\n",
       "      <th>dema</th>\n",
       "      <th>kama</th>\n",
       "      <th>trima</th>\n",
       "      <th>...</th>\n",
       "      <th>macdsignal</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>cci</th>\n",
       "      <th>mom20</th>\n",
       "      <th>mom10</th>\n",
       "      <th>ma20</th>\n",
       "      <th>ma10</th>\n",
       "      <th>roc</th>\n",
       "      <th>ult</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1980-12-12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>117258400.0</td>\n",
       "      <td>0.514509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43971200.0</td>\n",
       "      <td>0.487723</td>\n",
       "      <td>-0.052061</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26432000.0</td>\n",
       "      <td>0.452009</td>\n",
       "      <td>-0.073227</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21610400.0</td>\n",
       "      <td>0.463170</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-18</th>\n",
       "      <td>0.473648</td>\n",
       "      <td>0.471416</td>\n",
       "      <td>0.471292</td>\n",
       "      <td>0.471416</td>\n",
       "      <td>18362400.0</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>0.028916</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-19</th>\n",
       "      <td>0.497752</td>\n",
       "      <td>0.495520</td>\n",
       "      <td>0.495365</td>\n",
       "      <td>0.495520</td>\n",
       "      <td>12157600.0</td>\n",
       "      <td>0.505580</td>\n",
       "      <td>0.060890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-22</th>\n",
       "      <td>0.524430</td>\n",
       "      <td>0.522197</td>\n",
       "      <td>0.522073</td>\n",
       "      <td>0.522197</td>\n",
       "      <td>9340800.0</td>\n",
       "      <td>0.530134</td>\n",
       "      <td>0.048565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-23</th>\n",
       "      <td>0.549545</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>0.547228</td>\n",
       "      <td>0.547313</td>\n",
       "      <td>11737600.0</td>\n",
       "      <td>0.552455</td>\n",
       "      <td>0.042105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-24</th>\n",
       "      <td>0.578594</td>\n",
       "      <td>0.576362</td>\n",
       "      <td>0.576307</td>\n",
       "      <td>0.576362</td>\n",
       "      <td>12000800.0</td>\n",
       "      <td>0.581473</td>\n",
       "      <td>0.052525</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-26</th>\n",
       "      <td>0.627034</td>\n",
       "      <td>0.624801</td>\n",
       "      <td>0.624768</td>\n",
       "      <td>0.624801</td>\n",
       "      <td>13893600.0</td>\n",
       "      <td>0.635045</td>\n",
       "      <td>0.092131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-29</th>\n",
       "      <td>0.648122</td>\n",
       "      <td>0.645890</td>\n",
       "      <td>0.645870</td>\n",
       "      <td>0.645890</td>\n",
       "      <td>23290400.0</td>\n",
       "      <td>0.643973</td>\n",
       "      <td>0.014060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-30</th>\n",
       "      <td>0.640917</td>\n",
       "      <td>0.638685</td>\n",
       "      <td>0.640348</td>\n",
       "      <td>0.638685</td>\n",
       "      <td>17220000.0</td>\n",
       "      <td>0.628348</td>\n",
       "      <td>-0.024263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980-12-31</th>\n",
       "      <td>0.622860</td>\n",
       "      <td>0.620628</td>\n",
       "      <td>0.622853</td>\n",
       "      <td>0.620628</td>\n",
       "      <td>8937600.0</td>\n",
       "      <td>0.610491</td>\n",
       "      <td>-0.028419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>0.622229</td>\n",
       "      <td>0.618322</td>\n",
       "      <td>0.619016</td>\n",
       "      <td>0.618322</td>\n",
       "      <td>5415200.0</td>\n",
       "      <td>0.618304</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.576113</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>0.609929</td>\n",
       "      <td>0.607139</td>\n",
       "      <td>0.608950</td>\n",
       "      <td>0.607139</td>\n",
       "      <td>8932000.0</td>\n",
       "      <td>0.603795</td>\n",
       "      <td>-0.023466</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135723</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.589686</td>\n",
       "      <td>28.790395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-06</th>\n",
       "      <td>0.585510</td>\n",
       "      <td>0.583139</td>\n",
       "      <td>0.585335</td>\n",
       "      <td>0.583139</td>\n",
       "      <td>11289600.0</td>\n",
       "      <td>0.577009</td>\n",
       "      <td>-0.044362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.087618</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.598447</td>\n",
       "      <td>17.682086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-07</th>\n",
       "      <td>0.559144</td>\n",
       "      <td>0.556912</td>\n",
       "      <td>0.559213</td>\n",
       "      <td>0.556912</td>\n",
       "      <td>13921600.0</td>\n",
       "      <td>0.552455</td>\n",
       "      <td>-0.042553</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.601919</td>\n",
       "      <td>6.647757</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-08</th>\n",
       "      <td>0.542789</td>\n",
       "      <td>0.540592</td>\n",
       "      <td>0.542902</td>\n",
       "      <td>0.540592</td>\n",
       "      <td>9956800.0</td>\n",
       "      <td>0.541295</td>\n",
       "      <td>-0.020202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-50.810264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.006721</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.601247</td>\n",
       "      <td>-1.228067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-60.852742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-09</th>\n",
       "      <td>0.560369</td>\n",
       "      <td>0.558172</td>\n",
       "      <td>0.558790</td>\n",
       "      <td>0.558172</td>\n",
       "      <td>5376000.0</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-39.605208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.018190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.599428</td>\n",
       "      <td>-3.156036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-58.944450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-12</th>\n",
       "      <td>0.565855</td>\n",
       "      <td>0.561975</td>\n",
       "      <td>0.565364</td>\n",
       "      <td>0.561975</td>\n",
       "      <td>5924800.0</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-45.485436</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.062827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.593145</td>\n",
       "      <td>-10.055487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-68.411828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open     Close       Volume       mid  \\\n",
       "Date                                                                        \n",
       "1980-12-12       NaN       NaN       NaN       NaN  117258400.0  0.514509   \n",
       "1980-12-15       NaN       NaN       NaN       NaN   43971200.0  0.487723   \n",
       "1980-12-16       NaN       NaN       NaN       NaN   26432000.0  0.452009   \n",
       "1980-12-17       NaN       NaN       NaN       NaN   21610400.0  0.463170   \n",
       "1980-12-18  0.473648  0.471416  0.471292  0.471416   18362400.0  0.476562   \n",
       "1980-12-19  0.497752  0.495520  0.495365  0.495520   12157600.0  0.505580   \n",
       "1980-12-22  0.524430  0.522197  0.522073  0.522197    9340800.0  0.530134   \n",
       "1980-12-23  0.549545  0.547313  0.547228  0.547313   11737600.0  0.552455   \n",
       "1980-12-24  0.578594  0.576362  0.576307  0.576362   12000800.0  0.581473   \n",
       "1980-12-26  0.627034  0.624801  0.624768  0.624801   13893600.0  0.635045   \n",
       "1980-12-29  0.648122  0.645890  0.645870  0.645890   23290400.0  0.643973   \n",
       "1980-12-30  0.640917  0.638685  0.640348  0.638685   17220000.0  0.628348   \n",
       "1980-12-31  0.622860  0.620628  0.622853  0.620628    8937600.0  0.610491   \n",
       "1981-01-02  0.622229  0.618322  0.619016  0.618322    5415200.0  0.618304   \n",
       "1981-01-05  0.609929  0.607139  0.608950  0.607139    8932000.0  0.603795   \n",
       "1981-01-06  0.585510  0.583139  0.585335  0.583139   11289600.0  0.577009   \n",
       "1981-01-07  0.559144  0.556912  0.559213  0.556912   13921600.0  0.552455   \n",
       "1981-01-08  0.542789  0.540592  0.542902  0.540592    9956800.0  0.541295   \n",
       "1981-01-09  0.560369  0.558172  0.558790  0.558172    5376000.0  0.570312   \n",
       "1981-01-12  0.565855  0.561975  0.565364  0.561975    5924800.0  0.566964   \n",
       "\n",
       "            return_log  dema  kama  trima  ...  macdsignal  macdhist  \\\n",
       "Date                                       ...                         \n",
       "1980-12-12         NaN   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-15   -0.052061   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-16   -0.073227   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-17    0.024691   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-18    0.028916   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-19    0.060890   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-22    0.048565   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-23    0.042105   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-24    0.052525   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-26    0.092131   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-29    0.014060   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-30   -0.024263   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1980-12-31   -0.028419   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-02    0.012797   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-05   -0.023466   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-06   -0.044362   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-07   -0.042553   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-08   -0.020202   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-09    0.053608   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "1981-01-12   -0.005871   NaN   NaN    NaN  ...         NaN       NaN   \n",
       "\n",
       "                  cci  mom20     mom10  ma20      ma10        roc  ult  \\\n",
       "Date                                                                     \n",
       "1980-12-12        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-15        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-16        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-17        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-18        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-19        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-22        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-23        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-24        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-26        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-29        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-30        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1980-12-31        NaN    NaN       NaN   NaN       NaN        NaN  NaN   \n",
       "1981-01-02        NaN    NaN       NaN   NaN  0.576113        NaN  NaN   \n",
       "1981-01-05        NaN    NaN  0.135723   NaN  0.589686  28.790395  NaN   \n",
       "1981-01-06        NaN    NaN  0.087618   NaN  0.598447  17.682086  NaN   \n",
       "1981-01-07        NaN    NaN  0.034714   NaN  0.601919   6.647757  NaN   \n",
       "1981-01-08 -50.810264    NaN -0.006721   NaN  0.601247  -1.228067  NaN   \n",
       "1981-01-09 -39.605208    NaN -0.018190   NaN  0.599428  -3.156036  NaN   \n",
       "1981-01-12 -45.485436    NaN -0.062827   NaN  0.593145 -10.055487  NaN   \n",
       "\n",
       "                 will  \n",
       "Date                   \n",
       "1980-12-12        NaN  \n",
       "1980-12-15        NaN  \n",
       "1980-12-16        NaN  \n",
       "1980-12-17        NaN  \n",
       "1980-12-18        NaN  \n",
       "1980-12-19        NaN  \n",
       "1980-12-22        NaN  \n",
       "1980-12-23        NaN  \n",
       "1980-12-24        NaN  \n",
       "1980-12-26        NaN  \n",
       "1980-12-29        NaN  \n",
       "1980-12-30        NaN  \n",
       "1980-12-31        NaN  \n",
       "1981-01-02        NaN  \n",
       "1981-01-05        NaN  \n",
       "1981-01-06        NaN  \n",
       "1981-01-07        NaN  \n",
       "1981-01-08 -60.852742  \n",
       "1981-01-09 -58.944450  \n",
       "1981-01-12 -68.411828  \n",
       "\n",
       "[20 rows x 40 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"return_1df\"] = df[\"return_log\"].shift(-1)\n",
    "df = df.dropna()\n",
    "df_ = df.copy()\n",
    "df_ = df_.drop([\"return_1df\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mid</th>\n",
       "      <th>return_log</th>\n",
       "      <th>dema</th>\n",
       "      <th>kama</th>\n",
       "      <th>trima</th>\n",
       "      <th>...</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>cci</th>\n",
       "      <th>mom20</th>\n",
       "      <th>mom10</th>\n",
       "      <th>ma20</th>\n",
       "      <th>ma10</th>\n",
       "      <th>roc</th>\n",
       "      <th>ult</th>\n",
       "      <th>will</th>\n",
       "      <th>return_1df</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>197.265996</td>\n",
       "      <td>195.575126</td>\n",
       "      <td>196.220212</td>\n",
       "      <td>197.031482</td>\n",
       "      <td>18526600.0</td>\n",
       "      <td>196.514999</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>193.759852</td>\n",
       "      <td>190.743735</td>\n",
       "      <td>184.175888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246490</td>\n",
       "      <td>140.127714</td>\n",
       "      <td>24.226734</td>\n",
       "      <td>4.715067</td>\n",
       "      <td>188.725600</td>\n",
       "      <td>191.601115</td>\n",
       "      <td>2.451724</td>\n",
       "      <td>70.014381</td>\n",
       "      <td>-4.968364</td>\n",
       "      <td>0.009007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>199.731579</td>\n",
       "      <td>196.520478</td>\n",
       "      <td>196.690401</td>\n",
       "      <td>199.626228</td>\n",
       "      <td>25881700.0</td>\n",
       "      <td>198.284996</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>195.014138</td>\n",
       "      <td>192.113841</td>\n",
       "      <td>185.110959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.398503</td>\n",
       "      <td>154.390872</td>\n",
       "      <td>22.376199</td>\n",
       "      <td>9.688483</td>\n",
       "      <td>189.844410</td>\n",
       "      <td>192.569964</td>\n",
       "      <td>5.100873</td>\n",
       "      <td>74.730365</td>\n",
       "      <td>-0.739076</td>\n",
       "      <td>0.013894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>202.437587</td>\n",
       "      <td>198.879195</td>\n",
       "      <td>199.665354</td>\n",
       "      <td>199.943355</td>\n",
       "      <td>35768200.0</td>\n",
       "      <td>201.040001</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>196.170597</td>\n",
       "      <td>193.315479</td>\n",
       "      <td>186.039862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446746</td>\n",
       "      <td>159.652880</td>\n",
       "      <td>19.662007</td>\n",
       "      <td>12.362221</td>\n",
       "      <td>190.827510</td>\n",
       "      <td>193.806186</td>\n",
       "      <td>6.590333</td>\n",
       "      <td>68.410866</td>\n",
       "      <td>-14.722653</td>\n",
       "      <td>-0.007859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>201.634693</td>\n",
       "      <td>198.769072</td>\n",
       "      <td>199.342745</td>\n",
       "      <td>200.751794</td>\n",
       "      <td>21695300.0</td>\n",
       "      <td>199.459999</td>\n",
       "      <td>-0.007859</td>\n",
       "      <td>197.299303</td>\n",
       "      <td>194.467183</td>\n",
       "      <td>186.966904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457757</td>\n",
       "      <td>126.474375</td>\n",
       "      <td>18.955450</td>\n",
       "      <td>12.697592</td>\n",
       "      <td>191.775282</td>\n",
       "      <td>195.075945</td>\n",
       "      <td>6.752092</td>\n",
       "      <td>65.933528</td>\n",
       "      <td>-9.950695</td>\n",
       "      <td>0.001304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>201.405294</td>\n",
       "      <td>198.789279</td>\n",
       "      <td>200.723915</td>\n",
       "      <td>199.668455</td>\n",
       "      <td>20900800.0</td>\n",
       "      <td>199.720001</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>198.166161</td>\n",
       "      <td>195.280531</td>\n",
       "      <td>187.863616</td>\n",
       "      <td>...</td>\n",
       "      <td>0.323081</td>\n",
       "      <td>98.216001</td>\n",
       "      <td>15.999955</td>\n",
       "      <td>11.204537</td>\n",
       "      <td>192.575280</td>\n",
       "      <td>196.196399</td>\n",
       "      <td>5.945189</td>\n",
       "      <td>62.190461</td>\n",
       "      <td>-16.345295</td>\n",
       "      <td>-0.007736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close      Volume  \\\n",
       "Date                                                                     \n",
       "2019-04-05  197.265996  195.575126  196.220212  197.031482  18526600.0   \n",
       "2019-04-08  199.731579  196.520478  196.690401  199.626228  25881700.0   \n",
       "2019-04-09  202.437587  198.879195  199.665354  199.943355  35768200.0   \n",
       "2019-04-10  201.634693  198.769072  199.342745  200.751794  21695300.0   \n",
       "2019-04-11  201.405294  198.789279  200.723915  199.668455  20900800.0   \n",
       "\n",
       "                   mid  return_log        dema        kama       trima  ...  \\\n",
       "Date                                                                    ...   \n",
       "2019-04-05  196.514999    0.009037  193.759852  190.743735  184.175888  ...   \n",
       "2019-04-08  198.284996    0.009007  195.014138  192.113841  185.110959  ...   \n",
       "2019-04-09  201.040001    0.013894  196.170597  193.315479  186.039862  ...   \n",
       "2019-04-10  199.459999   -0.007859  197.299303  194.467183  186.966904  ...   \n",
       "2019-04-11  199.720001    0.001304  198.166161  195.280531  187.863616  ...   \n",
       "\n",
       "            macdhist         cci      mom20      mom10        ma20  \\\n",
       "Date                                                                 \n",
       "2019-04-05  0.246490  140.127714  24.226734   4.715067  188.725600   \n",
       "2019-04-08  0.398503  154.390872  22.376199   9.688483  189.844410   \n",
       "2019-04-09  0.446746  159.652880  19.662007  12.362221  190.827510   \n",
       "2019-04-10  0.457757  126.474375  18.955450  12.697592  191.775282   \n",
       "2019-04-11  0.323081   98.216001  15.999955  11.204537  192.575280   \n",
       "\n",
       "                  ma10       roc        ult       will  return_1df  \n",
       "Date                                                                \n",
       "2019-04-05  191.601115  2.451724  70.014381  -4.968364    0.009007  \n",
       "2019-04-08  192.569964  5.100873  74.730365  -0.739076    0.013894  \n",
       "2019-04-09  193.806186  6.590333  68.410866 -14.722653   -0.007859  \n",
       "2019-04-10  195.075945  6.752092  65.933528  -9.950695    0.001304  \n",
       "2019-04-11  196.196399  5.945189  62.190461 -16.345295   -0.007736  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>mid</th>\n",
       "      <th>return_log</th>\n",
       "      <th>dema</th>\n",
       "      <th>kama</th>\n",
       "      <th>trima</th>\n",
       "      <th>...</th>\n",
       "      <th>macdsignal</th>\n",
       "      <th>macdhist</th>\n",
       "      <th>cci</th>\n",
       "      <th>mom20</th>\n",
       "      <th>mom10</th>\n",
       "      <th>ma20</th>\n",
       "      <th>ma10</th>\n",
       "      <th>roc</th>\n",
       "      <th>ult</th>\n",
       "      <th>will</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04-05</th>\n",
       "      <td>197.265996</td>\n",
       "      <td>195.575126</td>\n",
       "      <td>196.220212</td>\n",
       "      <td>197.031482</td>\n",
       "      <td>18526600.0</td>\n",
       "      <td>196.514999</td>\n",
       "      <td>0.009037</td>\n",
       "      <td>193.759852</td>\n",
       "      <td>190.743735</td>\n",
       "      <td>184.175888</td>\n",
       "      <td>...</td>\n",
       "      <td>4.988015</td>\n",
       "      <td>0.246490</td>\n",
       "      <td>140.127714</td>\n",
       "      <td>24.226734</td>\n",
       "      <td>4.715067</td>\n",
       "      <td>188.725600</td>\n",
       "      <td>191.601115</td>\n",
       "      <td>2.451724</td>\n",
       "      <td>70.014381</td>\n",
       "      <td>-4.968364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-08</th>\n",
       "      <td>199.731579</td>\n",
       "      <td>196.520478</td>\n",
       "      <td>196.690401</td>\n",
       "      <td>199.626228</td>\n",
       "      <td>25881700.0</td>\n",
       "      <td>198.284996</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>195.014138</td>\n",
       "      <td>192.113841</td>\n",
       "      <td>185.110959</td>\n",
       "      <td>...</td>\n",
       "      <td>5.087640</td>\n",
       "      <td>0.398503</td>\n",
       "      <td>154.390872</td>\n",
       "      <td>22.376199</td>\n",
       "      <td>9.688483</td>\n",
       "      <td>189.844410</td>\n",
       "      <td>192.569964</td>\n",
       "      <td>5.100873</td>\n",
       "      <td>74.730365</td>\n",
       "      <td>-0.739076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-09</th>\n",
       "      <td>202.437587</td>\n",
       "      <td>198.879195</td>\n",
       "      <td>199.665354</td>\n",
       "      <td>199.943355</td>\n",
       "      <td>35768200.0</td>\n",
       "      <td>201.040001</td>\n",
       "      <td>0.013894</td>\n",
       "      <td>196.170597</td>\n",
       "      <td>193.315479</td>\n",
       "      <td>186.039862</td>\n",
       "      <td>...</td>\n",
       "      <td>5.199327</td>\n",
       "      <td>0.446746</td>\n",
       "      <td>159.652880</td>\n",
       "      <td>19.662007</td>\n",
       "      <td>12.362221</td>\n",
       "      <td>190.827510</td>\n",
       "      <td>193.806186</td>\n",
       "      <td>6.590333</td>\n",
       "      <td>68.410866</td>\n",
       "      <td>-14.722653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-10</th>\n",
       "      <td>201.634693</td>\n",
       "      <td>198.769072</td>\n",
       "      <td>199.342745</td>\n",
       "      <td>200.751794</td>\n",
       "      <td>21695300.0</td>\n",
       "      <td>199.459999</td>\n",
       "      <td>-0.007859</td>\n",
       "      <td>197.299303</td>\n",
       "      <td>194.467183</td>\n",
       "      <td>186.966904</td>\n",
       "      <td>...</td>\n",
       "      <td>5.313766</td>\n",
       "      <td>0.457757</td>\n",
       "      <td>126.474375</td>\n",
       "      <td>18.955450</td>\n",
       "      <td>12.697592</td>\n",
       "      <td>191.775282</td>\n",
       "      <td>195.075945</td>\n",
       "      <td>6.752092</td>\n",
       "      <td>65.933528</td>\n",
       "      <td>-9.950695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-11</th>\n",
       "      <td>201.405294</td>\n",
       "      <td>198.789279</td>\n",
       "      <td>200.723915</td>\n",
       "      <td>199.668455</td>\n",
       "      <td>20900800.0</td>\n",
       "      <td>199.720001</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>198.166161</td>\n",
       "      <td>195.280531</td>\n",
       "      <td>187.863616</td>\n",
       "      <td>...</td>\n",
       "      <td>5.394536</td>\n",
       "      <td>0.323081</td>\n",
       "      <td>98.216001</td>\n",
       "      <td>15.999955</td>\n",
       "      <td>11.204537</td>\n",
       "      <td>192.575280</td>\n",
       "      <td>196.196399</td>\n",
       "      <td>5.945189</td>\n",
       "      <td>62.190461</td>\n",
       "      <td>-16.345295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low        Open       Close      Volume  \\\n",
       "Date                                                                     \n",
       "2019-04-05  197.265996  195.575126  196.220212  197.031482  18526600.0   \n",
       "2019-04-08  199.731579  196.520478  196.690401  199.626228  25881700.0   \n",
       "2019-04-09  202.437587  198.879195  199.665354  199.943355  35768200.0   \n",
       "2019-04-10  201.634693  198.769072  199.342745  200.751794  21695300.0   \n",
       "2019-04-11  201.405294  198.789279  200.723915  199.668455  20900800.0   \n",
       "\n",
       "                   mid  return_log        dema        kama       trima  ...  \\\n",
       "Date                                                                    ...   \n",
       "2019-04-05  196.514999    0.009037  193.759852  190.743735  184.175888  ...   \n",
       "2019-04-08  198.284996    0.009007  195.014138  192.113841  185.110959  ...   \n",
       "2019-04-09  201.040001    0.013894  196.170597  193.315479  186.039862  ...   \n",
       "2019-04-10  199.459999   -0.007859  197.299303  194.467183  186.966904  ...   \n",
       "2019-04-11  199.720001    0.001304  198.166161  195.280531  187.863616  ...   \n",
       "\n",
       "            macdsignal  macdhist         cci      mom20      mom10  \\\n",
       "Date                                                                 \n",
       "2019-04-05    4.988015  0.246490  140.127714  24.226734   4.715067   \n",
       "2019-04-08    5.087640  0.398503  154.390872  22.376199   9.688483   \n",
       "2019-04-09    5.199327  0.446746  159.652880  19.662007  12.362221   \n",
       "2019-04-10    5.313766  0.457757  126.474375  18.955450  12.697592   \n",
       "2019-04-11    5.394536  0.323081   98.216001  15.999955  11.204537   \n",
       "\n",
       "                  ma20        ma10       roc        ult       will  \n",
       "Date                                                                \n",
       "2019-04-05  188.725600  191.601115  2.451724  70.014381  -4.968364  \n",
       "2019-04-08  189.844410  192.569964  5.100873  74.730365  -0.739076  \n",
       "2019-04-09  190.827510  193.806186  6.590333  68.410866 -14.722653  \n",
       "2019-04-10  191.775282  195.075945  6.752092  65.933528  -9.950695  \n",
       "2019-04-11  192.575280  196.196399  5.945189  62.190461 -16.345295  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 300\n",
    "pd.options.display.max_columns = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ = df_.iloc[:9000]\n",
    "test_df_ = df_.iloc[9000:9200]\n",
    "train_df = df.iloc[:9000]\n",
    "test_df = df.iloc[9000:9200]\n",
    "out_of_sample = df.iloc[9200:9500]\n",
    "out_of_sample_ = df_.iloc[9200:9500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_X = np.array(train_df_.values)\n",
    "train_data_y = np.array(train_df[\"return_1df\"].values)\n",
    "test_data_X = np.array(test_df_.values)\n",
    "test_data_y = np.array(test_df[\"return_1df\"].values)\n",
    "X_out_sample = np.array(out_of_sample_.values)\n",
    "y_out_sample = np.array(out_of_sample[\"return_1df\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window_size = 600\n",
    "scaler_min = MinMaxScaler()\n",
    "for di in range(0,9000,smoothing_window_size):\n",
    "    scaler_min.fit(train_data_X[di:di+smoothing_window_size,:])\n",
    "    train_data_X[di:di+smoothing_window_size,:] = scaler_min.transform(train_data_X[di:di+smoothing_window_size,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_X = scaler_min.transform(test_data_X)\n",
    "X_out_sample = scaler_min.transform(X_out_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train_data_X\n",
    "y_train = train_data_y\n",
    "X_valid = test_data_X\n",
    "y_valid = test_data_y\n",
    "X_test = X_out_sample\n",
    "y_test = y_out_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM autoencoders perform poorly\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "input = keras.layers.Input(shape=[n_steps, num_features])\n",
    "encoder = keras.layers.LSTM(num_encoder, return_sequences=True)(input)\n",
    "encoder1 = keras.layers.LSTM(num_encoder1, return_sequences=True)(encoder)\n",
    "encoder2 = keras.layers.LSTM(num_encoder2, return_sequences=True)(encoder1)\n",
    "encoded = keras.layers.LSTM(num_encoded)(encoder2)\n",
    "repeat = keras.layers.RepeatVector(n_steps, input_shape=[num_encoded])(encoded)\n",
    "decoder = keras.layers.LSTM(num_decoder, return_sequences=True)(repeat)\n",
    "decoder1 = keras.layers.LSTM(num_decoder1, return_sequences=True)(decoder)\n",
    "decoder2 = keras.layers.LSTM(num_decoder2, return_sequences=True)(decoder1)\n",
    "decoded = keras.layers.TimeDistributed(keras.layers.Dense(num_features, activation=\"sigmoid\"))(decoder2)\n",
    "ae_model = keras.models.Model(inputs=[input], outputs=[decoded])\n",
    "encode_model = keras.models.Model(inputs=[input], outputs=[encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        [(None, 20, 40)]          0         \n",
      "_________________________________________________________________\n",
      "unified_lstm_41 (UnifiedLSTM (None, 20, 100)           56400     \n",
      "_________________________________________________________________\n",
      "unified_lstm_42 (UnifiedLSTM (None, 20, 80)            57920     \n",
      "_________________________________________________________________\n",
      "unified_lstm_43 (UnifiedLSTM (None, 20, 40)            19360     \n",
      "_________________________________________________________________\n",
      "unified_lstm_44 (UnifiedLSTM (None, 20)                4880      \n",
      "_________________________________________________________________\n",
      "repeat_vector_13 (RepeatVect (None, 20, 20)            0         \n",
      "_________________________________________________________________\n",
      "unified_lstm_45 (UnifiedLSTM (None, 20, 40)            9760      \n",
      "_________________________________________________________________\n",
      "unified_lstm_46 (UnifiedLSTM (None, 20, 80)            38720     \n",
      "_________________________________________________________________\n",
      "unified_lstm_47 (UnifiedLSTM (None, 20, 100)           72400     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 20, 40)            4040      \n",
      "=================================================================\n",
      "Total params: 263,480\n",
      "Trainable params: 263,480\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1, n_steps, num_features))\n",
    "X_valid = X_valid.reshape((-1, n_steps, num_features))\n",
    "X_test = X_test.reshape((-1, n_steps, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 10 samples\n",
      "Epoch 1/2000\n",
      "450/450 [==============================] - 4s 9ms/sample - loss: 0.0661 - val_loss: 0.2217\n",
      "Epoch 2/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0533 - val_loss: 0.1787\n",
      "Epoch 3/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0491 - val_loss: 0.1583\n",
      "Epoch 4/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0457 - val_loss: 0.1518\n",
      "Epoch 5/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0432 - val_loss: 0.1470\n",
      "Epoch 6/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0387 - val_loss: 0.1748\n",
      "Epoch 7/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0390 - val_loss: 0.1499\n",
      "Epoch 8/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0345 - val_loss: 0.1291\n",
      "Epoch 9/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0326 - val_loss: 0.1169\n",
      "Epoch 10/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0317 - val_loss: 0.1096\n",
      "Epoch 11/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0302 - val_loss: 0.1051\n",
      "Epoch 12/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0306 - val_loss: 0.1111\n",
      "Epoch 13/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0299 - val_loss: 0.1051\n",
      "Epoch 14/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0286 - val_loss: 0.1017\n",
      "Epoch 15/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0291 - val_loss: 0.1007\n",
      "Epoch 16/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0283 - val_loss: 0.1010\n",
      "Epoch 17/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0279 - val_loss: 0.1013\n",
      "Epoch 18/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0279 - val_loss: 0.0996\n",
      "Epoch 19/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0280 - val_loss: 0.0978\n",
      "Epoch 20/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0277 - val_loss: 0.0946\n",
      "Epoch 21/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0267 - val_loss: 0.0980\n",
      "Epoch 22/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0275 - val_loss: 0.0983\n",
      "Epoch 23/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0251 - val_loss: 0.0988\n",
      "Epoch 24/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0269 - val_loss: 0.0950\n",
      "Epoch 25/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0247 - val_loss: 0.0923\n",
      "Epoch 26/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0247 - val_loss: 0.0926\n",
      "Epoch 27/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0239 - val_loss: 0.0901\n",
      "Epoch 28/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0237 - val_loss: 0.1037\n",
      "Epoch 29/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0242 - val_loss: 0.0878\n",
      "Epoch 30/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0230 - val_loss: 0.0870\n",
      "Epoch 31/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0224 - val_loss: 0.0846\n",
      "Epoch 32/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0222 - val_loss: 0.0846\n",
      "Epoch 33/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0222 - val_loss: 0.0848\n",
      "Epoch 34/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0220 - val_loss: 0.0851\n",
      "Epoch 35/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0222 - val_loss: 0.0841\n",
      "Epoch 36/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0214 - val_loss: 0.0838\n",
      "Epoch 37/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0214 - val_loss: 0.0867\n",
      "Epoch 38/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0215 - val_loss: 0.0845\n",
      "Epoch 39/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0213 - val_loss: 0.0902\n",
      "Epoch 40/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0213 - val_loss: 0.0837\n",
      "Epoch 41/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0203 - val_loss: 0.0825\n",
      "Epoch 42/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0212 - val_loss: 0.0819\n",
      "Epoch 43/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0209 - val_loss: 0.0878\n",
      "Epoch 44/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0215 - val_loss: 0.0847\n",
      "Epoch 45/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0201 - val_loss: 0.0825\n",
      "Epoch 46/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0199 - val_loss: 0.0809\n",
      "Epoch 47/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0200 - val_loss: 0.0794\n",
      "Epoch 48/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0196 - val_loss: 0.0801\n",
      "Epoch 49/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0203 - val_loss: 0.0868\n",
      "Epoch 50/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0210 - val_loss: 0.0792\n",
      "Epoch 51/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0200 - val_loss: 0.0797\n",
      "Epoch 52/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0191 - val_loss: 0.0789\n",
      "Epoch 53/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0186 - val_loss: 0.0801\n",
      "Epoch 54/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0187 - val_loss: 0.0819\n",
      "Epoch 55/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0198 - val_loss: 0.0818\n",
      "Epoch 56/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0188 - val_loss: 0.0759\n",
      "Epoch 57/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0176 - val_loss: 0.0741\n",
      "Epoch 58/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0179 - val_loss: 0.0802\n",
      "Epoch 59/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0178 - val_loss: 0.0784\n",
      "Epoch 60/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0176 - val_loss: 0.0739\n",
      "Epoch 61/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0173 - val_loss: 0.0750\n",
      "Epoch 62/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0192 - val_loss: 0.0753\n",
      "Epoch 63/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0175 - val_loss: 0.0734\n",
      "Epoch 64/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0168 - val_loss: 0.0829\n",
      "Epoch 65/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0169 - val_loss: 0.0735\n",
      "Epoch 66/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0163 - val_loss: 0.0763\n",
      "Epoch 67/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0159 - val_loss: 0.0746\n",
      "Epoch 68/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0162 - val_loss: 0.0732\n",
      "Epoch 69/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0156 - val_loss: 0.0790\n",
      "Epoch 70/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0157 - val_loss: 0.0742\n",
      "Epoch 71/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0151 - val_loss: 0.0771\n",
      "Epoch 72/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0154 - val_loss: 0.0741\n",
      "Epoch 73/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0149 - val_loss: 0.0730\n",
      "Epoch 74/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0148 - val_loss: 0.0746\n",
      "Epoch 75/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0150 - val_loss: 0.0704\n",
      "Epoch 76/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0152 - val_loss: 0.0743\n",
      "Epoch 77/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0147 - val_loss: 0.0736\n",
      "Epoch 78/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0150 - val_loss: 0.0755\n",
      "Epoch 79/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0146 - val_loss: 0.0798\n",
      "Epoch 80/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0146 - val_loss: 0.0697\n",
      "Epoch 81/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0144 - val_loss: 0.0706\n",
      "Epoch 82/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0148 - val_loss: 0.0694\n",
      "Epoch 83/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0138 - val_loss: 0.0700\n",
      "Epoch 84/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0136 - val_loss: 0.0738\n",
      "Epoch 85/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0137 - val_loss: 0.0699\n",
      "Epoch 86/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0138 - val_loss: 0.0703\n",
      "Epoch 87/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0135 - val_loss: 0.0700\n",
      "Epoch 88/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0134 - val_loss: 0.0714\n",
      "Epoch 89/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0134 - val_loss: 0.0722\n",
      "Epoch 90/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0133 - val_loss: 0.0724\n",
      "Epoch 91/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0140 - val_loss: 0.0752\n",
      "Epoch 92/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0137 - val_loss: 0.0721\n",
      "Epoch 93/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0131 - val_loss: 0.0718\n",
      "Epoch 94/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0131 - val_loss: 0.0724\n",
      "Epoch 95/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0135 - val_loss: 0.0699\n",
      "Epoch 96/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0129 - val_loss: 0.0716\n",
      "Epoch 97/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0130 - val_loss: 0.0754\n",
      "Epoch 98/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0130 - val_loss: 0.0694\n",
      "Epoch 99/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0131 - val_loss: 0.0713\n",
      "Epoch 100/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0128 - val_loss: 0.0706\n",
      "Epoch 101/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0127 - val_loss: 0.0706\n",
      "Epoch 102/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0129 - val_loss: 0.0716\n",
      "Epoch 103/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0128 - val_loss: 0.0751\n",
      "Epoch 104/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0126 - val_loss: 0.0718\n",
      "Epoch 105/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0124 - val_loss: 0.0755\n",
      "Epoch 106/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0123 - val_loss: 0.0719\n",
      "Epoch 107/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0127 - val_loss: 0.0725\n",
      "Epoch 108/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0128 - val_loss: 0.0804\n",
      "Epoch 109/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0125 - val_loss: 0.0730\n",
      "Epoch 110/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0120 - val_loss: 0.0726\n",
      "Epoch 111/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0122 - val_loss: 0.0741\n",
      "Epoch 112/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0123 - val_loss: 0.0763\n",
      "Epoch 113/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0122 - val_loss: 0.0725\n",
      "Epoch 114/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0121 - val_loss: 0.0719\n",
      "Epoch 115/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0118 - val_loss: 0.0731\n",
      "Epoch 116/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0122 - val_loss: 0.0718\n",
      "Epoch 117/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0121 - val_loss: 0.0713\n",
      "Epoch 118/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0122 - val_loss: 0.0707\n",
      "Epoch 119/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0119 - val_loss: 0.0721\n",
      "Epoch 120/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0116 - val_loss: 0.0706\n",
      "Epoch 121/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0115 - val_loss: 0.0720\n",
      "Epoch 122/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0118 - val_loss: 0.0836\n",
      "Epoch 123/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0121 - val_loss: 0.0730\n",
      "Epoch 124/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0118 - val_loss: 0.0712\n",
      "Epoch 125/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0122 - val_loss: 0.0708\n",
      "Epoch 126/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0114 - val_loss: 0.0678\n",
      "Epoch 127/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0115 - val_loss: 0.0722\n",
      "Epoch 128/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0114 - val_loss: 0.0731\n",
      "Epoch 129/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0114 - val_loss: 0.0738\n",
      "Epoch 130/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0113 - val_loss: 0.0768\n",
      "Epoch 131/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0123 - val_loss: 0.0730\n",
      "Epoch 132/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0123 - val_loss: 0.0739\n",
      "Epoch 133/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0112 - val_loss: 0.0745\n",
      "Epoch 134/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0120 - val_loss: 0.0700\n",
      "Epoch 135/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0111 - val_loss: 0.0695\n",
      "Epoch 136/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0112 - val_loss: 0.0701\n",
      "Epoch 137/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0112 - val_loss: 0.0753\n",
      "Epoch 138/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0111 - val_loss: 0.0743\n",
      "Epoch 139/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0110 - val_loss: 0.0759\n",
      "Epoch 140/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0112 - val_loss: 0.0702\n",
      "Epoch 141/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0108 - val_loss: 0.0716\n",
      "Epoch 142/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0107 - val_loss: 0.0724\n",
      "Epoch 143/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0107 - val_loss: 0.0709\n",
      "Epoch 144/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0108 - val_loss: 0.0732\n",
      "Epoch 145/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0106 - val_loss: 0.0731\n",
      "Epoch 146/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0106 - val_loss: 0.0722\n",
      "Epoch 147/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0106 - val_loss: 0.0785\n",
      "Epoch 148/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0107 - val_loss: 0.0740\n",
      "Epoch 149/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0105 - val_loss: 0.0757\n",
      "Epoch 150/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0105 - val_loss: 0.0723\n",
      "Epoch 151/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0106 - val_loss: 0.0715\n",
      "Epoch 152/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0104 - val_loss: 0.0726\n",
      "Epoch 153/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0101 - val_loss: 0.0730\n",
      "Epoch 154/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0106 - val_loss: 0.0765\n",
      "Epoch 155/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0105 - val_loss: 0.0765\n",
      "Epoch 156/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0107 - val_loss: 0.0720\n",
      "Epoch 157/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.1035\n",
      "Epoch 158/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0155 - val_loss: 0.0723\n",
      "Epoch 159/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0108 - val_loss: 0.0697\n",
      "Epoch 160/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0103 - val_loss: 0.0705\n",
      "Epoch 161/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.0699\n",
      "Epoch 162/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.0708\n",
      "Epoch 163/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0099 - val_loss: 0.0700\n",
      "Epoch 164/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.0713\n",
      "Epoch 165/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0099 - val_loss: 0.0696\n",
      "Epoch 166/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0096 - val_loss: 0.0709\n",
      "Epoch 167/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0096 - val_loss: 0.0695\n",
      "Epoch 168/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0098 - val_loss: 0.0708\n",
      "Epoch 169/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0097 - val_loss: 0.0722\n",
      "Epoch 170/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0097 - val_loss: 0.0711\n",
      "Epoch 171/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0096 - val_loss: 0.0693\n",
      "Epoch 172/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0094 - val_loss: 0.0715\n",
      "Epoch 173/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0095 - val_loss: 0.0710\n",
      "Epoch 174/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.0683\n",
      "Epoch 175/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0094 - val_loss: 0.0691\n",
      "Epoch 176/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0093 - val_loss: 0.0683\n",
      "Epoch 177/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0095 - val_loss: 0.0702\n",
      "Epoch 178/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0100 - val_loss: 0.0719\n",
      "Epoch 179/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0097 - val_loss: 0.0696\n",
      "Epoch 180/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0095 - val_loss: 0.0694\n",
      "Epoch 181/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0095 - val_loss: 0.0699\n",
      "Epoch 182/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0092 - val_loss: 0.0698\n",
      "Epoch 183/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0091 - val_loss: 0.0688\n",
      "Epoch 184/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0090 - val_loss: 0.0711\n",
      "Epoch 185/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0094 - val_loss: 0.0689\n",
      "Epoch 186/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0090 - val_loss: 0.0714\n",
      "Epoch 187/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0093 - val_loss: 0.0692\n",
      "Epoch 188/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0089 - val_loss: 0.0696\n",
      "Epoch 189/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0089 - val_loss: 0.0735\n",
      "Epoch 190/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0093 - val_loss: 0.0714\n",
      "Epoch 191/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0088 - val_loss: 0.0695\n",
      "Epoch 192/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0693\n",
      "Epoch 193/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0089 - val_loss: 0.0700\n",
      "Epoch 194/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0088 - val_loss: 0.0751\n",
      "Epoch 195/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0088 - val_loss: 0.0702\n",
      "Epoch 196/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0688\n",
      "Epoch 197/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0683\n",
      "Epoch 198/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0086 - val_loss: 0.0691\n",
      "Epoch 199/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0097 - val_loss: 0.0700\n",
      "Epoch 200/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0105 - val_loss: 0.0704\n",
      "Epoch 201/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0092 - val_loss: 0.0693\n",
      "Epoch 202/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0700\n",
      "Epoch 203/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0086 - val_loss: 0.0693\n",
      "Epoch 204/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0697\n",
      "Epoch 205/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0084 - val_loss: 0.0715\n",
      "Epoch 206/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0090 - val_loss: 0.0703\n",
      "Epoch 207/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0098 - val_loss: 0.0720\n",
      "Epoch 208/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0098 - val_loss: 0.0713\n",
      "Epoch 209/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0733\n",
      "Epoch 210/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0695\n",
      "Epoch 211/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0084 - val_loss: 0.0708\n",
      "Epoch 212/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0092 - val_loss: 0.0718\n",
      "Epoch 213/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0090 - val_loss: 0.0719\n",
      "Epoch 214/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0091 - val_loss: 0.0708\n",
      "Epoch 215/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0082 - val_loss: 0.0713\n",
      "Epoch 216/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0081 - val_loss: 0.0693\n",
      "Epoch 217/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0080 - val_loss: 0.0702\n",
      "Epoch 218/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0079 - val_loss: 0.0702\n",
      "Epoch 219/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0078 - val_loss: 0.0717\n",
      "Epoch 220/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0083 - val_loss: 0.0727\n",
      "Epoch 221/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0079 - val_loss: 0.0698\n",
      "Epoch 222/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0086 - val_loss: 0.0708\n",
      "Epoch 223/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0081 - val_loss: 0.0724\n",
      "Epoch 224/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0087 - val_loss: 0.0713\n",
      "Epoch 225/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0082 - val_loss: 0.0898\n",
      "Epoch 226/2000\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0096 - val_loss: 0.0689\n"
     ]
    }
   ],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n",
    "ae_model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "history = ae_model.fit(X_train, X_train, epochs=2000, validation_data=[X_valid, X_valid], callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-572139793b36>, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-572139793b36>\"\u001b[0;36m, line \u001b[0;32m37\u001b[0m\n\u001b[0;31m    autoencoder.fit(train_data, train_data, epochs=1000)\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class AutoEncoder:\n",
    "    def __init__(self, pool):\n",
    "        self.pool = pool\n",
    "\n",
    "    def build_train_model(self, input_shape, encoded1_shape, encoded2_shape, decoded1_shape, decoded2_shape):\n",
    "        input_data = keras.layers.Input(shape=(20, input_shape)) # 20 dims\n",
    "        normalize = keras.layers.BatchNormalization()(input_data)\n",
    "        encoded1 = keras.layers.Conv1D(encoded1_shape, 3, padding = \"same\", activation=\"selu\")(normalize) #20 dims\n",
    "        pool0 = keras.layers.MaxPool1D(pool)(encoded1) # 10 dims\n",
    "        encoded2 = keras.layers.Conv1D(encoded2_shape, 3, padding = \"same\", activation=\"selu\")(pool0) # 10 dims\n",
    "        pool1 = keras.layers.MaxPool1D(pool)(encoded2) # 5 dims, encoded data, upsample from here\n",
    "        decoded1 = keras.layers.Conv1D(decoded1_shape, 3, padding = \"same\", activation=\"selu\")(pool1) # 5 dims\n",
    "        upsample0 = keras.layers.UpSampling1D(pool)(decoded1) # 10 dims\n",
    "        decoded2 = keras.layers.Conv1D(decoded2_shape, 3, padding = \"same\", activation=\"selu\")(upsample0) # 10 dims\n",
    "        upsample1 = keras.layers.UpSampling1D(pool)(decoded2) # 20 dims\n",
    "        output_data = keras.layers.Conv1D(input_shape, 3, padding = \"same\", activation=\"sigmoid\")(upsample1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        autoencoder = keras.models.Model(inputs=input_data, outputs=output_data)\n",
    "\n",
    "        encoder = keras.models.Model(input_data, pool1)\n",
    "        autoencoder.compile(loss=\"mean_squared_error\", optimizer=\"Nadam\")\n",
    "\n",
    "        train = train_data_X\n",
    "        ntrain = np.array(train)\n",
    "        train_data = np.reshape(ntrain, (-1, 20, input_shape))\n",
    "        \n",
    "        test = test_data_X\n",
    "        ntest = np.array(test)\n",
    "        test_data = np.reshape(ntest, (-1, 20, input_shape))\n",
    "        \n",
    "        out_sample = X_out_sample\n",
    "        nout = np.array(out_sample)\n",
    "        out_data = np.reshape(nout, (len(-1, 20, input_shape))\n",
    "\n",
    "        autoencoder.fit(train_data, train_data, epochs=1000)\n",
    "        valid_error = autoencoder.evaluate(test_data, test_data)\n",
    "        test_error = autoencoder.evaluate(out_data, out_data)\n",
    "\n",
    "        \n",
    "    \n",
    "        coded_train = encoder.predict(train_data)\n",
    "        coded_test = encoder.predict(test_data)        \n",
    "        coded_outsample = encoder.predict(out_data)\n",
    "        return train_coded, test_coded, out_coded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = keras.layers.Input(shape=(20, 40))\n",
    "normalize = keras.layers.BatchNormalization()(input_data)\n",
    "encoded1 = keras.layers.Conv1D(80, 3, padding = \"same\", activation=\"selu\")(normalize) #20 dims\n",
    "pool0 = keras.layers.MaxPool1D(2)(encoded1) # 10 dims\n",
    "encoded2 = keras.layers.Conv1D(40, 3, padding = \"same\", activation=\"selu\")(pool0) # 10 dims\n",
    "pool1 = keras.layers.MaxPool1D(2)(encoded2) # 5 dims, encoded data, upsample from here\n",
    "decoded1 = keras.layers.Conv1D(40, 3, padding = \"same\", activation=\"selu\")(pool1) # 5 dims\n",
    "upsample0 = keras.layers.UpSampling1D(2)(decoded1) # 10 dims\n",
    "decoded2 = keras.layers.Conv1D(80, 3, padding = \"same\", activation=\"selu\")(upsample0) # 10 dims\n",
    "upsample1 = keras.layers.UpSampling1D(2)(decoded2) # 20 dims\n",
    "output_data = keras.layers.Conv1D(40, 3, padding = \"same\", activation=\"sigmoid\")(upsample1)\n",
    "autoencoder = keras.models.Model(inputs=input_data, outputs=output_data)\n",
    "encoder = keras.models.Model(input_data, pool1)\n",
    "autoencoder.compile(loss=\"mean_squared_error\", optimizer=\"Nadam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 20, 40)]          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_3 (Ba (None, 20, 40)            160       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 20, 80)            9680      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 10, 80)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 10, 40)            9640      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 5, 40)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 5, 40)             4840      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_5 (UpSampling1 (None, 10, 40)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 10, 80)            9680      \n",
      "_________________________________________________________________\n",
      "up_sampling1d_6 (UpSampling1 (None, 20, 80)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 20, 40)            9640      \n",
      "=================================================================\n",
      "Total params: 43,640\n",
      "Trainable params: 43,560\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "450/450 [==============================] - 1s 1ms/sample - loss: 0.0542\n",
      "Epoch 2/1000\n",
      "450/450 [==============================] - 0s 257us/sample - loss: 0.0227\n",
      "Epoch 3/1000\n",
      "450/450 [==============================] - 0s 250us/sample - loss: 0.0185\n",
      "Epoch 4/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0150\n",
      "Epoch 5/1000\n",
      "450/450 [==============================] - 0s 242us/sample - loss: 0.0147\n",
      "Epoch 6/1000\n",
      "450/450 [==============================] - 0s 249us/sample - loss: 0.0127\n",
      "Epoch 7/1000\n",
      "450/450 [==============================] - 0s 252us/sample - loss: 0.0129\n",
      "Epoch 8/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0126\n",
      "Epoch 9/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0122\n",
      "Epoch 10/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0116\n",
      "Epoch 11/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0116\n",
      "Epoch 12/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0109\n",
      "Epoch 13/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0115\n",
      "Epoch 14/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0097\n",
      "Epoch 15/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0093\n",
      "Epoch 16/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0105\n",
      "Epoch 17/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0091\n",
      "Epoch 18/1000\n",
      "450/450 [==============================] - 0s 270us/sample - loss: 0.0102\n",
      "Epoch 19/1000\n",
      "450/450 [==============================] - 0s 270us/sample - loss: 0.0103\n",
      "Epoch 20/1000\n",
      "450/450 [==============================] - 0s 257us/sample - loss: 0.0099\n",
      "Epoch 21/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0087\n",
      "Epoch 22/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0099\n",
      "Epoch 23/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0087\n",
      "Epoch 24/1000\n",
      "450/450 [==============================] - 0s 238us/sample - loss: 0.0084\n",
      "Epoch 25/1000\n",
      "450/450 [==============================] - 0s 250us/sample - loss: 0.0081\n",
      "Epoch 26/1000\n",
      "450/450 [==============================] - 0s 252us/sample - loss: 0.0079\n",
      "Epoch 27/1000\n",
      "450/450 [==============================] - 0s 268us/sample - loss: 0.0076\n",
      "Epoch 28/1000\n",
      "450/450 [==============================] - 0s 260us/sample - loss: 0.0076\n",
      "Epoch 29/1000\n",
      "450/450 [==============================] - 0s 274us/sample - loss: 0.0074\n",
      "Epoch 30/1000\n",
      "450/450 [==============================] - 0s 257us/sample - loss: 0.0072\n",
      "Epoch 31/1000\n",
      "450/450 [==============================] - 0s 256us/sample - loss: 0.0076\n",
      "Epoch 32/1000\n",
      "450/450 [==============================] - 0s 242us/sample - loss: 0.0072\n",
      "Epoch 33/1000\n",
      "450/450 [==============================] - 0s 257us/sample - loss: 0.0088\n",
      "Epoch 34/1000\n",
      "450/450 [==============================] - 0s 259us/sample - loss: 0.0087\n",
      "Epoch 35/1000\n",
      "450/450 [==============================] - 0s 253us/sample - loss: 0.0074\n",
      "Epoch 36/1000\n",
      "450/450 [==============================] - 0s 261us/sample - loss: 0.0071\n",
      "Epoch 37/1000\n",
      "450/450 [==============================] - 0s 259us/sample - loss: 0.0072\n",
      "Epoch 38/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0090\n",
      "Epoch 39/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0070\n",
      "Epoch 40/1000\n",
      "450/450 [==============================] - 0s 254us/sample - loss: 0.0069\n",
      "Epoch 41/1000\n",
      "450/450 [==============================] - 0s 264us/sample - loss: 0.0073\n",
      "Epoch 42/1000\n",
      "450/450 [==============================] - 0s 248us/sample - loss: 0.0094\n",
      "Epoch 43/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0075\n",
      "Epoch 44/1000\n",
      "450/450 [==============================] - 0s 238us/sample - loss: 0.0070\n",
      "Epoch 45/1000\n",
      "450/450 [==============================] - 0s 242us/sample - loss: 0.0073\n",
      "Epoch 46/1000\n",
      "450/450 [==============================] - 0s 248us/sample - loss: 0.0069\n",
      "Epoch 47/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0068\n",
      "Epoch 48/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0077\n",
      "Epoch 49/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0075\n",
      "Epoch 50/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0077\n",
      "Epoch 51/1000\n",
      "450/450 [==============================] - 0s 245us/sample - loss: 0.0076\n",
      "Epoch 52/1000\n",
      "450/450 [==============================] - 0s 248us/sample - loss: 0.0078\n",
      "Epoch 53/1000\n",
      "450/450 [==============================] - 0s 266us/sample - loss: 0.0063\n",
      "Epoch 54/1000\n",
      "450/450 [==============================] - 0s 259us/sample - loss: 0.0068\n",
      "Epoch 55/1000\n",
      "450/450 [==============================] - 0s 287us/sample - loss: 0.0076\n",
      "Epoch 56/1000\n",
      "450/450 [==============================] - 0s 252us/sample - loss: 0.0067\n",
      "Epoch 57/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0070\n",
      "Epoch 58/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0072\n",
      "Epoch 59/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0074\n",
      "Epoch 60/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0070\n",
      "Epoch 61/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0064\n",
      "Epoch 62/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0063\n",
      "Epoch 63/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0065\n",
      "Epoch 64/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0063\n",
      "Epoch 65/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0062\n",
      "Epoch 66/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0069\n",
      "Epoch 67/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0076\n",
      "Epoch 68/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0060\n",
      "Epoch 69/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0062\n",
      "Epoch 70/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0063\n",
      "Epoch 71/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0069\n",
      "Epoch 72/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0075\n",
      "Epoch 73/1000\n",
      "450/450 [==============================] - 0s 250us/sample - loss: 0.0064\n",
      "Epoch 74/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0063\n",
      "Epoch 75/1000\n",
      "450/450 [==============================] - 0s 249us/sample - loss: 0.0060\n",
      "Epoch 76/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0064\n",
      "Epoch 77/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0061\n",
      "Epoch 78/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0058\n",
      "Epoch 79/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0071\n",
      "Epoch 80/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0059\n",
      "Epoch 81/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0063\n",
      "Epoch 82/1000\n",
      "450/450 [==============================] - 0s 256us/sample - loss: 0.0085\n",
      "Epoch 83/1000\n",
      "450/450 [==============================] - 0s 254us/sample - loss: 0.0065\n",
      "Epoch 84/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0069\n",
      "Epoch 85/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0065\n",
      "Epoch 86/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0062\n",
      "Epoch 87/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0061\n",
      "Epoch 88/1000\n",
      "450/450 [==============================] - 0s 269us/sample - loss: 0.0068\n",
      "Epoch 89/1000\n",
      "450/450 [==============================] - 0s 260us/sample - loss: 0.0061\n",
      "Epoch 90/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0057\n",
      "Epoch 91/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0058\n",
      "Epoch 92/1000\n",
      "450/450 [==============================] - 0s 245us/sample - loss: 0.0062\n",
      "Epoch 93/1000\n",
      "450/450 [==============================] - 0s 285us/sample - loss: 0.0062\n",
      "Epoch 94/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 255us/sample - loss: 0.0062\n",
      "Epoch 95/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0069\n",
      "Epoch 96/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0064\n",
      "Epoch 97/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0055\n",
      "Epoch 98/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0058\n",
      "Epoch 99/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0055\n",
      "Epoch 100/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0061\n",
      "Epoch 101/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0061\n",
      "Epoch 102/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0065\n",
      "Epoch 103/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0057\n",
      "Epoch 104/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0058\n",
      "Epoch 105/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0062\n",
      "Epoch 106/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0059\n",
      "Epoch 107/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0059\n",
      "Epoch 108/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0060\n",
      "Epoch 109/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0058\n",
      "Epoch 110/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0051\n",
      "Epoch 111/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0061\n",
      "Epoch 112/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0055\n",
      "Epoch 113/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0060\n",
      "Epoch 114/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0058\n",
      "Epoch 115/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0060\n",
      "Epoch 116/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0057\n",
      "Epoch 117/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0055\n",
      "Epoch 118/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0057\n",
      "Epoch 119/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0057\n",
      "Epoch 120/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0053\n",
      "Epoch 121/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0056\n",
      "Epoch 122/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0052\n",
      "Epoch 123/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0054\n",
      "Epoch 124/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0055\n",
      "Epoch 125/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0054\n",
      "Epoch 126/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0059\n",
      "Epoch 127/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0056\n",
      "Epoch 128/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0053\n",
      "Epoch 129/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0066\n",
      "Epoch 130/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0056\n",
      "Epoch 131/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0055\n",
      "Epoch 132/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0056\n",
      "Epoch 133/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0059\n",
      "Epoch 134/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0056\n",
      "Epoch 135/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0057\n",
      "Epoch 136/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0071\n",
      "Epoch 137/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0061\n",
      "Epoch 138/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0054\n",
      "Epoch 139/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0051\n",
      "Epoch 140/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0056\n",
      "Epoch 141/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0056\n",
      "Epoch 142/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0058\n",
      "Epoch 143/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0060\n",
      "Epoch 144/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0055\n",
      "Epoch 145/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0052\n",
      "Epoch 146/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0053\n",
      "Epoch 147/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0059\n",
      "Epoch 148/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0056\n",
      "Epoch 149/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0055\n",
      "Epoch 150/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0058\n",
      "Epoch 151/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0053\n",
      "Epoch 152/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0063\n",
      "Epoch 153/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0066\n",
      "Epoch 154/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0053\n",
      "Epoch 155/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0062\n",
      "Epoch 156/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0053\n",
      "Epoch 157/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0055\n",
      "Epoch 158/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0055\n",
      "Epoch 159/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0054\n",
      "Epoch 160/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0056\n",
      "Epoch 161/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0046\n",
      "Epoch 162/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0047\n",
      "Epoch 163/1000\n",
      "450/450 [==============================] - 0s 260us/sample - loss: 0.0068\n",
      "Epoch 164/1000\n",
      "450/450 [==============================] - 0s 265us/sample - loss: 0.0055\n",
      "Epoch 165/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0054\n",
      "Epoch 166/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0059\n",
      "Epoch 167/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0052\n",
      "Epoch 168/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0057\n",
      "Epoch 169/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0051\n",
      "Epoch 170/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0045\n",
      "Epoch 171/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0050\n",
      "Epoch 172/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0054\n",
      "Epoch 173/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0058\n",
      "Epoch 174/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0053\n",
      "Epoch 175/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0050\n",
      "Epoch 176/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0053\n",
      "Epoch 177/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0049\n",
      "Epoch 178/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0052\n",
      "Epoch 179/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0050\n",
      "Epoch 180/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0052\n",
      "Epoch 181/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0050\n",
      "Epoch 182/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0052\n",
      "Epoch 183/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0054\n",
      "Epoch 184/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0059\n",
      "Epoch 185/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0056\n",
      "Epoch 186/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 209us/sample - loss: 0.0052\n",
      "Epoch 187/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0047\n",
      "Epoch 188/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0048\n",
      "Epoch 189/1000\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 0.0061\n",
      "Epoch 190/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0054\n",
      "Epoch 191/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0062\n",
      "Epoch 192/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0050\n",
      "Epoch 193/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0057\n",
      "Epoch 194/1000\n",
      "450/450 [==============================] - 0s 207us/sample - loss: 0.0059\n",
      "Epoch 195/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0056\n",
      "Epoch 196/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0051\n",
      "Epoch 197/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0053\n",
      "Epoch 198/1000\n",
      "450/450 [==============================] - 0s 207us/sample - loss: 0.0051\n",
      "Epoch 199/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0048\n",
      "Epoch 200/1000\n",
      "450/450 [==============================] - 0s 204us/sample - loss: 0.0049\n",
      "Epoch 201/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0055\n",
      "Epoch 202/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0046\n",
      "Epoch 203/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0048\n",
      "Epoch 204/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0051\n",
      "Epoch 205/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0048\n",
      "Epoch 206/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0051\n",
      "Epoch 207/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0058\n",
      "Epoch 208/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0050\n",
      "Epoch 209/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0045\n",
      "Epoch 210/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0047\n",
      "Epoch 211/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0050\n",
      "Epoch 212/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0048\n",
      "Epoch 213/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0052\n",
      "Epoch 214/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0052\n",
      "Epoch 215/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0050\n",
      "Epoch 216/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0053\n",
      "Epoch 217/1000\n",
      "450/450 [==============================] - 0s 204us/sample - loss: 0.0053\n",
      "Epoch 218/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0050\n",
      "Epoch 219/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0058\n",
      "Epoch 220/1000\n",
      "450/450 [==============================] - 0s 257us/sample - loss: 0.0048\n",
      "Epoch 221/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0048\n",
      "Epoch 222/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0046\n",
      "Epoch 223/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0045\n",
      "Epoch 224/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0052\n",
      "Epoch 225/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0045\n",
      "Epoch 226/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0046\n",
      "Epoch 227/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0045\n",
      "Epoch 228/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0044\n",
      "Epoch 229/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0045\n",
      "Epoch 230/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0050\n",
      "Epoch 231/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0047\n",
      "Epoch 232/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0047\n",
      "Epoch 233/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0044\n",
      "Epoch 234/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0047\n",
      "Epoch 235/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0046\n",
      "Epoch 236/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0047\n",
      "Epoch 237/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0048\n",
      "Epoch 238/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0051\n",
      "Epoch 239/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0047\n",
      "Epoch 240/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0045\n",
      "Epoch 241/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0056\n",
      "Epoch 242/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0048\n",
      "Epoch 243/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0049\n",
      "Epoch 244/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0044\n",
      "Epoch 245/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0046\n",
      "Epoch 246/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0046\n",
      "Epoch 247/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0049\n",
      "Epoch 248/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0048\n",
      "Epoch 249/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0048\n",
      "Epoch 250/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0056\n",
      "Epoch 251/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0048\n",
      "Epoch 252/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 253/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0051\n",
      "Epoch 254/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0052\n",
      "Epoch 255/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0053\n",
      "Epoch 256/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0051\n",
      "Epoch 257/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0058\n",
      "Epoch 258/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0051\n",
      "Epoch 259/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0053\n",
      "Epoch 260/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0046\n",
      "Epoch 261/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0056\n",
      "Epoch 262/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0046\n",
      "Epoch 263/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0055\n",
      "Epoch 264/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0047\n",
      "Epoch 265/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0063\n",
      "Epoch 266/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0053\n",
      "Epoch 267/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0055\n",
      "Epoch 268/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0050\n",
      "Epoch 269/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0047\n",
      "Epoch 270/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0047\n",
      "Epoch 271/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0051\n",
      "Epoch 272/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0050\n",
      "Epoch 273/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0058\n",
      "Epoch 274/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0047\n",
      "Epoch 275/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0048\n",
      "Epoch 276/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0045\n",
      "Epoch 277/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0047\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0044\n",
      "Epoch 279/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0057\n",
      "Epoch 280/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0051\n",
      "Epoch 281/1000\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 0.0048\n",
      "Epoch 282/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 283/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0049\n",
      "Epoch 284/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0048\n",
      "Epoch 285/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0053\n",
      "Epoch 286/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0048\n",
      "Epoch 287/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0048\n",
      "Epoch 288/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0045\n",
      "Epoch 289/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0051\n",
      "Epoch 290/1000\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 0.0048\n",
      "Epoch 291/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0052\n",
      "Epoch 292/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0046\n",
      "Epoch 293/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0047\n",
      "Epoch 294/1000\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 0.0053\n",
      "Epoch 295/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0056\n",
      "Epoch 296/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0049\n",
      "Epoch 297/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0046\n",
      "Epoch 298/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0047\n",
      "Epoch 299/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 300/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0047\n",
      "Epoch 301/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0043\n",
      "Epoch 302/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0044\n",
      "Epoch 303/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0050\n",
      "Epoch 304/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0044\n",
      "Epoch 305/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0045\n",
      "Epoch 306/1000\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 0.0044\n",
      "Epoch 307/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0046\n",
      "Epoch 308/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0048\n",
      "Epoch 309/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0044\n",
      "Epoch 310/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0049\n",
      "Epoch 311/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0046\n",
      "Epoch 312/1000\n",
      "450/450 [==============================] - 0s 182us/sample - loss: 0.0048\n",
      "Epoch 313/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0043\n",
      "Epoch 314/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0046\n",
      "Epoch 315/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0046\n",
      "Epoch 316/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0049\n",
      "Epoch 317/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0049\n",
      "Epoch 318/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0054\n",
      "Epoch 319/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0058\n",
      "Epoch 320/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0049\n",
      "Epoch 321/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0048\n",
      "Epoch 322/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0044\n",
      "Epoch 323/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0043\n",
      "Epoch 324/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0058\n",
      "Epoch 325/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0049\n",
      "Epoch 326/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0050\n",
      "Epoch 327/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0043\n",
      "Epoch 328/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0046\n",
      "Epoch 329/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0046\n",
      "Epoch 330/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0042\n",
      "Epoch 331/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0046\n",
      "Epoch 332/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0046\n",
      "Epoch 333/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0048\n",
      "Epoch 334/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0054\n",
      "Epoch 335/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0051\n",
      "Epoch 336/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0048\n",
      "Epoch 337/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0050\n",
      "Epoch 338/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0051\n",
      "Epoch 339/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0044\n",
      "Epoch 340/1000\n",
      "450/450 [==============================] - 0s 204us/sample - loss: 0.0049\n",
      "Epoch 341/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0046\n",
      "Epoch 342/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0044\n",
      "Epoch 343/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0041\n",
      "Epoch 344/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0041\n",
      "Epoch 345/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0041\n",
      "Epoch 346/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0045\n",
      "Epoch 347/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0047\n",
      "Epoch 348/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0050\n",
      "Epoch 349/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0052\n",
      "Epoch 350/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0042\n",
      "Epoch 351/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0042\n",
      "Epoch 352/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0044\n",
      "Epoch 353/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0043\n",
      "Epoch 354/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0057\n",
      "Epoch 355/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0046\n",
      "Epoch 356/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0044\n",
      "Epoch 357/1000\n",
      "450/450 [==============================] - 0s 356us/sample - loss: 0.0048\n",
      "Epoch 358/1000\n",
      "450/450 [==============================] - 0s 301us/sample - loss: 0.0046\n",
      "Epoch 359/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0042\n",
      "Epoch 360/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0042\n",
      "Epoch 361/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0041\n",
      "Epoch 362/1000\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 0.0047\n",
      "Epoch 363/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0050\n",
      "Epoch 364/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0049\n",
      "Epoch 365/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0046\n",
      "Epoch 366/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0043\n",
      "Epoch 367/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0044\n",
      "Epoch 368/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0040\n",
      "Epoch 369/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0044\n",
      "Epoch 370/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0044\n",
      "Epoch 371/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0047\n",
      "Epoch 372/1000\n",
      "450/450 [==============================] - 0s 181us/sample - loss: 0.0045\n",
      "Epoch 373/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0040\n",
      "Epoch 374/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0045\n",
      "Epoch 375/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0045\n",
      "Epoch 376/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0047\n",
      "Epoch 377/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0041\n",
      "Epoch 378/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0052\n",
      "Epoch 379/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0050\n",
      "Epoch 380/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0043\n",
      "Epoch 381/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0046\n",
      "Epoch 382/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0043\n",
      "Epoch 383/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0042\n",
      "Epoch 384/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0043\n",
      "Epoch 385/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0048\n",
      "Epoch 386/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0046\n",
      "Epoch 387/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 388/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0047\n",
      "Epoch 389/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0044\n",
      "Epoch 390/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0053\n",
      "Epoch 391/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0048\n",
      "Epoch 392/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0042\n",
      "Epoch 393/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0053\n",
      "Epoch 394/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0043\n",
      "Epoch 395/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0047\n",
      "Epoch 396/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0043\n",
      "Epoch 397/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0048\n",
      "Epoch 398/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0041\n",
      "Epoch 399/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0044\n",
      "Epoch 400/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0047\n",
      "Epoch 401/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0050\n",
      "Epoch 402/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0046\n",
      "Epoch 403/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0041\n",
      "Epoch 404/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0044\n",
      "Epoch 405/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0039\n",
      "Epoch 406/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0041\n",
      "Epoch 407/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0040\n",
      "Epoch 408/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0053\n",
      "Epoch 409/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0048\n",
      "Epoch 410/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0045\n",
      "Epoch 411/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0047\n",
      "Epoch 412/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0059\n",
      "Epoch 413/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0045\n",
      "Epoch 414/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0048\n",
      "Epoch 415/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0042\n",
      "Epoch 416/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0046\n",
      "Epoch 417/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0050\n",
      "Epoch 418/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0043\n",
      "Epoch 419/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0047\n",
      "Epoch 420/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0042\n",
      "Epoch 421/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0048\n",
      "Epoch 422/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0042\n",
      "Epoch 423/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0044\n",
      "Epoch 424/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0046\n",
      "Epoch 425/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0047\n",
      "Epoch 426/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0041\n",
      "Epoch 427/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0047\n",
      "Epoch 428/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0045\n",
      "Epoch 429/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0043\n",
      "Epoch 430/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0043\n",
      "Epoch 431/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0041\n",
      "Epoch 432/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0041\n",
      "Epoch 433/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0045\n",
      "Epoch 434/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0048\n",
      "Epoch 435/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0041\n",
      "Epoch 436/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0043\n",
      "Epoch 437/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0040\n",
      "Epoch 438/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0042\n",
      "Epoch 439/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0039\n",
      "Epoch 440/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0040\n",
      "Epoch 441/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0045\n",
      "Epoch 442/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0041\n",
      "Epoch 443/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0050\n",
      "Epoch 444/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0044\n",
      "Epoch 445/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0045\n",
      "Epoch 446/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0042\n",
      "Epoch 447/1000\n",
      "450/450 [==============================] - 0s 356us/sample - loss: 0.0039\n",
      "Epoch 448/1000\n",
      "450/450 [==============================] - 0s 301us/sample - loss: 0.0044\n",
      "Epoch 449/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0046\n",
      "Epoch 450/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0043\n",
      "Epoch 451/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0051\n",
      "Epoch 452/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0048\n",
      "Epoch 453/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0047\n",
      "Epoch 454/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0043\n",
      "Epoch 455/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0047\n",
      "Epoch 456/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0055\n",
      "Epoch 457/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0048\n",
      "Epoch 458/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0043\n",
      "Epoch 459/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0041\n",
      "Epoch 460/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0041\n",
      "Epoch 461/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0046\n",
      "Epoch 462/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 207us/sample - loss: 0.0045\n",
      "Epoch 463/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0041\n",
      "Epoch 464/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0041\n",
      "Epoch 465/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0044\n",
      "Epoch 466/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0039\n",
      "Epoch 467/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 468/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0041\n",
      "Epoch 469/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0050\n",
      "Epoch 470/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0044\n",
      "Epoch 471/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0046\n",
      "Epoch 472/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0050\n",
      "Epoch 473/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0040\n",
      "Epoch 474/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0048\n",
      "Epoch 475/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0045\n",
      "Epoch 476/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0042\n",
      "Epoch 477/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0049\n",
      "Epoch 478/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0042\n",
      "Epoch 479/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0046\n",
      "Epoch 480/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0043\n",
      "Epoch 481/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0045\n",
      "Epoch 482/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0047\n",
      "Epoch 483/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0046\n",
      "Epoch 484/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0048\n",
      "Epoch 485/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0042\n",
      "Epoch 486/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0044\n",
      "Epoch 487/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0039\n",
      "Epoch 488/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0042\n",
      "Epoch 489/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0044\n",
      "Epoch 490/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0039\n",
      "Epoch 491/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0034\n",
      "Epoch 492/1000\n",
      "450/450 [==============================] - 0s 183us/sample - loss: 0.0040\n",
      "Epoch 493/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0042\n",
      "Epoch 494/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0042\n",
      "Epoch 495/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0043\n",
      "Epoch 496/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0046\n",
      "Epoch 497/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0049\n",
      "Epoch 498/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0050\n",
      "Epoch 499/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0047\n",
      "Epoch 500/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0049\n",
      "Epoch 501/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0049\n",
      "Epoch 502/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0042\n",
      "Epoch 503/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0039\n",
      "Epoch 504/1000\n",
      "450/450 [==============================] - 0s 314us/sample - loss: 0.0037\n",
      "Epoch 505/1000\n",
      "450/450 [==============================] - 0s 333us/sample - loss: 0.0041\n",
      "Epoch 506/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0043\n",
      "Epoch 507/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0045\n",
      "Epoch 508/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0040\n",
      "Epoch 509/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0038\n",
      "Epoch 510/1000\n",
      "450/450 [==============================] - 0s 184us/sample - loss: 0.0042\n",
      "Epoch 511/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0038\n",
      "Epoch 512/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0037\n",
      "Epoch 513/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0040\n",
      "Epoch 514/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0045\n",
      "Epoch 515/1000\n",
      "450/450 [==============================] - 0s 190us/sample - loss: 0.0043\n",
      "Epoch 516/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0040\n",
      "Epoch 517/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0041\n",
      "Epoch 518/1000\n",
      "450/450 [==============================] - 0s 191us/sample - loss: 0.0042\n",
      "Epoch 519/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0050\n",
      "Epoch 520/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0042\n",
      "Epoch 521/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0038\n",
      "Epoch 522/1000\n",
      "450/450 [==============================] - 0s 188us/sample - loss: 0.0042\n",
      "Epoch 523/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0045\n",
      "Epoch 524/1000\n",
      "450/450 [==============================] - 0s 197us/sample - loss: 0.0045\n",
      "Epoch 525/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0041\n",
      "Epoch 526/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0039\n",
      "Epoch 527/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0040\n",
      "Epoch 528/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0040\n",
      "Epoch 529/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0042\n",
      "Epoch 530/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0039\n",
      "Epoch 531/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0043\n",
      "Epoch 532/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0049\n",
      "Epoch 533/1000\n",
      "450/450 [==============================] - 0s 207us/sample - loss: 0.0045\n",
      "Epoch 534/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0042\n",
      "Epoch 535/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0040\n",
      "Epoch 536/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0039\n",
      "Epoch 537/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0041\n",
      "Epoch 538/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0043\n",
      "Epoch 539/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0046\n",
      "Epoch 540/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0042\n",
      "Epoch 541/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0041\n",
      "Epoch 542/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0045\n",
      "Epoch 543/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0045\n",
      "Epoch 544/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0038\n",
      "Epoch 545/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0041\n",
      "Epoch 546/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0045\n",
      "Epoch 547/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0040\n",
      "Epoch 548/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0044\n",
      "Epoch 549/1000\n",
      "450/450 [==============================] - 0s 198us/sample - loss: 0.0038\n",
      "Epoch 550/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0039\n",
      "Epoch 551/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0041\n",
      "Epoch 552/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0040\n",
      "Epoch 553/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0037\n",
      "Epoch 554/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0037\n",
      "Epoch 555/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0034\n",
      "Epoch 556/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0044\n",
      "Epoch 557/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0045\n",
      "Epoch 558/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0039\n",
      "Epoch 559/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0040\n",
      "Epoch 560/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0037\n",
      "Epoch 561/1000\n",
      "450/450 [==============================] - 0s 193us/sample - loss: 0.0036\n",
      "Epoch 562/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0038\n",
      "Epoch 563/1000\n",
      "450/450 [==============================] - 0s 194us/sample - loss: 0.0041\n",
      "Epoch 564/1000\n",
      "450/450 [==============================] - 0s 185us/sample - loss: 0.0039\n",
      "Epoch 565/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0047\n",
      "Epoch 566/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0053\n",
      "Epoch 567/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0046\n",
      "Epoch 568/1000\n",
      "450/450 [==============================] - 0s 189us/sample - loss: 0.0046\n",
      "Epoch 569/1000\n",
      "450/450 [==============================] - 0s 186us/sample - loss: 0.0046\n",
      "Epoch 570/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0045\n",
      "Epoch 571/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0044\n",
      "Epoch 572/1000\n",
      "450/450 [==============================] - 0s 192us/sample - loss: 0.0043\n",
      "Epoch 573/1000\n",
      "450/450 [==============================] - 0s 187us/sample - loss: 0.0048\n",
      "Epoch 574/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0039\n",
      "Epoch 575/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0038\n",
      "Epoch 576/1000\n",
      "450/450 [==============================] - 0s 247us/sample - loss: 0.0036\n",
      "Epoch 577/1000\n",
      "450/450 [==============================] - 0s 252us/sample - loss: 0.0038\n",
      "Epoch 578/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0045\n",
      "Epoch 579/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0039\n",
      "Epoch 580/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0034\n",
      "Epoch 581/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0040\n",
      "Epoch 582/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0039\n",
      "Epoch 583/1000\n",
      "450/450 [==============================] - 0s 200us/sample - loss: 0.0040\n",
      "Epoch 584/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0045\n",
      "Epoch 585/1000\n",
      "450/450 [==============================] - 0s 195us/sample - loss: 0.0042\n",
      "Epoch 586/1000\n",
      "450/450 [==============================] - 0s 201us/sample - loss: 0.0043\n",
      "Epoch 587/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0043\n",
      "Epoch 588/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0040\n",
      "Epoch 589/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0038\n",
      "Epoch 590/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0038\n",
      "Epoch 591/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0038\n",
      "Epoch 592/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0041\n",
      "Epoch 593/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0052\n",
      "Epoch 594/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0050\n",
      "Epoch 595/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0046\n",
      "Epoch 596/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0039\n",
      "Epoch 597/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0044\n",
      "Epoch 598/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0045\n",
      "Epoch 599/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0043\n",
      "Epoch 600/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0041\n",
      "Epoch 601/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0047\n",
      "Epoch 602/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0041\n",
      "Epoch 603/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0037\n",
      "Epoch 604/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0043\n",
      "Epoch 605/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0048\n",
      "Epoch 606/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0040\n",
      "Epoch 607/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0039\n",
      "Epoch 608/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0037\n",
      "Epoch 609/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0036\n",
      "Epoch 610/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0038\n",
      "Epoch 611/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 612/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0038\n",
      "Epoch 613/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0043\n",
      "Epoch 614/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0041\n",
      "Epoch 615/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0043\n",
      "Epoch 616/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0040\n",
      "Epoch 617/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0039\n",
      "Epoch 618/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0041\n",
      "Epoch 619/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0039\n",
      "Epoch 620/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 621/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 622/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0037\n",
      "Epoch 623/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0034\n",
      "Epoch 624/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0039\n",
      "Epoch 625/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0034\n",
      "Epoch 626/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0043\n",
      "Epoch 627/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0049\n",
      "Epoch 628/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0054\n",
      "Epoch 629/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0040\n",
      "Epoch 630/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0038\n",
      "Epoch 631/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0036\n",
      "Epoch 632/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0040\n",
      "Epoch 633/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0037\n",
      "Epoch 634/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0039\n",
      "Epoch 635/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0051\n",
      "Epoch 636/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0043\n",
      "Epoch 637/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0043\n",
      "Epoch 638/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0041\n",
      "Epoch 639/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0040\n",
      "Epoch 640/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0042\n",
      "Epoch 641/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0040\n",
      "Epoch 642/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0039\n",
      "Epoch 643/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0041\n",
      "Epoch 644/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0039\n",
      "Epoch 645/1000\n",
      "450/450 [==============================] - 0s 238us/sample - loss: 0.0037\n",
      "Epoch 646/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0041\n",
      "Epoch 647/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0044\n",
      "Epoch 648/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0038\n",
      "Epoch 649/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0041\n",
      "Epoch 650/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0045\n",
      "Epoch 651/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0039\n",
      "Epoch 652/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0037\n",
      "Epoch 653/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0036\n",
      "Epoch 654/1000\n",
      "450/450 [==============================] - 0s 196us/sample - loss: 0.0038\n",
      "Epoch 655/1000\n",
      "450/450 [==============================] - 0s 199us/sample - loss: 0.0041\n",
      "Epoch 656/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0036\n",
      "Epoch 657/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0043\n",
      "Epoch 658/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0044\n",
      "Epoch 659/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0042\n",
      "Epoch 660/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0044\n",
      "Epoch 661/1000\n",
      "450/450 [==============================] - 0s 210us/sample - loss: 0.0044\n",
      "Epoch 662/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0044\n",
      "Epoch 663/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0043\n",
      "Epoch 664/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0040\n",
      "Epoch 665/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0041\n",
      "Epoch 666/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0040\n",
      "Epoch 667/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0034\n",
      "Epoch 668/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0037\n",
      "Epoch 669/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0048\n",
      "Epoch 670/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0040\n",
      "Epoch 671/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0038\n",
      "Epoch 672/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0042\n",
      "Epoch 673/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0043\n",
      "Epoch 674/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0060\n",
      "Epoch 675/1000\n",
      "450/450 [==============================] - 0s 209us/sample - loss: 0.0044\n",
      "Epoch 676/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0039\n",
      "Epoch 677/1000\n",
      "450/450 [==============================] - 0s 206us/sample - loss: 0.0040\n",
      "Epoch 678/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0043\n",
      "Epoch 679/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0038\n",
      "Epoch 680/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0040\n",
      "Epoch 681/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0039\n",
      "Epoch 682/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0041\n",
      "Epoch 683/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 684/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0038\n",
      "Epoch 685/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0035\n",
      "Epoch 686/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0040\n",
      "Epoch 687/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0040\n",
      "Epoch 688/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0034\n",
      "Epoch 689/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 690/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0038\n",
      "Epoch 691/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0038\n",
      "Epoch 692/1000\n",
      "450/450 [==============================] - 0s 205us/sample - loss: 0.0037\n",
      "Epoch 693/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0036\n",
      "Epoch 694/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0035\n",
      "Epoch 695/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0038\n",
      "Epoch 696/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0038\n",
      "Epoch 697/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0037\n",
      "Epoch 698/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0039\n",
      "Epoch 699/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0043\n",
      "Epoch 700/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0047\n",
      "Epoch 701/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0040\n",
      "Epoch 702/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0040\n",
      "Epoch 703/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0040\n",
      "Epoch 704/1000\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 0.0044\n",
      "Epoch 705/1000\n",
      "450/450 [==============================] - 0s 209us/sample - loss: 0.0048\n",
      "Epoch 706/1000\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 0.0038\n",
      "Epoch 707/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0040\n",
      "Epoch 708/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0036\n",
      "Epoch 709/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0040\n",
      "Epoch 710/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0036\n",
      "Epoch 711/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0039\n",
      "Epoch 712/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0036\n",
      "Epoch 713/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0032\n",
      "Epoch 714/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0034\n",
      "Epoch 715/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0036\n",
      "Epoch 716/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 717/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0037\n",
      "Epoch 718/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0041\n",
      "Epoch 719/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0045\n",
      "Epoch 720/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0037\n",
      "Epoch 721/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0034\n",
      "Epoch 722/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0037\n",
      "Epoch 723/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0035\n",
      "Epoch 724/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0039\n",
      "Epoch 725/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0040\n",
      "Epoch 726/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0039\n",
      "Epoch 727/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0035\n",
      "Epoch 728/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0036\n",
      "Epoch 729/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0034\n",
      "Epoch 730/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 731/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0041\n",
      "Epoch 732/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0034\n",
      "Epoch 733/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0041\n",
      "Epoch 734/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0038\n",
      "Epoch 735/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0042\n",
      "Epoch 736/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0041\n",
      "Epoch 737/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0036\n",
      "Epoch 738/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0039\n",
      "Epoch 739/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0035\n",
      "Epoch 740/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0037\n",
      "Epoch 741/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0041\n",
      "Epoch 742/1000\n",
      "450/450 [==============================] - 0s 213us/sample - loss: 0.0045\n",
      "Epoch 743/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0040\n",
      "Epoch 744/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0042\n",
      "Epoch 745/1000\n",
      "450/450 [==============================] - 0s 203us/sample - loss: 0.0040\n",
      "Epoch 746/1000\n",
      "450/450 [==============================] - 0s 207us/sample - loss: 0.0040\n",
      "Epoch 747/1000\n",
      "450/450 [==============================] - 0s 208us/sample - loss: 0.0038\n",
      "Epoch 748/1000\n",
      "450/450 [==============================] - 0s 202us/sample - loss: 0.0040\n",
      "Epoch 749/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0046\n",
      "Epoch 750/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0043\n",
      "Epoch 751/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0036\n",
      "Epoch 752/1000\n",
      "450/450 [==============================] - 0s 211us/sample - loss: 0.0038\n",
      "Epoch 753/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0035\n",
      "Epoch 754/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0037\n",
      "Epoch 755/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0037\n",
      "Epoch 756/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0037\n",
      "Epoch 757/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0039\n",
      "Epoch 758/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0038\n",
      "Epoch 759/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0039\n",
      "Epoch 760/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0046\n",
      "Epoch 761/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0043\n",
      "Epoch 762/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0039\n",
      "Epoch 763/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0040\n",
      "Epoch 764/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0042\n",
      "Epoch 765/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0044\n",
      "Epoch 766/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0048\n",
      "Epoch 767/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0041\n",
      "Epoch 768/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0040\n",
      "Epoch 769/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0047\n",
      "Epoch 770/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0045\n",
      "Epoch 771/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0043\n",
      "Epoch 772/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0045\n",
      "Epoch 773/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0044\n",
      "Epoch 774/1000\n",
      "450/450 [==============================] - 0s 215us/sample - loss: 0.0044\n",
      "Epoch 775/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0047\n",
      "Epoch 776/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 777/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0039\n",
      "Epoch 778/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0042\n",
      "Epoch 779/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0041\n",
      "Epoch 780/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0041\n",
      "Epoch 781/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0043\n",
      "Epoch 782/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0040\n",
      "Epoch 783/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0042\n",
      "Epoch 784/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0041\n",
      "Epoch 785/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0042\n",
      "Epoch 786/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0040\n",
      "Epoch 787/1000\n",
      "450/450 [==============================] - 0s 245us/sample - loss: 0.0040\n",
      "Epoch 788/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0036\n",
      "Epoch 789/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0038\n",
      "Epoch 790/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0039\n",
      "Epoch 791/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0034\n",
      "Epoch 792/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0042\n",
      "Epoch 793/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0035\n",
      "Epoch 794/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0036\n",
      "Epoch 795/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0038\n",
      "Epoch 796/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0036\n",
      "Epoch 797/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0036\n",
      "Epoch 798/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0038\n",
      "Epoch 799/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0035\n",
      "Epoch 800/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0043\n",
      "Epoch 801/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0042\n",
      "Epoch 802/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0040\n",
      "Epoch 803/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0039\n",
      "Epoch 804/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0035\n",
      "Epoch 805/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0034\n",
      "Epoch 806/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0038\n",
      "Epoch 807/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0045\n",
      "Epoch 808/1000\n",
      "450/450 [==============================] - 0s 238us/sample - loss: 0.0042\n",
      "Epoch 809/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0039\n",
      "Epoch 810/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0034\n",
      "Epoch 811/1000\n",
      "450/450 [==============================] - 0s 246us/sample - loss: 0.0036\n",
      "Epoch 812/1000\n",
      "450/450 [==============================] - 0s 249us/sample - loss: 0.0035\n",
      "Epoch 813/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0033\n",
      "Epoch 814/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0040\n",
      "Epoch 815/1000\n",
      "450/450 [==============================] - 0s 264us/sample - loss: 0.0037\n",
      "Epoch 816/1000\n",
      "450/450 [==============================] - 0s 254us/sample - loss: 0.0033\n",
      "Epoch 817/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0033\n",
      "Epoch 818/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0033\n",
      "Epoch 819/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0044\n",
      "Epoch 820/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0043\n",
      "Epoch 821/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0036\n",
      "Epoch 822/1000\n",
      "450/450 [==============================] - 0s 249us/sample - loss: 0.0037\n",
      "Epoch 823/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0035\n",
      "Epoch 824/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0033\n",
      "Epoch 825/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0034\n",
      "Epoch 826/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0037\n",
      "Epoch 827/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0042\n",
      "Epoch 828/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0035\n",
      "Epoch 829/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0033\n",
      "Epoch 830/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0032\n",
      "Epoch 831/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0034\n",
      "Epoch 832/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0037\n",
      "Epoch 833/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0034\n",
      "Epoch 834/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0037\n",
      "Epoch 835/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0034\n",
      "Epoch 836/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0040\n",
      "Epoch 837/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0038\n",
      "Epoch 838/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0038\n",
      "Epoch 839/1000\n",
      "450/450 [==============================] - 0s 238us/sample - loss: 0.0033\n",
      "Epoch 840/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0037\n",
      "Epoch 841/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0036\n",
      "Epoch 842/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0039\n",
      "Epoch 843/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0038\n",
      "Epoch 844/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0034\n",
      "Epoch 845/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0038\n",
      "Epoch 846/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0037\n",
      "Epoch 847/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0035\n",
      "Epoch 848/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0041\n",
      "Epoch 849/1000\n",
      "450/450 [==============================] - 0s 240us/sample - loss: 0.0041\n",
      "Epoch 850/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0040\n",
      "Epoch 851/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0038\n",
      "Epoch 852/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0038\n",
      "Epoch 853/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0035\n",
      "Epoch 854/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0037\n",
      "Epoch 855/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0033\n",
      "Epoch 856/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0035\n",
      "Epoch 857/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0051\n",
      "Epoch 858/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0038\n",
      "Epoch 859/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0036\n",
      "Epoch 860/1000\n",
      "450/450 [==============================] - 0s 212us/sample - loss: 0.0035\n",
      "Epoch 861/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0034\n",
      "Epoch 862/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0045\n",
      "Epoch 863/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0040\n",
      "Epoch 864/1000\n",
      "450/450 [==============================] - 0s 242us/sample - loss: 0.0035\n",
      "Epoch 865/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0035\n",
      "Epoch 866/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0036\n",
      "Epoch 867/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0043\n",
      "Epoch 868/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0034\n",
      "Epoch 869/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0037\n",
      "Epoch 870/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0038\n",
      "Epoch 871/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0041\n",
      "Epoch 872/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0045\n",
      "Epoch 873/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0035\n",
      "Epoch 874/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0037\n",
      "Epoch 875/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0037\n",
      "Epoch 876/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0039\n",
      "Epoch 877/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0034\n",
      "Epoch 878/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0033\n",
      "Epoch 879/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0033\n",
      "Epoch 880/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0039\n",
      "Epoch 881/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0035\n",
      "Epoch 882/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0036\n",
      "Epoch 883/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0045\n",
      "Epoch 884/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0038\n",
      "Epoch 885/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0038\n",
      "Epoch 886/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0041\n",
      "Epoch 887/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0034\n",
      "Epoch 888/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0034\n",
      "Epoch 889/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0037\n",
      "Epoch 890/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0038\n",
      "Epoch 891/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0047\n",
      "Epoch 892/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0036\n",
      "Epoch 893/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0035\n",
      "Epoch 894/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0036\n",
      "Epoch 895/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0034\n",
      "Epoch 896/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0033\n",
      "Epoch 897/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0041\n",
      "Epoch 898/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0041\n",
      "Epoch 899/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0034\n",
      "Epoch 900/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0033\n",
      "Epoch 901/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0031\n",
      "Epoch 902/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0033\n",
      "Epoch 903/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0036\n",
      "Epoch 904/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0033\n",
      "Epoch 905/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0036\n",
      "Epoch 906/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0038\n",
      "Epoch 907/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0038\n",
      "Epoch 908/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0043\n",
      "Epoch 909/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0047\n",
      "Epoch 910/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0046\n",
      "Epoch 911/1000\n",
      "450/450 [==============================] - 0s 227us/sample - loss: 0.0040\n",
      "Epoch 912/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0036\n",
      "Epoch 913/1000\n",
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0036\n",
      "Epoch 914/1000\n",
      "450/450 [==============================] - 0s 245us/sample - loss: 0.0036\n",
      "Epoch 915/1000\n",
      "450/450 [==============================] - 0s 241us/sample - loss: 0.0036\n",
      "Epoch 916/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0034\n",
      "Epoch 917/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0040\n",
      "Epoch 918/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0038\n",
      "Epoch 919/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0040\n",
      "Epoch 920/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0034\n",
      "Epoch 921/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0036\n",
      "Epoch 922/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 239us/sample - loss: 0.0041\n",
      "Epoch 923/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0036\n",
      "Epoch 924/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0039\n",
      "Epoch 925/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0035\n",
      "Epoch 926/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0042\n",
      "Epoch 927/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0036\n",
      "Epoch 928/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0036\n",
      "Epoch 929/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0043\n",
      "Epoch 930/1000\n",
      "450/450 [==============================] - 0s 234us/sample - loss: 0.0037\n",
      "Epoch 931/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0042\n",
      "Epoch 932/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0040\n",
      "Epoch 933/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0047\n",
      "Epoch 934/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0038\n",
      "Epoch 935/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0039\n",
      "Epoch 936/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0036\n",
      "Epoch 937/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0038\n",
      "Epoch 938/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0036\n",
      "Epoch 939/1000\n",
      "450/450 [==============================] - 0s 243us/sample - loss: 0.0032\n",
      "Epoch 940/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0035\n",
      "Epoch 941/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0036\n",
      "Epoch 942/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0034\n",
      "Epoch 943/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0037\n",
      "Epoch 944/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0037\n",
      "Epoch 945/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0036\n",
      "Epoch 946/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0034\n",
      "Epoch 947/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0036\n",
      "Epoch 948/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0036\n",
      "Epoch 949/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0041\n",
      "Epoch 950/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0044\n",
      "Epoch 951/1000\n",
      "450/450 [==============================] - 0s 237us/sample - loss: 0.0038\n",
      "Epoch 952/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0038\n",
      "Epoch 953/1000\n",
      "450/450 [==============================] - 0s 229us/sample - loss: 0.0036\n",
      "Epoch 954/1000\n",
      "450/450 [==============================] - 0s 236us/sample - loss: 0.0033\n",
      "Epoch 955/1000\n",
      "450/450 [==============================] - 0s 244us/sample - loss: 0.0032\n",
      "Epoch 956/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0037\n",
      "Epoch 957/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0035\n",
      "Epoch 958/1000\n",
      "450/450 [==============================] - 0s 230us/sample - loss: 0.0035\n",
      "Epoch 959/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0034\n",
      "Epoch 960/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0036\n",
      "Epoch 961/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0036\n",
      "Epoch 962/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0037\n",
      "Epoch 963/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0039\n",
      "Epoch 964/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0036\n",
      "Epoch 965/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0039\n",
      "Epoch 966/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0037\n",
      "Epoch 967/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0038\n",
      "Epoch 968/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0035\n",
      "Epoch 969/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0035\n",
      "Epoch 970/1000\n",
      "450/450 [==============================] - 0s 233us/sample - loss: 0.0034\n",
      "Epoch 971/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0032\n",
      "Epoch 972/1000\n",
      "450/450 [==============================] - 0s 224us/sample - loss: 0.0034\n",
      "Epoch 973/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0032\n",
      "Epoch 974/1000\n",
      "450/450 [==============================] - 0s 219us/sample - loss: 0.0034\n",
      "Epoch 975/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0035\n",
      "Epoch 976/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0043\n",
      "Epoch 977/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0044\n",
      "Epoch 978/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0044\n",
      "Epoch 979/1000\n",
      "450/450 [==============================] - 0s 235us/sample - loss: 0.0046\n",
      "Epoch 980/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0037\n",
      "Epoch 981/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0037\n",
      "Epoch 982/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0040\n",
      "Epoch 983/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0036\n",
      "Epoch 984/1000\n",
      "450/450 [==============================] - 0s 217us/sample - loss: 0.0042\n",
      "Epoch 985/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0043\n",
      "Epoch 986/1000\n",
      "450/450 [==============================] - 0s 232us/sample - loss: 0.0042\n",
      "Epoch 987/1000\n",
      "450/450 [==============================] - 0s 216us/sample - loss: 0.0036\n",
      "Epoch 988/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0038\n",
      "Epoch 989/1000\n",
      "450/450 [==============================] - 0s 218us/sample - loss: 0.0037\n",
      "Epoch 990/1000\n",
      "450/450 [==============================] - 0s 228us/sample - loss: 0.0037\n",
      "Epoch 991/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0037\n",
      "Epoch 992/1000\n",
      "450/450 [==============================] - 0s 214us/sample - loss: 0.0043\n",
      "Epoch 993/1000\n",
      "450/450 [==============================] - 0s 222us/sample - loss: 0.0042\n",
      "Epoch 994/1000\n",
      "450/450 [==============================] - 0s 221us/sample - loss: 0.0041\n",
      "Epoch 995/1000\n",
      "450/450 [==============================] - 0s 231us/sample - loss: 0.0037\n",
      "Epoch 996/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0036\n",
      "Epoch 997/1000\n",
      "450/450 [==============================] - 0s 220us/sample - loss: 0.0037\n",
      "Epoch 998/1000\n",
      "450/450 [==============================] - 0s 223us/sample - loss: 0.0038\n",
      "Epoch 999/1000\n",
      "450/450 [==============================] - 0s 225us/sample - loss: 0.0034\n",
      "Epoch 1000/1000\n",
      "450/450 [==============================] - 0s 226us/sample - loss: 0.0036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10301db70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 20, 40)\n",
    "X_valid = X_valid.reshape(-1, 20, 40)\n",
    "X_test = X_test.reshape(-1, 20, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coded = encoder.predict(X_train)\n",
    "X_valid_coded = encoder.predict(X_valid)        \n",
    "X_test_coded = encoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coded = X_train_coded.reshape(9000, -1)\n",
    "X_valid_coded = X_valid_coded.reshape(200, -1)\n",
    "X_test_coded = X_test_coded.reshape(300, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(n_hidden=2, nodes=30, input_shape=X_train_coded.shape[1:], activation=\"selu\", rate=0.2, optimizer=\"Nadam\"):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(nodes, input_shape=input_shape))\n",
    "    if activation == \"selu\":\n",
    "        initializer = \"lecun_normal\"\n",
    "    else:\n",
    "        initializer = \"he_normal\"\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(nodes,  kernel_initializer=initializer, activation=activation))\n",
    "    if activation == \"selu\":\n",
    "        model.add(MCAlphaDropout(rate=rate))\n",
    "    else:\n",
    "        model.add(MCDropout(rate=rate))\n",
    "    model.add(keras.layers.Dense(1, activation=\"tanh\"))\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    #early_stp = keras.callbacks.EarlyStopping(patience=60, restore_best_weights=True)\n",
    "    #history = model.fit(X_train_enc_, y_train, epochs=1000, batch_size=params['batch_size'], callbacks=[early_stp], validation_data=[X_valid_enc_, y_valid])\n",
    "    #y_pred  = model.predict(X_valid_enc_)\n",
    "    #loss = mean_squared_error(y_valid, y_pred)\n",
    "    return model\n",
    "    \n",
    "space = {\"nodes\": [10, 20, 30, 40, 50, 100, 150, 200, 250, 300, 500], \n",
    "        \"activation\": [\"selu\", \"elu\", \"relu\"], \n",
    "        \"rate\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.6], \n",
    "        \"optimizer\": [\"RMSprop\", \"Adam\", \"Nadam\"],  \n",
    "        \"n_hidden\": [1, 2, 3, 4, 5, 6]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_reg = keras.wrappers.scikit_learn.KerasRegressor(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "9000/9000 [==============================] - 1s 116us/sample - loss: 0.1642 - val_loss: 0.0314\n",
      "Epoch 2/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0100 - val_loss: 0.0041\n",
      "Epoch 3/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 4/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0015 - val_loss: 0.0011\n",
      "Epoch 5/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0011 - val_loss: 6.3306e-04\n",
      "Epoch 6/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 9.4686e-04 - val_loss: 0.0011\n",
      "Epoch 7/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 8.7301e-04 - val_loss: 7.2160e-04\n",
      "Epoch 8/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 8.0676e-04 - val_loss: 3.4990e-04\n",
      "Epoch 9/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 7.6103e-04 - val_loss: 2.1250e-04\n",
      "Epoch 10/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 7.5388e-04 - val_loss: 6.4540e-04\n",
      "Epoch 11/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 7.3006e-04 - val_loss: 2.9664e-04\n",
      "Epoch 12/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 7.1393e-04 - val_loss: 1.8659e-04\n",
      "Epoch 13/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 7.1323e-04 - val_loss: 2.9441e-04\n",
      "Epoch 14/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 7.1224e-04 - val_loss: 1.6391e-04\n",
      "Epoch 15/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 7.0126e-04 - val_loss: 1.9496e-04\n",
      "Epoch 16/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.9685e-04 - val_loss: 1.2161e-04\n",
      "Epoch 17/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.9294e-04 - val_loss: 4.6784e-04\n",
      "Epoch 18/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.9067e-04 - val_loss: 1.3309e-04\n",
      "Epoch 19/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.8137e-04 - val_loss: 1.8207e-04\n",
      "Epoch 20/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.7440e-04 - val_loss: 3.6321e-04\n",
      "Epoch 21/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.7975e-04 - val_loss: 1.2448e-04\n",
      "Epoch 22/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.6943e-04 - val_loss: 3.7963e-04\n",
      "Epoch 23/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.6953e-04 - val_loss: 1.3924e-04\n",
      "Epoch 24/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.6358e-04 - val_loss: 2.6180e-04\n",
      "Epoch 25/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.6830e-04 - val_loss: 2.6322e-04\n",
      "Epoch 26/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.6852e-04 - val_loss: 1.1073e-04\n",
      "Epoch 27/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.6249e-04 - val_loss: 1.4164e-04\n",
      "Epoch 28/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.6404e-04 - val_loss: 1.6484e-04\n",
      "Epoch 29/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.6148e-04 - val_loss: 1.0419e-04\n",
      "Epoch 30/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.6059e-04 - val_loss: 1.2745e-04\n",
      "Epoch 31/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.5770e-04 - val_loss: 7.1531e-04\n",
      "Epoch 32/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.5393e-04 - val_loss: 1.3409e-04\n",
      "Epoch 33/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.6189e-04 - val_loss: 1.0131e-04\n",
      "Epoch 34/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.5249e-04 - val_loss: 1.4443e-04\n",
      "Epoch 35/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.5091e-04 - val_loss: 1.7819e-04\n",
      "Epoch 36/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.5859e-04 - val_loss: 1.4275e-04\n",
      "Epoch 37/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.5445e-04 - val_loss: 1.0831e-04\n",
      "Epoch 38/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.4850e-04 - val_loss: 1.5132e-04\n",
      "Epoch 39/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.5716e-04 - val_loss: 1.4192e-04\n",
      "Epoch 40/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 6.4889e-04 - val_loss: 1.2502e-04\n",
      "Epoch 41/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.5300e-04 - val_loss: 5.8336e-04\n",
      "Epoch 42/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.5209e-04 - val_loss: 1.2962e-04\n",
      "Epoch 43/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4668e-04 - val_loss: 1.2572e-04\n",
      "Epoch 44/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.5169e-04 - val_loss: 1.3979e-04\n",
      "Epoch 45/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4979e-04 - val_loss: 1.2580e-04\n",
      "Epoch 46/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.4521e-04 - val_loss: 1.2835e-04\n",
      "Epoch 47/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4689e-04 - val_loss: 1.9761e-04\n",
      "Epoch 48/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4794e-04 - val_loss: 1.5193e-04\n",
      "Epoch 49/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.4694e-04 - val_loss: 1.3962e-04\n",
      "Epoch 50/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.4096e-04 - val_loss: 1.3905e-04\n",
      "Epoch 51/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4341e-04 - val_loss: 1.6076e-04\n",
      "Epoch 52/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.4623e-04 - val_loss: 1.3046e-04\n",
      "Epoch 53/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.3576e-04 - val_loss: 1.6652e-04\n",
      "Epoch 54/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.4752e-04 - val_loss: 5.8890e-04\n",
      "Epoch 55/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.3754e-04 - val_loss: 3.5790e-04\n",
      "Epoch 56/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4321e-04 - val_loss: 1.2719e-04\n",
      "Epoch 57/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.3967e-04 - val_loss: 2.4531e-04\n",
      "Epoch 58/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.3740e-04 - val_loss: 1.2412e-04\n",
      "Epoch 59/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3794e-04 - val_loss: 1.5490e-04\n",
      "Epoch 60/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.4892e-04 - val_loss: 2.1950e-04\n",
      "Epoch 61/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.4541e-04 - val_loss: 1.1554e-04\n",
      "Epoch 62/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.4512e-04 - val_loss: 2.1805e-04\n",
      "Epoch 63/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3667e-04 - val_loss: 1.4459e-04\n",
      "Epoch 64/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4615e-04 - val_loss: 5.5584e-04\n",
      "Epoch 65/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.4677e-04 - val_loss: 1.6930e-04\n",
      "Epoch 66/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.4443e-04 - val_loss: 1.4540e-04\n",
      "Epoch 67/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.4295e-04 - val_loss: 1.9794e-04\n",
      "Epoch 68/1000\n",
      "9000/9000 [==============================] - 1s 64us/sample - loss: 6.4094e-04 - val_loss: 1.4063e-04\n",
      "Epoch 69/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.1824e-04 - val_loss: 3.0012e-04\n",
      "Epoch 70/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.2830e-04 - val_loss: 1.2151e-04\n",
      "Epoch 71/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.3548e-04 - val_loss: 7.3693e-04\n",
      "Epoch 72/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3067e-04 - val_loss: 6.1682e-04\n",
      "Epoch 73/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.7942e-04 - val_loss: 1.1483e-04\n",
      "Epoch 74/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.4248e-04 - val_loss: 2.4594e-04\n",
      "Epoch 75/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.3669e-04 - val_loss: 1.7968e-04\n",
      "Epoch 76/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3366e-04 - val_loss: 3.3805e-04\n",
      "Epoch 77/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 6.3047e-04 - val_loss: 3.4295e-04\n",
      "Epoch 78/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 6.3068e-04 - val_loss: 2.2144e-04\n",
      "Epoch 79/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.4193e-04 - val_loss: 3.2883e-04\n",
      "Epoch 80/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3414e-04 - val_loss: 1.4848e-04\n",
      "Epoch 81/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.2054e-04 - val_loss: 5.3464e-04\n",
      "Epoch 82/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.3076e-04 - val_loss: 1.5346e-04\n",
      "Epoch 83/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.2668e-04 - val_loss: 3.1832e-04\n",
      "Epoch 84/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 6.2283e-04 - val_loss: 3.8570e-04\n",
      "Epoch 85/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3166e-04 - val_loss: 1.7056e-04\n",
      "Epoch 86/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 6.4422e-04 - val_loss: 1.5824e-04\n",
      "Epoch 87/1000\n",
      "9000/9000 [==============================] - 0s 56us/sample - loss: 6.2661e-04 - val_loss: 1.4610e-04\n",
      "Epoch 88/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.3240e-04 - val_loss: 2.3691e-04\n",
      "Epoch 89/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 6.3648e-04 - val_loss: 3.3038e-04\n",
      "Epoch 90/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 6.2789e-04 - val_loss: 7.6708e-04\n",
      "Epoch 91/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.5476e-04 - val_loss: 1.5495e-04\n",
      "Epoch 92/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 6.3689e-04 - val_loss: 5.0447e-04\n",
      "Epoch 93/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.3140e-04 - val_loss: 1.7730e-04\n",
      "Epoch 94/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.2564e-04 - val_loss: 7.5369e-04\n",
      "Epoch 95/1000\n",
      "9000/9000 [==============================] - 1s 66us/sample - loss: 6.2467e-04 - val_loss: 3.1671e-04\n",
      "Epoch 96/1000\n",
      "9000/9000 [==============================] - 1s 65us/sample - loss: 6.1735e-04 - val_loss: 7.6402e-04\n",
      "Epoch 97/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 6.1669e-04 - val_loss: 1.6509e-04\n",
      "Epoch 98/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.3918e-04 - val_loss: 1.7413e-04\n",
      "Epoch 99/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.2153e-04 - val_loss: 2.3944e-04\n",
      "Epoch 100/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.1103e-04 - val_loss: 4.7886e-04\n",
      "Epoch 101/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.0993e-04 - val_loss: 3.1673e-04\n",
      "Epoch 102/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.1025e-04 - val_loss: 1.1336e-04\n",
      "Epoch 103/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3278e-04 - val_loss: 1.8501e-04\n",
      "Epoch 104/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 6.3286e-04 - val_loss: 1.7523e-04\n",
      "Epoch 105/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.2750e-04 - val_loss: 2.1124e-04\n",
      "Epoch 106/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.3329e-04 - val_loss: 1.8933e-04\n",
      "Epoch 107/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 6.1459e-04 - val_loss: 3.6134e-04\n",
      "Epoch 108/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.2219e-04 - val_loss: 1.8620e-04\n",
      "Epoch 109/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3528e-04 - val_loss: 1.3079e-04\n",
      "Epoch 110/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.2295e-04 - val_loss: 1.9885e-04\n",
      "Epoch 111/1000\n",
      "9000/9000 [==============================] - 1s 72us/sample - loss: 6.3512e-04 - val_loss: 2.2004e-04\n",
      "Epoch 112/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.1708e-04 - val_loss: 2.8673e-04\n",
      "Epoch 113/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.2940e-04 - val_loss: 5.0817e-04\n",
      "Epoch 114/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.3917e-04 - val_loss: 2.3376e-04\n",
      "Epoch 115/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.2898e-04 - val_loss: 2.4119e-04\n",
      "Epoch 116/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.0532e-04 - val_loss: 2.5606e-04\n",
      "Epoch 117/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.3643e-04 - val_loss: 1.9928e-04\n",
      "Epoch 118/1000\n",
      "9000/9000 [==============================] - 1s 69us/sample - loss: 6.1807e-04 - val_loss: 3.1496e-04\n",
      "Epoch 119/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.2099e-04 - val_loss: 3.2464e-04\n",
      "Epoch 120/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.1250e-04 - val_loss: 1.5482e-04\n",
      "Epoch 121/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.2140e-04 - val_loss: 1.5416e-04\n",
      "Epoch 122/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.2547e-04 - val_loss: 1.6272e-04\n",
      "Epoch 123/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.2648e-04 - val_loss: 1.9295e-04\n",
      "Epoch 124/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3319e-04 - val_loss: 2.2400e-04\n",
      "Epoch 125/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.1595e-04 - val_loss: 3.1443e-04\n",
      "Epoch 126/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3482e-04 - val_loss: 1.6976e-04\n",
      "Epoch 127/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.3171e-04 - val_loss: 1.9058e-04\n",
      "Epoch 128/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 6.1979e-04 - val_loss: 3.2902e-04\n",
      "Epoch 129/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 6.1871e-04 - val_loss: 1.9448e-04\n",
      "Epoch 130/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 6.1111e-04 - val_loss: 2.4732e-04\n",
      "Epoch 131/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 6.0488e-04 - val_loss: 2.1241e-04\n",
      "Epoch 132/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.0776e-04 - val_loss: 4.4177e-04\n",
      "Epoch 133/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.1466e-04 - val_loss: 3.9160e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x10225b6a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_reg.fit(X_train_coded, y_train, epochs=1000, validation_data=(X_valid_coded, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomizedSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-68635de87fcb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnd_search_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m rnd_search_cv.fit(X_train, y_train, epochs=1000, validation_data=(X_valid, y_valid), \n\u001b[1;32m      3\u001b[0m                   callbacks=[keras.callbacks.EarlyStopping(patience=50)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RandomizedSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "rnd_search_cv = RandomizedSearchCV(nn_reg, space, n_iter=100, cv=3)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=1000, validation_data=(X_valid, y_valid), \n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnd_search_cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-81f53dbb141a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnd_search_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rnd_search_cv' is not defined"
     ]
    }
   ],
   "source": [
    "rnd_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 75us/sample - loss: 1.8187e-04\n"
     ]
    }
   ],
   "source": [
    "mse_test = keras_reg.score(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "9000/9000 [==============================] - 5s 582us/sample - loss: 0.0231 - val_loss: 0.0807\n",
      "Epoch 2/1000\n",
      "9000/9000 [==============================] - 2s 259us/sample - loss: 0.0093 - val_loss: 0.0653\n",
      "Epoch 3/1000\n",
      "9000/9000 [==============================] - 2s 228us/sample - loss: 0.0073 - val_loss: 0.0621\n",
      "Epoch 4/1000\n",
      "9000/9000 [==============================] - 2s 210us/sample - loss: 0.0065 - val_loss: 0.0600\n",
      "Epoch 5/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0060 - val_loss: 0.0590\n",
      "Epoch 6/1000\n",
      "9000/9000 [==============================] - 2s 202us/sample - loss: 0.0057 - val_loss: 0.0584\n",
      "Epoch 7/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0054 - val_loss: 0.0557\n",
      "Epoch 8/1000\n",
      "9000/9000 [==============================] - 2s 213us/sample - loss: 0.0051 - val_loss: 0.0583\n",
      "Epoch 9/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0050 - val_loss: 0.0574\n",
      "Epoch 10/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0048 - val_loss: 0.0565\n",
      "Epoch 11/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0047 - val_loss: 0.0550\n",
      "Epoch 12/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0046 - val_loss: 0.0534\n",
      "Epoch 13/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0045 - val_loss: 0.0524\n",
      "Epoch 14/1000\n",
      "9000/9000 [==============================] - 2s 205us/sample - loss: 0.0044 - val_loss: 0.0539\n",
      "Epoch 15/1000\n",
      "9000/9000 [==============================] - 2s 205us/sample - loss: 0.0044 - val_loss: 0.0529\n",
      "Epoch 16/1000\n",
      "9000/9000 [==============================] - 2s 200us/sample - loss: 0.0044 - val_loss: 0.0523\n",
      "Epoch 17/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0044 - val_loss: 0.0530\n",
      "Epoch 18/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0042 - val_loss: 0.0549\n",
      "Epoch 19/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0044 - val_loss: 0.0520\n",
      "Epoch 20/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0042 - val_loss: 0.0508\n",
      "Epoch 21/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0043 - val_loss: 0.0526\n",
      "Epoch 22/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0041 - val_loss: 0.0513\n",
      "Epoch 23/1000\n",
      "9000/9000 [==============================] - 2s 201us/sample - loss: 0.0043 - val_loss: 0.0518\n",
      "Epoch 24/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0043 - val_loss: 0.0526\n",
      "Epoch 25/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0041 - val_loss: 0.0517\n",
      "Epoch 26/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0040 - val_loss: 0.0520\n",
      "Epoch 27/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0042 - val_loss: 0.0518\n",
      "Epoch 28/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0040 - val_loss: 0.0505\n",
      "Epoch 29/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0042 - val_loss: 0.0507\n",
      "Epoch 30/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0039 - val_loss: 0.0515\n",
      "Epoch 31/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0040 - val_loss: 0.0512\n",
      "Epoch 32/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0041 - val_loss: 0.0505\n",
      "Epoch 33/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0040 - val_loss: 0.0501\n",
      "Epoch 34/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0041 - val_loss: 0.0518\n",
      "Epoch 35/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0039 - val_loss: 0.0503\n",
      "Epoch 36/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0040 - val_loss: 0.0527\n",
      "Epoch 37/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0040 - val_loss: 0.0505\n",
      "Epoch 38/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0040 - val_loss: 0.0506\n",
      "Epoch 39/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0040 - val_loss: 0.0506\n",
      "Epoch 40/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0040 - val_loss: 0.0501\n",
      "Epoch 41/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0039 - val_loss: 0.0499\n",
      "Epoch 42/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0040 - val_loss: 0.0499\n",
      "Epoch 43/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0040 - val_loss: 0.0519\n",
      "Epoch 44/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0039 - val_loss: 0.0499\n",
      "Epoch 45/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0039 - val_loss: 0.0504\n",
      "Epoch 46/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0039 - val_loss: 0.0502\n",
      "Epoch 47/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0039 - val_loss: 0.0500\n",
      "Epoch 48/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0039 - val_loss: 0.0498\n",
      "Epoch 49/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0039 - val_loss: 0.0496\n",
      "Epoch 50/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0039 - val_loss: 0.0493\n",
      "Epoch 51/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0038 - val_loss: 0.0509\n",
      "Epoch 52/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0040 - val_loss: 0.0502\n",
      "Epoch 53/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0039 - val_loss: 0.0498\n",
      "Epoch 54/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0039 - val_loss: 0.0496\n",
      "Epoch 55/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0037 - val_loss: 0.0500\n",
      "Epoch 56/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0037 - val_loss: 0.0493\n",
      "Epoch 57/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0038 - val_loss: 0.0496\n",
      "Epoch 58/1000\n",
      "9000/9000 [==============================] - 2s 207us/sample - loss: 0.0039 - val_loss: 0.0506\n",
      "Epoch 59/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0038 - val_loss: 0.0496\n",
      "Epoch 60/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0038 - val_loss: 0.0490\n",
      "Epoch 61/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0038 - val_loss: 0.0496\n",
      "Epoch 62/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0038 - val_loss: 0.0498\n",
      "Epoch 63/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0039 - val_loss: 0.0497\n",
      "Epoch 64/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0038 - val_loss: 0.0510\n",
      "Epoch 65/1000\n",
      "9000/9000 [==============================] - 2s 218us/sample - loss: 0.0039 - val_loss: 0.0490\n",
      "Epoch 66/1000\n",
      "9000/9000 [==============================] - 2s 202us/sample - loss: 0.0039 - val_loss: 0.0501\n",
      "Epoch 67/1000\n",
      "9000/9000 [==============================] - 2s 200us/sample - loss: 0.0039 - val_loss: 0.0488\n",
      "Epoch 68/1000\n",
      "9000/9000 [==============================] - 2s 203us/sample - loss: 0.0038 - val_loss: 0.0497\n",
      "Epoch 69/1000\n",
      "9000/9000 [==============================] - 2s 208us/sample - loss: 0.0038 - val_loss: 0.0495\n",
      "Epoch 70/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0038 - val_loss: 0.0494\n",
      "Epoch 71/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0038 - val_loss: 0.0492\n",
      "Epoch 72/1000\n",
      "9000/9000 [==============================] - 2s 212us/sample - loss: 0.0038 - val_loss: 0.0501\n",
      "Epoch 73/1000\n",
      "9000/9000 [==============================] - 2s 205us/sample - loss: 0.0038 - val_loss: 0.0513\n",
      "Epoch 74/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0038 - val_loss: 0.0488\n",
      "Epoch 75/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0038 - val_loss: 0.0491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0038 - val_loss: 0.0495\n",
      "Epoch 77/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0038 - val_loss: 0.0494\n",
      "Epoch 78/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0037 - val_loss: 0.0487\n",
      "Epoch 79/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0039 - val_loss: 0.0491\n",
      "Epoch 80/1000\n",
      "9000/9000 [==============================] - 2s 203us/sample - loss: 0.0038 - val_loss: 0.0504\n",
      "Epoch 81/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0038 - val_loss: 0.0495\n",
      "Epoch 82/1000\n",
      "9000/9000 [==============================] - 2s 199us/sample - loss: 0.0037 - val_loss: 0.0489\n",
      "Epoch 83/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0038 - val_loss: 0.0491\n",
      "Epoch 84/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0039 - val_loss: 0.0496\n",
      "Epoch 85/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0038 - val_loss: 0.0501\n",
      "Epoch 86/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0038 - val_loss: 0.0493\n",
      "Epoch 87/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0038 - val_loss: 0.0492\n",
      "Epoch 88/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0037 - val_loss: 0.0501\n",
      "Epoch 89/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0038 - val_loss: 0.0492\n",
      "Epoch 90/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0037 - val_loss: 0.0494\n",
      "Epoch 91/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0038 - val_loss: 0.0504\n",
      "Epoch 92/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0037 - val_loss: 0.0488\n",
      "Epoch 93/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0036 - val_loss: 0.0493\n",
      "Epoch 94/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0037 - val_loss: 0.0495\n",
      "Epoch 95/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0037 - val_loss: 0.0508\n",
      "Epoch 96/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0037 - val_loss: 0.0500\n",
      "Epoch 97/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0037 - val_loss: 0.0499\n",
      "Epoch 98/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0037 - val_loss: 0.0497\n",
      "Epoch 99/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0036 - val_loss: 0.0503\n",
      "Epoch 100/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0036 - val_loss: 0.0493\n",
      "Epoch 101/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0038 - val_loss: 0.0495\n",
      "Epoch 102/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0036 - val_loss: 0.0485\n",
      "Epoch 103/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0038 - val_loss: 0.0510\n",
      "Epoch 104/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0037 - val_loss: 0.0494\n",
      "Epoch 105/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0036 - val_loss: 0.0499\n",
      "Epoch 106/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0036 - val_loss: 0.0499\n",
      "Epoch 107/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0038 - val_loss: 0.0504\n",
      "Epoch 108/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0037 - val_loss: 0.0493\n",
      "Epoch 109/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0036 - val_loss: 0.0489\n",
      "Epoch 110/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0037 - val_loss: 0.0491\n",
      "Epoch 111/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0036 - val_loss: 0.0495\n",
      "Epoch 112/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0488\n",
      "Epoch 113/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0035 - val_loss: 0.0494\n",
      "Epoch 114/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0038 - val_loss: 0.0496\n",
      "Epoch 115/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0037 - val_loss: 0.0503\n",
      "Epoch 116/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0038 - val_loss: 0.0507\n",
      "Epoch 117/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0037 - val_loss: 0.0499\n",
      "Epoch 118/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 119/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0503\n",
      "Epoch 120/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0487\n",
      "Epoch 121/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0490\n",
      "Epoch 122/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0036 - val_loss: 0.0487\n",
      "Epoch 123/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0037 - val_loss: 0.0496\n",
      "Epoch 124/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0036 - val_loss: 0.0498\n",
      "Epoch 125/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0037 - val_loss: 0.0493\n",
      "Epoch 126/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0036 - val_loss: 0.0491\n",
      "Epoch 127/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0037 - val_loss: 0.0489\n",
      "Epoch 128/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0497\n",
      "Epoch 129/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0503\n",
      "Epoch 130/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0037 - val_loss: 0.0505\n",
      "Epoch 131/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0037 - val_loss: 0.0506\n",
      "Epoch 132/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0037 - val_loss: 0.0511\n",
      "Epoch 133/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0502\n",
      "Epoch 134/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 135/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0037 - val_loss: 0.0489\n",
      "Epoch 136/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0500\n",
      "Epoch 137/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0038 - val_loss: 0.0488\n",
      "Epoch 138/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0035 - val_loss: 0.0500\n",
      "Epoch 139/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0507\n",
      "Epoch 140/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0492\n",
      "Epoch 141/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0035 - val_loss: 0.0497\n",
      "Epoch 142/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0036 - val_loss: 0.0492\n",
      "Epoch 143/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0496\n",
      "Epoch 144/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0037 - val_loss: 0.0504\n",
      "Epoch 145/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0036 - val_loss: 0.0504\n",
      "Epoch 146/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0034 - val_loss: 0.0493\n",
      "Epoch 147/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0503\n",
      "Epoch 148/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0035 - val_loss: 0.0495\n",
      "Epoch 149/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0035 - val_loss: 0.0494\n",
      "Epoch 150/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0036 - val_loss: 0.0502\n",
      "Epoch 151/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0499\n",
      "Epoch 152/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0035 - val_loss: 0.0498\n",
      "Epoch 153/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0036 - val_loss: 0.0508\n",
      "Epoch 154/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0036 - val_loss: 0.0495\n",
      "Epoch 155/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0036 - val_loss: 0.0497\n",
      "Epoch 156/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0036 - val_loss: 0.0495\n",
      "Epoch 157/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0035 - val_loss: 0.0511\n",
      "Epoch 158/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0035 - val_loss: 0.0503\n",
      "Epoch 159/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0035 - val_loss: 0.0494\n",
      "Epoch 160/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0036 - val_loss: 0.0491\n",
      "Epoch 161/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0037 - val_loss: 0.0497\n",
      "Epoch 162/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 163/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 164/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0035 - val_loss: 0.0499\n",
      "Epoch 165/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0491\n",
      "Epoch 166/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 167/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0035 - val_loss: 0.0493\n",
      "Epoch 168/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0036 - val_loss: 0.0493\n",
      "Epoch 169/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0034 - val_loss: 0.0493\n",
      "Epoch 170/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0492\n",
      "Epoch 171/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0502\n",
      "Epoch 172/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0035 - val_loss: 0.0501\n",
      "Epoch 173/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0497\n",
      "Epoch 174/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0035 - val_loss: 0.0508\n",
      "Epoch 175/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0035 - val_loss: 0.0491\n",
      "Epoch 176/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0035 - val_loss: 0.0495\n",
      "Epoch 177/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0036 - val_loss: 0.0504\n",
      "Epoch 178/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0034 - val_loss: 0.0494\n",
      "Epoch 179/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0035 - val_loss: 0.0499\n",
      "Epoch 180/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0036 - val_loss: 0.0492\n",
      "Epoch 181/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0036 - val_loss: 0.0499\n",
      "Epoch 182/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0035 - val_loss: 0.0496\n",
      "Epoch 183/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 184/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0036 - val_loss: 0.0494\n",
      "Epoch 185/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0035 - val_loss: 0.0497\n",
      "Epoch 186/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0034 - val_loss: 0.0501\n",
      "Epoch 187/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0035 - val_loss: 0.0496\n",
      "Epoch 188/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0034 - val_loss: 0.0513\n",
      "Epoch 189/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0036 - val_loss: 0.0501\n",
      "Epoch 190/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 191/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0036 - val_loss: 0.0493\n",
      "Epoch 192/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0035 - val_loss: 0.0492\n",
      "Epoch 193/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0035 - val_loss: 0.0509\n",
      "Epoch 194/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0037 - val_loss: 0.0496\n",
      "Epoch 195/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0035 - val_loss: 0.0525\n",
      "Epoch 196/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0036 - val_loss: 0.0499\n",
      "Epoch 197/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0035 - val_loss: 0.0491\n",
      "Epoch 198/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0035 - val_loss: 0.0492\n",
      "Epoch 199/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0036 - val_loss: 0.0502\n",
      "Epoch 200/1000\n",
      "9000/9000 [==============================] - 2s 205us/sample - loss: 0.0035 - val_loss: 0.0502\n",
      "Epoch 201/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0035 - val_loss: 0.0497\n",
      "Epoch 202/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0035 - val_loss: 0.0496\n",
      "Epoch 203/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0035 - val_loss: 0.0498\n",
      "Epoch 204/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0035 - val_loss: 0.0496\n",
      "Epoch 205/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0501\n",
      "Epoch 206/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0035 - val_loss: 0.0498\n",
      "Epoch 207/1000\n",
      "9000/9000 [==============================] - 2s 199us/sample - loss: 0.0035 - val_loss: 0.0494\n",
      "Epoch 208/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0490\n",
      "Epoch 209/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 210/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0510\n",
      "Epoch 211/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0036 - val_loss: 0.0497\n",
      "Epoch 212/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0035 - val_loss: 0.0496\n",
      "Epoch 213/1000\n",
      "9000/9000 [==============================] - 2s 202us/sample - loss: 0.0034 - val_loss: 0.0491\n",
      "Epoch 214/1000\n",
      "9000/9000 [==============================] - 2s 211us/sample - loss: 0.0035 - val_loss: 0.0508\n",
      "Epoch 215/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0035 - val_loss: 0.0497\n",
      "Epoch 216/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0034 - val_loss: 0.0500\n",
      "Epoch 217/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0491\n",
      "Epoch 218/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0035 - val_loss: 0.0504\n",
      "Epoch 219/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0500\n",
      "Epoch 220/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 221/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0035 - val_loss: 0.0502\n",
      "Epoch 222/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0035 - val_loss: 0.0495\n",
      "Epoch 223/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 224/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 225/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0035 - val_loss: 0.0505\n",
      "Epoch 226/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0035 - val_loss: 0.0493\n",
      "Epoch 227/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0035 - val_loss: 0.0500\n",
      "Epoch 228/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0034 - val_loss: 0.0491\n",
      "Epoch 229/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0035 - val_loss: 0.0501\n",
      "Epoch 230/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0035 - val_loss: 0.0499\n",
      "Epoch 231/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0035 - val_loss: 0.0499\n",
      "Epoch 232/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 233/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 234/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0035 - val_loss: 0.0495\n",
      "Epoch 235/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0034 - val_loss: 0.0491\n",
      "Epoch 236/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0500\n",
      "Epoch 237/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0501\n",
      "Epoch 238/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0494\n",
      "Epoch 239/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0511\n",
      "Epoch 240/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0035 - val_loss: 0.0506\n",
      "Epoch 241/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 242/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 243/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 244/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 245/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 246/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0034 - val_loss: 0.0495\n",
      "Epoch 247/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 248/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0495\n",
      "Epoch 249/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0502\n",
      "Epoch 250/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 251/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0506\n",
      "Epoch 252/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0035 - val_loss: 0.0499\n",
      "Epoch 253/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0034 - val_loss: 0.0504\n",
      "Epoch 254/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0035 - val_loss: 0.0492\n",
      "Epoch 255/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 256/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0035 - val_loss: 0.0495\n",
      "Epoch 257/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 258/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 259/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 260/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0034 - val_loss: 0.0505\n",
      "Epoch 261/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 262/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0510\n",
      "Epoch 263/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0035 - val_loss: 0.0504\n",
      "Epoch 264/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0034 - val_loss: 0.0504\n",
      "Epoch 265/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 266/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0034 - val_loss: 0.0492\n",
      "Epoch 267/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0507\n",
      "Epoch 268/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0497\n",
      "Epoch 269/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0509\n",
      "Epoch 270/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 271/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0034 - val_loss: 0.0517\n",
      "Epoch 272/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0508\n",
      "Epoch 273/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0495\n",
      "Epoch 274/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0501\n",
      "Epoch 275/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 276/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0035 - val_loss: 0.0498\n",
      "Epoch 277/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0034 - val_loss: 0.0493\n",
      "Epoch 278/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 279/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0503\n",
      "Epoch 280/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 281/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 282/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 283/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0501\n",
      "Epoch 284/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0501\n",
      "Epoch 285/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 286/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 287/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0504\n",
      "Epoch 288/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 289/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 290/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0035 - val_loss: 0.0493\n",
      "Epoch 291/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 292/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0500\n",
      "Epoch 293/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 294/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 295/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0034 - val_loss: 0.0503\n",
      "Epoch 296/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0033 - val_loss: 0.0508\n",
      "Epoch 297/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 298/1000\n",
      "9000/9000 [==============================] - 2s 210us/sample - loss: 0.0034 - val_loss: 0.0506\n",
      "Epoch 299/1000\n",
      "9000/9000 [==============================] - 2s 206us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 300/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0505\n",
      "Epoch 301/1000\n",
      "9000/9000 [==============================] - 2s 200us/sample - loss: 0.0035 - val_loss: 0.0502\n",
      "Epoch 302/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 303/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 304/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0521\n",
      "Epoch 305/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0034 - val_loss: 0.0503\n",
      "Epoch 306/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 307/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 308/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 309/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0510\n",
      "Epoch 310/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 311/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 312/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0035 - val_loss: 0.0508\n",
      "Epoch 313/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 314/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0033 - val_loss: 0.0512\n",
      "Epoch 315/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0034 - val_loss: 0.0500\n",
      "Epoch 316/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 317/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 318/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 319/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 320/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0511\n",
      "Epoch 321/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0503\n",
      "Epoch 322/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 323/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0506\n",
      "Epoch 324/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 325/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0505\n",
      "Epoch 326/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 327/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0035 - val_loss: 0.0500\n",
      "Epoch 328/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0504\n",
      "Epoch 329/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 330/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 331/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0512\n",
      "Epoch 332/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 333/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 334/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0034 - val_loss: 0.0503\n",
      "Epoch 335/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0511\n",
      "Epoch 336/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0034 - val_loss: 0.0494\n",
      "Epoch 337/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 338/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 339/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 340/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 341/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0033 - val_loss: 0.0505\n",
      "Epoch 342/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 343/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 344/1000\n",
      "9000/9000 [==============================] - 2s 204us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 345/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0033 - val_loss: 0.0505\n",
      "Epoch 346/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 347/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 348/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 349/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0035 - val_loss: 0.0507\n",
      "Epoch 350/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0519\n",
      "Epoch 351/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 352/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 353/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0498\n",
      "Epoch 354/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0034 - val_loss: 0.0501\n",
      "Epoch 355/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0034 - val_loss: 0.0501\n",
      "Epoch 356/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0508\n",
      "Epoch 357/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 358/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 359/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 360/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 361/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 362/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0507\n",
      "Epoch 363/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 364/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 365/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0505\n",
      "Epoch 366/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 367/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 368/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 369/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0493\n",
      "Epoch 370/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 371/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 372/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 373/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 374/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 375/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0496\n",
      "Epoch 376/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0034 - val_loss: 0.0502\n",
      "Epoch 377/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 378/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 379/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0032 - val_loss: 0.0504\n",
      "Epoch 380/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 381/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 382/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0034 - val_loss: 0.0501\n",
      "Epoch 383/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0507\n",
      "Epoch 384/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 385/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 386/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 387/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 388/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 389/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 390/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0034 - val_loss: 0.0497\n",
      "Epoch 391/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 392/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 393/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 394/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0034 - val_loss: 0.0500\n",
      "Epoch 395/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 396/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 397/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0034 - val_loss: 0.0499\n",
      "Epoch 398/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0034 - val_loss: 0.0495\n",
      "Epoch 399/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 400/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 401/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 402/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 403/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 404/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 405/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0511\n",
      "Epoch 406/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0490\n",
      "Epoch 407/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 408/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 409/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0031 - val_loss: 0.0496\n",
      "Epoch 410/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 411/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 412/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0034 - val_loss: 0.0493\n",
      "Epoch 413/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 414/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 415/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0034 - val_loss: 0.0505\n",
      "Epoch 416/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 417/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 418/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 419/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 420/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0033 - val_loss: 0.0501\n",
      "Epoch 421/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 422/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0032 - val_loss: 0.0500\n",
      "Epoch 423/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 424/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0033 - val_loss: 0.0513\n",
      "Epoch 425/1000\n",
      "9000/9000 [==============================] - 2s 176us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 426/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 427/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 428/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0032 - val_loss: 0.0504\n",
      "Epoch 429/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 430/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 431/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 432/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0033 - val_loss: 0.0501\n",
      "Epoch 433/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 434/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 435/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 436/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 437/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 438/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0034 - val_loss: 0.0498\n",
      "Epoch 439/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 440/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 441/1000\n",
      "9000/9000 [==============================] - 2s 179us/sample - loss: 0.0034 - val_loss: 0.0495\n",
      "Epoch 442/1000\n",
      "9000/9000 [==============================] - 2s 178us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 443/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 444/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 445/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 446/1000\n",
      "9000/9000 [==============================] - 2s 174us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 447/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 448/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0032 - val_loss: 0.0495\n",
      "Epoch 449/1000\n",
      "9000/9000 [==============================] - 2s 172us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 450/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 451/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 452/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 453/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 454/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 455/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 456/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 457/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 458/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 459/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 460/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 461/1000\n",
      "9000/9000 [==============================] - 2s 168us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 462/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 463/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 464/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0031 - val_loss: 0.0501\n",
      "Epoch 465/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 466/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0034 - val_loss: 0.0492\n",
      "Epoch 467/1000\n",
      "9000/9000 [==============================] - 2s 175us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 468/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0031 - val_loss: 0.0504\n",
      "Epoch 469/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 470/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 471/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0495\n",
      "Epoch 472/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 473/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 474/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 475/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0506\n",
      "Epoch 476/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 477/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 478/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0507\n",
      "Epoch 479/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 480/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 481/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 482/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 483/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 484/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0033 - val_loss: 0.0505\n",
      "Epoch 485/1000\n",
      "9000/9000 [==============================] - 2s 170us/sample - loss: 0.0032 - val_loss: 0.0500\n",
      "Epoch 486/1000\n",
      "9000/9000 [==============================] - 2s 171us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 487/1000\n",
      "9000/9000 [==============================] - 2s 173us/sample - loss: 0.0033 - val_loss: 0.0505\n",
      "Epoch 488/1000\n",
      "9000/9000 [==============================] - 2s 177us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 489/1000\n",
      "9000/9000 [==============================] - 2s 199us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 490/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0498\n",
      "Epoch 491/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0501\n",
      "Epoch 492/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 493/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 494/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 495/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 496/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0032 - val_loss: 0.0503\n",
      "Epoch 497/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 498/1000\n",
      "9000/9000 [==============================] - 2s 200us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 499/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 500/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0491\n",
      "Epoch 501/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 502/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0504\n",
      "Epoch 503/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0034 - val_loss: 0.0492\n",
      "Epoch 504/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0032 - val_loss: 0.0505\n",
      "Epoch 505/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0497\n",
      "Epoch 506/1000\n",
      "9000/9000 [==============================] - 2s 196us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 507/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0033 - val_loss: 0.0493\n",
      "Epoch 508/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0032 - val_loss: 0.0491\n",
      "Epoch 509/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0488\n",
      "Epoch 510/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 511/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 512/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 513/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0033 - val_loss: 0.0504\n",
      "Epoch 514/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 515/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 516/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0031 - val_loss: 0.0498\n",
      "Epoch 517/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 518/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0499\n",
      "Epoch 519/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0498\n",
      "Epoch 520/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0490\n",
      "Epoch 521/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 522/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0503\n",
      "Epoch 523/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 524/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 525/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 526/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 527/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 528/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0034 - val_loss: 0.0492\n",
      "Epoch 529/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0031 - val_loss: 0.0497\n",
      "Epoch 530/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 531/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0032 - val_loss: 0.0495\n",
      "Epoch 532/1000\n",
      "9000/9000 [==============================] - 2s 183us/sample - loss: 0.0033 - val_loss: 0.0490\n",
      "Epoch 533/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 534/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0511\n",
      "Epoch 535/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 536/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0504\n",
      "Epoch 537/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0033 - val_loss: 0.0496\n",
      "Epoch 538/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 539/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0032 - val_loss: 0.0491\n",
      "Epoch 540/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0031 - val_loss: 0.0492\n",
      "Epoch 541/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0491\n",
      "Epoch 542/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0490\n",
      "Epoch 543/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0031 - val_loss: 0.0495\n",
      "Epoch 544/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 545/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0491\n",
      "Epoch 546/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 547/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 548/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 549/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0031 - val_loss: 0.0497\n",
      "Epoch 550/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0495\n",
      "Epoch 551/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 552/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 553/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0031 - val_loss: 0.0494\n",
      "Epoch 554/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 555/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 556/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0033 - val_loss: 0.0502\n",
      "Epoch 557/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0031 - val_loss: 0.0499\n",
      "Epoch 558/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0486\n",
      "Epoch 559/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 560/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0031 - val_loss: 0.0496\n",
      "Epoch 561/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 562/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 563/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 564/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 565/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0491\n",
      "Epoch 566/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 567/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0031 - val_loss: 0.0496\n",
      "Epoch 568/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0501\n",
      "Epoch 569/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 570/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 571/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0033 - val_loss: 0.0492\n",
      "Epoch 572/1000\n",
      "9000/9000 [==============================] - 2s 194us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 573/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0032 - val_loss: 0.0497\n",
      "Epoch 574/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0033 - val_loss: 0.0494\n",
      "Epoch 575/1000\n",
      "9000/9000 [==============================] - 2s 192us/sample - loss: 0.0031 - val_loss: 0.0491\n",
      "Epoch 576/1000\n",
      "9000/9000 [==============================] - 2s 197us/sample - loss: 0.0031 - val_loss: 0.0496\n",
      "Epoch 577/1000\n",
      "9000/9000 [==============================] - 2s 199us/sample - loss: 0.0033 - val_loss: 0.0495\n",
      "Epoch 578/1000\n",
      "9000/9000 [==============================] - 2s 200us/sample - loss: 0.0031 - val_loss: 0.0493\n",
      "Epoch 579/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0031 - val_loss: 0.0498\n",
      "Epoch 580/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0498\n",
      "Epoch 581/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0498\n",
      "Epoch 582/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 0.0032 - val_loss: 0.0495\n",
      "Epoch 583/1000\n",
      "9000/9000 [==============================] - 2s 186us/sample - loss: 0.0033 - val_loss: 0.0500\n",
      "Epoch 584/1000\n",
      "9000/9000 [==============================] - 2s 189us/sample - loss: 0.0032 - val_loss: 0.0493\n",
      "Epoch 585/1000\n",
      "9000/9000 [==============================] - 2s 188us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 586/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0489\n",
      "Epoch 587/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0032 - val_loss: 0.0499\n",
      "Epoch 588/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0496\n",
      "Epoch 589/1000\n",
      "9000/9000 [==============================] - 2s 187us/sample - loss: 0.0032 - val_loss: 0.0498\n",
      "Epoch 590/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0033 - val_loss: 0.0503\n",
      "Epoch 591/1000\n",
      "9000/9000 [==============================] - 2s 198us/sample - loss: 0.0032 - val_loss: 0.0504\n",
      "Epoch 592/1000\n",
      "9000/9000 [==============================] - 2s 191us/sample - loss: 0.0032 - val_loss: 0.0494\n",
      "Epoch 593/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0031 - val_loss: 0.0503\n",
      "Epoch 594/1000\n",
      "9000/9000 [==============================] - 2s 195us/sample - loss: 0.0031 - val_loss: 0.0494\n",
      "Epoch 595/1000\n",
      "9000/9000 [==============================] - 2s 190us/sample - loss: 0.0033 - val_loss: 0.0510\n",
      "Epoch 596/1000\n",
      "9000/9000 [==============================] - 2s 185us/sample - loss: 0.0032 - val_loss: 0.0492\n",
      "Epoch 597/1000\n",
      "9000/9000 [==============================] - 2s 184us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 598/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0031 - val_loss: 0.0495\n",
      "Epoch 599/1000\n",
      "9000/9000 [==============================] - 2s 180us/sample - loss: 0.0032 - val_loss: 0.0501\n",
      "Epoch 600/1000\n",
      "9000/9000 [==============================] - 2s 182us/sample - loss: 0.0032 - val_loss: 0.0490\n",
      "Epoch 601/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0032 - val_loss: 0.0491\n",
      "Epoch 602/1000\n",
      "9000/9000 [==============================] - 2s 181us/sample - loss: 0.0032 - val_loss: 0.0507\n",
      "200/200 [==============================] - 0s 75us/sample - loss: 0.0485\n",
      "300/300 [==============================] - 0s 67us/sample - loss: 0.5899\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "if __name__ == \"__main__\":\n",
    "    autoencoder = AutoEncoder(15)\n",
    "    X_train_enc, X_valid_enc, X_test_enc, valid_error_, test_error_ = autoencoder.build_train_model(40, 30, 20, 20, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04854849748313427"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valid error with he_init and elu activation\n",
    "valid_error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5899068768819173"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Valid error with lecun and selu activation\n",
    "test_error_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc_ = X_train_enc.values\n",
    "X_valid_enc_ = X_valid_enc.values\n",
    "X_test_enc_ = X_test_enc.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_enc_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-341702b8498e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_enc_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_enc_' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_enc_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = keras.layers.Input(shape=X_train_enc_.shape[1:])\n",
    "normalize0 = keras.layers.BatchNormalization()(encoded_input)\n",
    "hidden0 = keras.layers.Dense(20, kernel_initializer=\"lecun_normal\", activation=\"selu\")(normalize0)\n",
    "hidden1 = keras.layers.Dense(20, kernel_initializer=\"lecun_normal\", activation=\"selu\")(hidden0)\n",
    "#hidden2 = keras.layers.Dense(100, kernel_initializer=\"lecun_normal\", activation=\"selu\")(hidden1)\n",
    "#hidden3 = keras.layers.Dense(50, kernel_initializer=\"lecun_normal\", activation=\"selu\")(hidden2)\n",
    "#hidden4 = keras.layers.Dense(20, kernel_initializer=\"lecun_normal\", activation=\"selu\")(hidden3)\n",
    "#dropout = MCAlphaDropout(rate=0.2)(hidden4)\n",
    "output = keras.layers.Dense(1, activation=\"tanh\")(hidden1)\n",
    "mlp = keras.models.Model(inputs=[encoded_input], outputs=[output])\n",
    "early_stp = keras.callbacks.EarlyStopping(patience=300, restore_best_weights=True)\n",
    "mlp.compile(loss=\"mse\", optimizer=\"nadam\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_36 (InputLayer)        [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v2_21 (B (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 941\n",
      "Trainable params: 901\n",
      "Non-trainable params: 40\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9000 samples, validate on 200 samples\n",
      "Epoch 1/1000\n",
      "9000/9000 [==============================] - 1s 128us/sample - loss: 0.0667 - val_loss: 0.0161\n",
      "Epoch 2/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 0.0086 - val_loss: 0.0185\n",
      "Epoch 3/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 0.0048 - val_loss: 0.0170\n",
      "Epoch 4/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 0.0034 - val_loss: 0.0074\n",
      "Epoch 5/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 0.0026 - val_loss: 0.0070\n",
      "Epoch 6/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0022 - val_loss: 0.0057\n",
      "Epoch 7/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 0.0019 - val_loss: 0.0052\n",
      "Epoch 8/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 0.0017 - val_loss: 0.0042\n",
      "Epoch 9/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 10/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 0.0013 - val_loss: 0.0042\n",
      "Epoch 11/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 0.0012 - val_loss: 0.0025\n",
      "Epoch 12/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 0.0011 - val_loss: 0.0020\n",
      "Epoch 13/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 9.9223e-04 - val_loss: 0.0014\n",
      "Epoch 14/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 9.1002e-04 - val_loss: 0.0036\n",
      "Epoch 15/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 8.5112e-04 - val_loss: 0.0017\n",
      "Epoch 16/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 8.3429e-04 - val_loss: 0.0018\n",
      "Epoch 17/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 7.8034e-04 - val_loss: 0.0013\n",
      "Epoch 18/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 7.5589e-04 - val_loss: 0.0018\n",
      "Epoch 19/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 7.2331e-04 - val_loss: 0.0015\n",
      "Epoch 20/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 7.0673e-04 - val_loss: 0.0012\n",
      "Epoch 21/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 6.8559e-04 - val_loss: 0.0016\n",
      "Epoch 22/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.8029e-04 - val_loss: 0.0014\n",
      "Epoch 23/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.5988e-04 - val_loss: 6.4168e-04\n",
      "Epoch 24/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 6.4017e-04 - val_loss: 5.5035e-04\n",
      "Epoch 25/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3788e-04 - val_loss: 6.8474e-04\n",
      "Epoch 26/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.3371e-04 - val_loss: 4.9639e-04\n",
      "Epoch 27/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.1931e-04 - val_loss: 6.4300e-04\n",
      "Epoch 28/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.1072e-04 - val_loss: 8.4014e-04\n",
      "Epoch 29/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 6.0694e-04 - val_loss: 8.5114e-04\n",
      "Epoch 30/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 6.0639e-04 - val_loss: 6.9542e-04\n",
      "Epoch 31/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.9685e-04 - val_loss: 4.5756e-04\n",
      "Epoch 32/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 6.0164e-04 - val_loss: 1.9835e-04\n",
      "Epoch 33/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.9053e-04 - val_loss: 2.5327e-04\n",
      "Epoch 34/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.8609e-04 - val_loss: 5.6722e-04\n",
      "Epoch 35/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.8904e-04 - val_loss: 1.3630e-04\n",
      "Epoch 36/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.7877e-04 - val_loss: 2.5971e-04\n",
      "Epoch 37/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.8545e-04 - val_loss: 3.8928e-04\n",
      "Epoch 38/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.8028e-04 - val_loss: 2.6844e-04\n",
      "Epoch 39/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.7689e-04 - val_loss: 1.3144e-04\n",
      "Epoch 40/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.7191e-04 - val_loss: 0.0055\n",
      "Epoch 41/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.8774e-04 - val_loss: 1.6768e-04\n",
      "Epoch 42/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.6302e-04 - val_loss: 1.5403e-04\n",
      "Epoch 43/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.6430e-04 - val_loss: 1.4138e-04\n",
      "Epoch 44/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.6342e-04 - val_loss: 2.2507e-04\n",
      "Epoch 45/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.5946e-04 - val_loss: 1.5617e-04\n",
      "Epoch 46/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.5665e-04 - val_loss: 1.1063e-04\n",
      "Epoch 47/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.5632e-04 - val_loss: 2.0214e-04\n",
      "Epoch 48/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.5836e-04 - val_loss: 3.8868e-04\n",
      "Epoch 49/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.5390e-04 - val_loss: 1.3428e-04\n",
      "Epoch 50/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.5796e-04 - val_loss: 2.3610e-04\n",
      "Epoch 51/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.6077e-04 - val_loss: 1.2676e-04\n",
      "Epoch 52/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.5505e-04 - val_loss: 1.0211e-04\n",
      "Epoch 53/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.5195e-04 - val_loss: 1.4748e-04\n",
      "Epoch 54/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.5419e-04 - val_loss: 1.0357e-04\n",
      "Epoch 55/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.4800e-04 - val_loss: 2.7096e-04\n",
      "Epoch 56/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.5150e-04 - val_loss: 1.3602e-04\n",
      "Epoch 57/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.4839e-04 - val_loss: 1.4410e-04\n",
      "Epoch 58/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.4759e-04 - val_loss: 1.7745e-04\n",
      "Epoch 59/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.4637e-04 - val_loss: 2.4928e-04\n",
      "Epoch 60/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.4712e-04 - val_loss: 1.2760e-04\n",
      "Epoch 61/1000\n",
      "9000/9000 [==============================] - 0s 56us/sample - loss: 5.5115e-04 - val_loss: 1.1924e-04\n",
      "Epoch 62/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.4487e-04 - val_loss: 1.1539e-04\n",
      "Epoch 63/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.4586e-04 - val_loss: 1.1347e-04\n",
      "Epoch 64/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.4443e-04 - val_loss: 1.2656e-04\n",
      "Epoch 65/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.4421e-04 - val_loss: 1.4331e-04\n",
      "Epoch 66/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.4143e-04 - val_loss: 1.6294e-04\n",
      "Epoch 67/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.4468e-04 - val_loss: 1.2601e-04\n",
      "Epoch 68/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.4710e-04 - val_loss: 1.5041e-04\n",
      "Epoch 69/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.4621e-04 - val_loss: 1.4596e-04\n",
      "Epoch 70/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.4169e-04 - val_loss: 1.2127e-04\n",
      "Epoch 71/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.4052e-04 - val_loss: 1.4993e-04\n",
      "Epoch 72/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.4420e-04 - val_loss: 1.2359e-04\n",
      "Epoch 73/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.4187e-04 - val_loss: 1.6340e-04\n",
      "Epoch 74/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3972e-04 - val_loss: 1.1146e-04\n",
      "Epoch 75/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.4432e-04 - val_loss: 1.0791e-04\n",
      "Epoch 76/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3970e-04 - val_loss: 1.2563e-04\n",
      "Epoch 77/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.4246e-04 - val_loss: 1.5224e-04\n",
      "Epoch 78/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3977e-04 - val_loss: 1.2551e-04\n",
      "Epoch 79/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3814e-04 - val_loss: 1.9770e-04\n",
      "Epoch 80/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3897e-04 - val_loss: 1.1133e-04\n",
      "Epoch 81/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.3604e-04 - val_loss: 1.0468e-04\n",
      "Epoch 82/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3944e-04 - val_loss: 1.0430e-04\n",
      "Epoch 83/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3745e-04 - val_loss: 1.2633e-04\n",
      "Epoch 84/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3664e-04 - val_loss: 1.4294e-04\n",
      "Epoch 85/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3697e-04 - val_loss: 1.5590e-04\n",
      "Epoch 86/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3499e-04 - val_loss: 1.3867e-04\n",
      "Epoch 87/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3768e-04 - val_loss: 1.4284e-04\n",
      "Epoch 88/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3319e-04 - val_loss: 1.5371e-04\n",
      "Epoch 89/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3575e-04 - val_loss: 1.3649e-04\n",
      "Epoch 90/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3288e-04 - val_loss: 1.2667e-04\n",
      "Epoch 91/1000\n",
      "9000/9000 [==============================] - 0s 56us/sample - loss: 5.3581e-04 - val_loss: 9.9789e-05\n",
      "Epoch 92/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3446e-04 - val_loss: 1.1355e-04\n",
      "Epoch 93/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3260e-04 - val_loss: 1.4343e-04\n",
      "Epoch 94/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3464e-04 - val_loss: 1.2572e-04\n",
      "Epoch 95/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3434e-04 - val_loss: 1.2867e-04\n",
      "Epoch 96/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3402e-04 - val_loss: 1.3742e-04\n",
      "Epoch 97/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3651e-04 - val_loss: 1.5555e-04\n",
      "Epoch 98/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3404e-04 - val_loss: 1.3992e-04\n",
      "Epoch 99/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.3445e-04 - val_loss: 1.1849e-04\n",
      "Epoch 100/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3595e-04 - val_loss: 1.2135e-04\n",
      "Epoch 101/1000\n",
      "9000/9000 [==============================] - 0s 56us/sample - loss: 5.3654e-04 - val_loss: 1.0568e-04\n",
      "Epoch 102/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3482e-04 - val_loss: 1.3502e-04\n",
      "Epoch 103/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3544e-04 - val_loss: 1.1579e-04\n",
      "Epoch 104/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3616e-04 - val_loss: 1.1927e-04\n",
      "Epoch 105/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3170e-04 - val_loss: 1.7803e-04\n",
      "Epoch 106/1000\n",
      "9000/9000 [==============================] - 1s 70us/sample - loss: 5.2987e-04 - val_loss: 1.5364e-04\n",
      "Epoch 107/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3326e-04 - val_loss: 1.6487e-04\n",
      "Epoch 108/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3414e-04 - val_loss: 9.8856e-05\n",
      "Epoch 109/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.3320e-04 - val_loss: 1.1490e-04\n",
      "Epoch 110/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3186e-04 - val_loss: 1.0284e-04\n",
      "Epoch 111/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2801e-04 - val_loss: 1.8609e-04\n",
      "Epoch 112/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3286e-04 - val_loss: 1.1077e-04\n",
      "Epoch 113/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3435e-04 - val_loss: 1.1564e-04\n",
      "Epoch 114/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3177e-04 - val_loss: 1.2527e-04\n",
      "Epoch 115/1000\n",
      "9000/9000 [==============================] - 1s 70us/sample - loss: 5.3486e-04 - val_loss: 1.7112e-04\n",
      "Epoch 116/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3282e-04 - val_loss: 1.2932e-04\n",
      "Epoch 117/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3279e-04 - val_loss: 1.5062e-04\n",
      "Epoch 118/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3446e-04 - val_loss: 1.2976e-04\n",
      "Epoch 119/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3082e-04 - val_loss: 1.1817e-04\n",
      "Epoch 120/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2847e-04 - val_loss: 1.3599e-04\n",
      "Epoch 121/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3185e-04 - val_loss: 1.1804e-04\n",
      "Epoch 122/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3067e-04 - val_loss: 1.5136e-04\n",
      "Epoch 123/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3046e-04 - val_loss: 1.4819e-04\n",
      "Epoch 124/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3380e-04 - val_loss: 1.3482e-04\n",
      "Epoch 125/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3296e-04 - val_loss: 1.0510e-04\n",
      "Epoch 126/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2915e-04 - val_loss: 1.1883e-04\n",
      "Epoch 127/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3201e-04 - val_loss: 1.9418e-04\n",
      "Epoch 128/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3139e-04 - val_loss: 1.1974e-04\n",
      "Epoch 129/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3227e-04 - val_loss: 1.1505e-04\n",
      "Epoch 130/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.3015e-04 - val_loss: 1.4508e-04\n",
      "Epoch 131/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3034e-04 - val_loss: 1.2000e-04\n",
      "Epoch 132/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3268e-04 - val_loss: 1.3011e-04\n",
      "Epoch 133/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.3088e-04 - val_loss: 1.1914e-04\n",
      "Epoch 134/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.3154e-04 - val_loss: 1.1259e-04\n",
      "Epoch 135/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3137e-04 - val_loss: 1.1230e-04\n",
      "Epoch 136/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3045e-04 - val_loss: 1.4980e-04\n",
      "Epoch 137/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2867e-04 - val_loss: 1.6548e-04\n",
      "Epoch 138/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3152e-04 - val_loss: 1.1565e-04\n",
      "Epoch 139/1000\n",
      "9000/9000 [==============================] - 1s 66us/sample - loss: 5.3131e-04 - val_loss: 1.2694e-04\n",
      "Epoch 140/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2928e-04 - val_loss: 1.4067e-04\n",
      "Epoch 141/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.3182e-04 - val_loss: 1.2954e-04\n",
      "Epoch 142/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2921e-04 - val_loss: 1.2479e-04\n",
      "Epoch 143/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3316e-04 - val_loss: 1.0799e-04\n",
      "Epoch 144/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2687e-04 - val_loss: 1.1267e-04\n",
      "Epoch 145/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3033e-04 - val_loss: 1.0526e-04\n",
      "Epoch 146/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2766e-04 - val_loss: 1.4467e-04\n",
      "Epoch 147/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2764e-04 - val_loss: 1.1727e-04\n",
      "Epoch 148/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3347e-04 - val_loss: 1.5545e-04\n",
      "Epoch 149/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3379e-04 - val_loss: 1.0702e-04\n",
      "Epoch 150/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3109e-04 - val_loss: 1.0222e-04\n",
      "Epoch 151/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2479e-04 - val_loss: 1.1150e-04\n",
      "Epoch 152/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2869e-04 - val_loss: 1.3657e-04\n",
      "Epoch 153/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3009e-04 - val_loss: 1.1026e-04\n",
      "Epoch 154/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.3032e-04 - val_loss: 1.1677e-04\n",
      "Epoch 155/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.3063e-04 - val_loss: 1.0465e-04\n",
      "Epoch 156/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.3388e-04 - val_loss: 1.3055e-04\n",
      "Epoch 157/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2708e-04 - val_loss: 1.2389e-04\n",
      "Epoch 158/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2930e-04 - val_loss: 1.1660e-04\n",
      "Epoch 159/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2689e-04 - val_loss: 1.3319e-04\n",
      "Epoch 160/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3026e-04 - val_loss: 1.8738e-04\n",
      "Epoch 161/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2662e-04 - val_loss: 1.0598e-04\n",
      "Epoch 162/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.3067e-04 - val_loss: 1.1657e-04\n",
      "Epoch 163/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.2904e-04 - val_loss: 1.0983e-04\n",
      "Epoch 164/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2788e-04 - val_loss: 1.0657e-04\n",
      "Epoch 165/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.2639e-04 - val_loss: 1.2306e-04\n",
      "Epoch 166/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.3118e-04 - val_loss: 1.1592e-04\n",
      "Epoch 167/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2922e-04 - val_loss: 1.8706e-04\n",
      "Epoch 168/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.3099e-04 - val_loss: 1.3107e-04\n",
      "Epoch 169/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2918e-04 - val_loss: 1.1705e-04\n",
      "Epoch 170/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2983e-04 - val_loss: 1.0819e-04\n",
      "Epoch 171/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2750e-04 - val_loss: 1.1807e-04\n",
      "Epoch 172/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2811e-04 - val_loss: 1.2145e-04\n",
      "Epoch 173/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.3155e-04 - val_loss: 1.2167e-04\n",
      "Epoch 174/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2976e-04 - val_loss: 1.3524e-04\n",
      "Epoch 175/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2918e-04 - val_loss: 1.0978e-04\n",
      "Epoch 176/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2422e-04 - val_loss: 1.4131e-04\n",
      "Epoch 177/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.3009e-04 - val_loss: 1.0759e-04\n",
      "Epoch 178/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2659e-04 - val_loss: 1.1267e-04\n",
      "Epoch 179/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2632e-04 - val_loss: 2.1917e-04\n",
      "Epoch 180/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2935e-04 - val_loss: 1.3467e-04\n",
      "Epoch 181/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2674e-04 - val_loss: 1.2030e-04\n",
      "Epoch 182/1000\n",
      "9000/9000 [==============================] - 1s 65us/sample - loss: 5.2790e-04 - val_loss: 1.2471e-04\n",
      "Epoch 183/1000\n",
      "9000/9000 [==============================] - 1s 64us/sample - loss: 5.2855e-04 - val_loss: 1.0506e-04\n",
      "Epoch 184/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 5.3266e-04 - val_loss: 1.1330e-04\n",
      "Epoch 185/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2507e-04 - val_loss: 1.6626e-04\n",
      "Epoch 186/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2425e-04 - val_loss: 1.1779e-04\n",
      "Epoch 187/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2619e-04 - val_loss: 1.1900e-04\n",
      "Epoch 188/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2535e-04 - val_loss: 1.3539e-04\n",
      "Epoch 189/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.3320e-04 - val_loss: 1.3728e-04\n",
      "Epoch 190/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2698e-04 - val_loss: 1.1290e-04\n",
      "Epoch 191/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2661e-04 - val_loss: 1.3943e-04\n",
      "Epoch 192/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2741e-04 - val_loss: 1.1421e-04\n",
      "Epoch 193/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2967e-04 - val_loss: 1.2779e-04\n",
      "Epoch 194/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2747e-04 - val_loss: 9.7823e-05\n",
      "Epoch 195/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2510e-04 - val_loss: 1.6676e-04\n",
      "Epoch 196/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2879e-04 - val_loss: 1.2318e-04\n",
      "Epoch 197/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2836e-04 - val_loss: 1.2554e-04\n",
      "Epoch 198/1000\n",
      "9000/9000 [==============================] - 0s 56us/sample - loss: 5.2491e-04 - val_loss: 1.3020e-04\n",
      "Epoch 199/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.3127e-04 - val_loss: 1.2529e-04\n",
      "Epoch 200/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2852e-04 - val_loss: 1.0444e-04\n",
      "Epoch 201/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2891e-04 - val_loss: 1.0948e-04\n",
      "Epoch 202/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2736e-04 - val_loss: 1.4570e-04\n",
      "Epoch 203/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2613e-04 - val_loss: 1.3678e-04\n",
      "Epoch 204/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2864e-04 - val_loss: 1.1123e-04\n",
      "Epoch 205/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2831e-04 - val_loss: 1.2943e-04\n",
      "Epoch 206/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2759e-04 - val_loss: 1.1942e-04\n",
      "Epoch 207/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2565e-04 - val_loss: 1.0421e-04\n",
      "Epoch 208/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2451e-04 - val_loss: 1.1543e-04\n",
      "Epoch 209/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2908e-04 - val_loss: 1.1975e-04\n",
      "Epoch 210/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2671e-04 - val_loss: 1.3263e-04\n",
      "Epoch 211/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2410e-04 - val_loss: 1.1863e-04\n",
      "Epoch 212/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.3218e-04 - val_loss: 1.2107e-04\n",
      "Epoch 213/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2553e-04 - val_loss: 1.3167e-04\n",
      "Epoch 214/1000\n",
      "9000/9000 [==============================] - 1s 66us/sample - loss: 5.2922e-04 - val_loss: 1.5508e-04\n",
      "Epoch 215/1000\n",
      "9000/9000 [==============================] - 1s 68us/sample - loss: 5.2605e-04 - val_loss: 1.2690e-04\n",
      "Epoch 216/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2639e-04 - val_loss: 1.2839e-04\n",
      "Epoch 217/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2243e-04 - val_loss: 1.4175e-04\n",
      "Epoch 218/1000\n",
      "9000/9000 [==============================] - 1s 74us/sample - loss: 5.2660e-04 - val_loss: 1.2662e-04\n",
      "Epoch 219/1000\n",
      "9000/9000 [==============================] - 1s 72us/sample - loss: 5.2298e-04 - val_loss: 1.1320e-04\n",
      "Epoch 220/1000\n",
      "9000/9000 [==============================] - 1s 77us/sample - loss: 5.2774e-04 - val_loss: 1.4187e-04\n",
      "Epoch 221/1000\n",
      "9000/9000 [==============================] - 1s 81us/sample - loss: 5.2898e-04 - val_loss: 1.1928e-04\n",
      "Epoch 222/1000\n",
      "9000/9000 [==============================] - 1s 85us/sample - loss: 5.2906e-04 - val_loss: 2.2197e-04\n",
      "Epoch 223/1000\n",
      "9000/9000 [==============================] - 1s 91us/sample - loss: 5.2688e-04 - val_loss: 1.4901e-04\n",
      "Epoch 224/1000\n",
      "9000/9000 [==============================] - 1s 95us/sample - loss: 5.2616e-04 - val_loss: 1.0413e-04\n",
      "Epoch 225/1000\n",
      "9000/9000 [==============================] - 1s 96us/sample - loss: 5.2895e-04 - val_loss: 1.2266e-04\n",
      "Epoch 226/1000\n",
      "9000/9000 [==============================] - 1s 100us/sample - loss: 5.2619e-04 - val_loss: 1.1229e-04\n",
      "Epoch 227/1000\n",
      "9000/9000 [==============================] - 1s 100us/sample - loss: 5.2675e-04 - val_loss: 1.5482e-04\n",
      "Epoch 228/1000\n",
      "9000/9000 [==============================] - 1s 99us/sample - loss: 5.2548e-04 - val_loss: 1.0553e-04\n",
      "Epoch 229/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.2571e-04 - val_loss: 1.3757e-04\n",
      "Epoch 230/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.2319e-04 - val_loss: 1.2410e-04\n",
      "Epoch 231/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.3059e-04 - val_loss: 1.4056e-04\n",
      "Epoch 232/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.2979e-04 - val_loss: 1.1972e-04\n",
      "Epoch 233/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.2639e-04 - val_loss: 1.3028e-04\n",
      "Epoch 234/1000\n",
      "9000/9000 [==============================] - 1s 93us/sample - loss: 5.2679e-04 - val_loss: 1.9382e-04\n",
      "Epoch 235/1000\n",
      "9000/9000 [==============================] - 1s 92us/sample - loss: 5.2844e-04 - val_loss: 1.4273e-04\n",
      "Epoch 236/1000\n",
      "9000/9000 [==============================] - 1s 91us/sample - loss: 5.2776e-04 - val_loss: 1.0848e-04\n",
      "Epoch 237/1000\n",
      "9000/9000 [==============================] - 1s 90us/sample - loss: 5.2717e-04 - val_loss: 1.4826e-04\n",
      "Epoch 238/1000\n",
      "9000/9000 [==============================] - 1s 88us/sample - loss: 5.2743e-04 - val_loss: 1.4187e-04\n",
      "Epoch 239/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2335e-04 - val_loss: 1.2165e-04\n",
      "Epoch 240/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2896e-04 - val_loss: 1.3578e-04\n",
      "Epoch 241/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2738e-04 - val_loss: 1.1255e-04\n",
      "Epoch 242/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2536e-04 - val_loss: 1.6226e-04\n",
      "Epoch 243/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2638e-04 - val_loss: 1.3547e-04\n",
      "Epoch 244/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2475e-04 - val_loss: 1.0561e-04\n",
      "Epoch 245/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2497e-04 - val_loss: 1.1562e-04\n",
      "Epoch 246/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2676e-04 - val_loss: 1.0709e-04\n",
      "Epoch 247/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2546e-04 - val_loss: 1.4718e-04\n",
      "Epoch 248/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2727e-04 - val_loss: 1.1243e-04\n",
      "Epoch 249/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2426e-04 - val_loss: 1.2796e-04\n",
      "Epoch 250/1000\n",
      "9000/9000 [==============================] - 1s 89us/sample - loss: 5.2404e-04 - val_loss: 1.2265e-04\n",
      "Epoch 251/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2694e-04 - val_loss: 1.2560e-04\n",
      "Epoch 252/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2208e-04 - val_loss: 1.0014e-04\n",
      "Epoch 253/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2571e-04 - val_loss: 1.0883e-04\n",
      "Epoch 254/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2515e-04 - val_loss: 1.1259e-04\n",
      "Epoch 255/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2485e-04 - val_loss: 1.4569e-04\n",
      "Epoch 256/1000\n",
      "9000/9000 [==============================] - 1s 86us/sample - loss: 5.2897e-04 - val_loss: 1.1542e-04\n",
      "Epoch 257/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2936e-04 - val_loss: 1.0756e-04\n",
      "Epoch 258/1000\n",
      "9000/9000 [==============================] - 1s 89us/sample - loss: 5.2401e-04 - val_loss: 1.2257e-04\n",
      "Epoch 259/1000\n",
      "9000/9000 [==============================] - 1s 89us/sample - loss: 5.2353e-04 - val_loss: 1.2096e-04\n",
      "Epoch 260/1000\n",
      "9000/9000 [==============================] - 1s 93us/sample - loss: 5.2771e-04 - val_loss: 1.2439e-04\n",
      "Epoch 261/1000\n",
      "9000/9000 [==============================] - 1s 92us/sample - loss: 5.2009e-04 - val_loss: 1.1697e-04\n",
      "Epoch 262/1000\n",
      "9000/9000 [==============================] - 1s 93us/sample - loss: 5.2439e-04 - val_loss: 1.1355e-04\n",
      "Epoch 263/1000\n",
      "9000/9000 [==============================] - 1s 92us/sample - loss: 5.2677e-04 - val_loss: 1.2226e-04\n",
      "Epoch 264/1000\n",
      "9000/9000 [==============================] - 1s 91us/sample - loss: 5.2196e-04 - val_loss: 1.2557e-04\n",
      "Epoch 265/1000\n",
      "9000/9000 [==============================] - 1s 96us/sample - loss: 5.2777e-04 - val_loss: 1.1899e-04\n",
      "Epoch 266/1000\n",
      "9000/9000 [==============================] - 1s 97us/sample - loss: 5.2758e-04 - val_loss: 1.1346e-04\n",
      "Epoch 267/1000\n",
      "9000/9000 [==============================] - 1s 100us/sample - loss: 5.2493e-04 - val_loss: 1.1153e-04\n",
      "Epoch 268/1000\n",
      "9000/9000 [==============================] - 1s 105us/sample - loss: 5.2242e-04 - val_loss: 1.3683e-04\n",
      "Epoch 269/1000\n",
      "9000/9000 [==============================] - 1s 105us/sample - loss: 5.2790e-04 - val_loss: 1.0914e-04\n",
      "Epoch 270/1000\n",
      "9000/9000 [==============================] - 1s 104us/sample - loss: 5.2597e-04 - val_loss: 1.3796e-04\n",
      "Epoch 271/1000\n",
      "9000/9000 [==============================] - 1s 106us/sample - loss: 5.2458e-04 - val_loss: 1.4360e-04\n",
      "Epoch 272/1000\n",
      "9000/9000 [==============================] - 1s 118us/sample - loss: 5.2234e-04 - val_loss: 1.2752e-04\n",
      "Epoch 273/1000\n",
      "9000/9000 [==============================] - 1s 141us/sample - loss: 5.2504e-04 - val_loss: 1.1814e-04\n",
      "Epoch 274/1000\n",
      "9000/9000 [==============================] - 1s 143us/sample - loss: 5.2280e-04 - val_loss: 1.2595e-04\n",
      "Epoch 275/1000\n",
      "9000/9000 [==============================] - 1s 137us/sample - loss: 5.2608e-04 - val_loss: 1.2454e-04\n",
      "Epoch 276/1000\n",
      "9000/9000 [==============================] - 1s 150us/sample - loss: 5.2636e-04 - val_loss: 1.3169e-04\n",
      "Epoch 277/1000\n",
      "9000/9000 [==============================] - 2s 169us/sample - loss: 5.2328e-04 - val_loss: 1.2591e-04\n",
      "Epoch 278/1000\n",
      "9000/9000 [==============================] - 2s 193us/sample - loss: 5.2168e-04 - val_loss: 1.1308e-04\n",
      "Epoch 279/1000\n",
      "9000/9000 [==============================] - 1s 164us/sample - loss: 5.2468e-04 - val_loss: 1.1628e-04\n",
      "Epoch 280/1000\n",
      "9000/9000 [==============================] - 1s 128us/sample - loss: 5.2792e-04 - val_loss: 9.9853e-05\n",
      "Epoch 281/1000\n",
      "9000/9000 [==============================] - 1s 111us/sample - loss: 5.2318e-04 - val_loss: 1.1664e-04\n",
      "Epoch 282/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 102us/sample - loss: 5.2663e-04 - val_loss: 1.0888e-04\n",
      "Epoch 283/1000\n",
      "9000/9000 [==============================] - 1s 90us/sample - loss: 5.2309e-04 - val_loss: 1.3297e-04\n",
      "Epoch 284/1000\n",
      "9000/9000 [==============================] - 1s 80us/sample - loss: 5.2035e-04 - val_loss: 1.3118e-04\n",
      "Epoch 285/1000\n",
      "9000/9000 [==============================] - 1s 76us/sample - loss: 5.2208e-04 - val_loss: 1.2661e-04\n",
      "Epoch 286/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2308e-04 - val_loss: 1.1805e-04\n",
      "Epoch 287/1000\n",
      "9000/9000 [==============================] - 1s 72us/sample - loss: 5.2566e-04 - val_loss: 1.9431e-04\n",
      "Epoch 288/1000\n",
      "9000/9000 [==============================] - 1s 67us/sample - loss: 5.3158e-04 - val_loss: 1.5228e-04\n",
      "Epoch 289/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 5.2486e-04 - val_loss: 1.1887e-04\n",
      "Epoch 290/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2990e-04 - val_loss: 1.4785e-04\n",
      "Epoch 291/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2580e-04 - val_loss: 1.0739e-04\n",
      "Epoch 292/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2724e-04 - val_loss: 1.5164e-04\n",
      "Epoch 293/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2498e-04 - val_loss: 1.2147e-04\n",
      "Epoch 294/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2631e-04 - val_loss: 1.3883e-04\n",
      "Epoch 295/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2982e-04 - val_loss: 1.4835e-04\n",
      "Epoch 296/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2460e-04 - val_loss: 1.2263e-04\n",
      "Epoch 297/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2759e-04 - val_loss: 1.4334e-04\n",
      "Epoch 298/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2430e-04 - val_loss: 1.2238e-04\n",
      "Epoch 299/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2414e-04 - val_loss: 1.5729e-04\n",
      "Epoch 300/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2452e-04 - val_loss: 1.2546e-04\n",
      "Epoch 301/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2390e-04 - val_loss: 1.0638e-04\n",
      "Epoch 302/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2028e-04 - val_loss: 2.0754e-04\n",
      "Epoch 303/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2814e-04 - val_loss: 1.3562e-04\n",
      "Epoch 304/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2595e-04 - val_loss: 1.2591e-04\n",
      "Epoch 305/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2409e-04 - val_loss: 1.1469e-04\n",
      "Epoch 306/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2483e-04 - val_loss: 1.0609e-04\n",
      "Epoch 307/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.1975e-04 - val_loss: 1.2145e-04\n",
      "Epoch 308/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2574e-04 - val_loss: 2.0000e-04\n",
      "Epoch 309/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.3597e-04 - val_loss: 1.2718e-04\n",
      "Epoch 310/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2562e-04 - val_loss: 1.0236e-04\n",
      "Epoch 311/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2471e-04 - val_loss: 1.1338e-04\n",
      "Epoch 312/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2381e-04 - val_loss: 1.0756e-04\n",
      "Epoch 313/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 5.2554e-04 - val_loss: 1.3464e-04\n",
      "Epoch 314/1000\n",
      "9000/9000 [==============================] - 1s 75us/sample - loss: 5.2316e-04 - val_loss: 1.1262e-04\n",
      "Epoch 315/1000\n",
      "9000/9000 [==============================] - 1s 84us/sample - loss: 5.2852e-04 - val_loss: 1.2930e-04\n",
      "Epoch 316/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2391e-04 - val_loss: 1.2797e-04\n",
      "Epoch 317/1000\n",
      "9000/9000 [==============================] - 1s 81us/sample - loss: 5.2405e-04 - val_loss: 1.3766e-04\n",
      "Epoch 318/1000\n",
      "9000/9000 [==============================] - 1s 84us/sample - loss: 5.2758e-04 - val_loss: 1.2238e-04\n",
      "Epoch 319/1000\n",
      "9000/9000 [==============================] - 1s 83us/sample - loss: 5.2724e-04 - val_loss: 1.1948e-04\n",
      "Epoch 320/1000\n",
      "9000/9000 [==============================] - 1s 84us/sample - loss: 5.2627e-04 - val_loss: 1.2731e-04\n",
      "Epoch 321/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 5.2215e-04 - val_loss: 1.1497e-04\n",
      "Epoch 322/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 5.2308e-04 - val_loss: 1.1849e-04\n",
      "Epoch 323/1000\n",
      "9000/9000 [==============================] - 1s 75us/sample - loss: 5.2314e-04 - val_loss: 1.6613e-04\n",
      "Epoch 324/1000\n",
      "9000/9000 [==============================] - 1s 69us/sample - loss: 5.2107e-04 - val_loss: 1.1215e-04\n",
      "Epoch 325/1000\n",
      "9000/9000 [==============================] - 1s 77us/sample - loss: 5.2369e-04 - val_loss: 1.2905e-04\n",
      "Epoch 326/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 5.2661e-04 - val_loss: 1.1902e-04\n",
      "Epoch 327/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 5.2403e-04 - val_loss: 1.6583e-04\n",
      "Epoch 328/1000\n",
      "9000/9000 [==============================] - 1s 78us/sample - loss: 5.2468e-04 - val_loss: 1.3046e-04\n",
      "Epoch 329/1000\n",
      "9000/9000 [==============================] - 1s 78us/sample - loss: 5.2237e-04 - val_loss: 1.1814e-04\n",
      "Epoch 330/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2688e-04 - val_loss: 1.2045e-04\n",
      "Epoch 331/1000\n",
      "9000/9000 [==============================] - 1s 111us/sample - loss: 5.2270e-04 - val_loss: 1.2686e-04\n",
      "Epoch 332/1000\n",
      "9000/9000 [==============================] - 1s 111us/sample - loss: 5.2311e-04 - val_loss: 1.1233e-04\n",
      "Epoch 333/1000\n",
      "9000/9000 [==============================] - 1s 131us/sample - loss: 5.2542e-04 - val_loss: 1.5220e-04\n",
      "Epoch 334/1000\n",
      "9000/9000 [==============================] - 1s 111us/sample - loss: 5.2613e-04 - val_loss: 1.6077e-04\n",
      "Epoch 335/1000\n",
      "9000/9000 [==============================] - 1s 92us/sample - loss: 5.2134e-04 - val_loss: 1.1748e-04\n",
      "Epoch 336/1000\n",
      "9000/9000 [==============================] - 1s 88us/sample - loss: 5.2335e-04 - val_loss: 1.1632e-04\n",
      "Epoch 337/1000\n",
      "9000/9000 [==============================] - 1s 87us/sample - loss: 5.2318e-04 - val_loss: 1.0751e-04\n",
      "Epoch 338/1000\n",
      "9000/9000 [==============================] - 1s 79us/sample - loss: 5.2778e-04 - val_loss: 1.2822e-04\n",
      "Epoch 339/1000\n",
      "9000/9000 [==============================] - 1s 79us/sample - loss: 5.2330e-04 - val_loss: 1.2904e-04\n",
      "Epoch 340/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2506e-04 - val_loss: 1.2533e-04\n",
      "Epoch 341/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 5.2147e-04 - val_loss: 1.1902e-04\n",
      "Epoch 342/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2338e-04 - val_loss: 1.1311e-04\n",
      "Epoch 343/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2097e-04 - val_loss: 1.3031e-04\n",
      "Epoch 344/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2522e-04 - val_loss: 1.2285e-04\n",
      "Epoch 345/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2256e-04 - val_loss: 1.2075e-04\n",
      "Epoch 346/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2453e-04 - val_loss: 1.1912e-04\n",
      "Epoch 347/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2722e-04 - val_loss: 1.2572e-04\n",
      "Epoch 348/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2154e-04 - val_loss: 1.1978e-04\n",
      "Epoch 349/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2610e-04 - val_loss: 1.2542e-04\n",
      "Epoch 350/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2605e-04 - val_loss: 1.2493e-04\n",
      "Epoch 351/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2302e-04 - val_loss: 1.7942e-04\n",
      "Epoch 352/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2552e-04 - val_loss: 1.5464e-04\n",
      "Epoch 353/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2335e-04 - val_loss: 1.4862e-04\n",
      "Epoch 354/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2777e-04 - val_loss: 1.4101e-04\n",
      "Epoch 355/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2311e-04 - val_loss: 1.7052e-04\n",
      "Epoch 356/1000\n",
      "9000/9000 [==============================] - 1s 56us/sample - loss: 5.2718e-04 - val_loss: 1.1185e-04\n",
      "Epoch 357/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2621e-04 - val_loss: 1.1361e-04\n",
      "Epoch 358/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2083e-04 - val_loss: 1.2977e-04\n",
      "Epoch 359/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2197e-04 - val_loss: 1.1593e-04\n",
      "Epoch 360/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2208e-04 - val_loss: 1.3695e-04\n",
      "Epoch 361/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2274e-04 - val_loss: 1.5513e-04\n",
      "Epoch 362/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2069e-04 - val_loss: 1.5490e-04\n",
      "Epoch 363/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2256e-04 - val_loss: 1.3212e-04\n",
      "Epoch 364/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2833e-04 - val_loss: 1.0730e-04\n",
      "Epoch 365/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2284e-04 - val_loss: 1.2696e-04\n",
      "Epoch 366/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2009e-04 - val_loss: 1.4829e-04\n",
      "Epoch 367/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2652e-04 - val_loss: 1.2234e-04\n",
      "Epoch 368/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2225e-04 - val_loss: 1.3945e-04\n",
      "Epoch 369/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2168e-04 - val_loss: 1.1999e-04\n",
      "Epoch 370/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2317e-04 - val_loss: 1.1609e-04\n",
      "Epoch 371/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2123e-04 - val_loss: 1.0843e-04\n",
      "Epoch 372/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2609e-04 - val_loss: 1.2841e-04\n",
      "Epoch 373/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2175e-04 - val_loss: 1.2858e-04\n",
      "Epoch 374/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.2252e-04 - val_loss: 1.5166e-04\n",
      "Epoch 375/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2241e-04 - val_loss: 1.1848e-04\n",
      "Epoch 376/1000\n",
      "9000/9000 [==============================] - 0s 55us/sample - loss: 5.2572e-04 - val_loss: 1.2623e-04\n",
      "Epoch 377/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2501e-04 - val_loss: 1.5518e-04\n",
      "Epoch 378/1000\n",
      "9000/9000 [==============================] - 0s 54us/sample - loss: 5.2114e-04 - val_loss: 1.2361e-04\n",
      "Epoch 379/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.2061e-04 - val_loss: 1.3594e-04\n",
      "Epoch 380/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.1956e-04 - val_loss: 1.5308e-04\n",
      "Epoch 381/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.2591e-04 - val_loss: 1.5288e-04\n",
      "Epoch 382/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2309e-04 - val_loss: 1.2635e-04\n",
      "Epoch 383/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2035e-04 - val_loss: 1.6867e-04\n",
      "Epoch 384/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2461e-04 - val_loss: 1.2672e-04\n",
      "Epoch 385/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2144e-04 - val_loss: 1.2134e-04\n",
      "Epoch 386/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2416e-04 - val_loss: 1.9517e-04\n",
      "Epoch 387/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2053e-04 - val_loss: 1.5694e-04\n",
      "Epoch 388/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2220e-04 - val_loss: 1.3697e-04\n",
      "Epoch 389/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.1984e-04 - val_loss: 2.2448e-04\n",
      "Epoch 390/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2191e-04 - val_loss: 1.2597e-04\n",
      "Epoch 391/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2145e-04 - val_loss: 1.1072e-04\n",
      "Epoch 392/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2387e-04 - val_loss: 1.3360e-04\n",
      "Epoch 393/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2420e-04 - val_loss: 1.7568e-04\n",
      "Epoch 394/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2180e-04 - val_loss: 1.5067e-04\n",
      "Epoch 395/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2159e-04 - val_loss: 1.2377e-04\n",
      "Epoch 396/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2573e-04 - val_loss: 1.4063e-04\n",
      "Epoch 397/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2227e-04 - val_loss: 1.1800e-04\n",
      "Epoch 398/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2370e-04 - val_loss: 1.2790e-04\n",
      "Epoch 399/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2338e-04 - val_loss: 1.8517e-04\n",
      "Epoch 400/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2121e-04 - val_loss: 1.3601e-04\n",
      "Epoch 401/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2142e-04 - val_loss: 1.1423e-04\n",
      "Epoch 402/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2419e-04 - val_loss: 1.3131e-04\n",
      "Epoch 403/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2374e-04 - val_loss: 1.3204e-04\n",
      "Epoch 404/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2494e-04 - val_loss: 1.4489e-04\n",
      "Epoch 405/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2406e-04 - val_loss: 1.2086e-04\n",
      "Epoch 406/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2335e-04 - val_loss: 1.2630e-04\n",
      "Epoch 407/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2227e-04 - val_loss: 1.5377e-04\n",
      "Epoch 408/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2007e-04 - val_loss: 1.3378e-04\n",
      "Epoch 409/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2330e-04 - val_loss: 1.2658e-04\n",
      "Epoch 410/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.1819e-04 - val_loss: 1.2564e-04\n",
      "Epoch 411/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2387e-04 - val_loss: 1.1442e-04\n",
      "Epoch 412/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2206e-04 - val_loss: 1.1817e-04\n",
      "Epoch 413/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2140e-04 - val_loss: 1.3277e-04\n",
      "Epoch 414/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2038e-04 - val_loss: 1.1181e-04\n",
      "Epoch 415/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2168e-04 - val_loss: 1.1496e-04\n",
      "Epoch 416/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.1971e-04 - val_loss: 1.3595e-04\n",
      "Epoch 417/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2079e-04 - val_loss: 1.1898e-04\n",
      "Epoch 418/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2317e-04 - val_loss: 1.3105e-04\n",
      "Epoch 419/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2289e-04 - val_loss: 1.6165e-04\n",
      "Epoch 420/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2248e-04 - val_loss: 1.7713e-04\n",
      "Epoch 421/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.1875e-04 - val_loss: 1.7448e-04\n",
      "Epoch 422/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.1962e-04 - val_loss: 1.3865e-04\n",
      "Epoch 423/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2506e-04 - val_loss: 1.3957e-04\n",
      "Epoch 424/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2588e-04 - val_loss: 1.2048e-04\n",
      "Epoch 425/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.1831e-04 - val_loss: 1.4562e-04\n",
      "Epoch 426/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.1912e-04 - val_loss: 1.3628e-04\n",
      "Epoch 427/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2329e-04 - val_loss: 1.1728e-04\n",
      "Epoch 428/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2211e-04 - val_loss: 1.2550e-04\n",
      "Epoch 429/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2150e-04 - val_loss: 1.3120e-04\n",
      "Epoch 430/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2306e-04 - val_loss: 1.6985e-04\n",
      "Epoch 431/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2095e-04 - val_loss: 1.3015e-04\n",
      "Epoch 432/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2026e-04 - val_loss: 1.0384e-04\n",
      "Epoch 433/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.1756e-04 - val_loss: 1.6932e-04\n",
      "Epoch 434/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2129e-04 - val_loss: 1.1136e-04\n",
      "Epoch 435/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2204e-04 - val_loss: 1.5090e-04\n",
      "Epoch 436/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2616e-04 - val_loss: 1.4589e-04\n",
      "Epoch 437/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2299e-04 - val_loss: 1.2459e-04\n",
      "Epoch 438/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2417e-04 - val_loss: 2.0078e-04\n",
      "Epoch 439/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2053e-04 - val_loss: 1.7079e-04\n",
      "Epoch 440/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.1896e-04 - val_loss: 1.7643e-04\n",
      "Epoch 441/1000\n",
      "9000/9000 [==============================] - 0s 45us/sample - loss: 5.2018e-04 - val_loss: 1.1524e-04\n",
      "Epoch 442/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2231e-04 - val_loss: 1.5904e-04\n",
      "Epoch 443/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.2025e-04 - val_loss: 1.3605e-04\n",
      "Epoch 444/1000\n",
      "9000/9000 [==============================] - 0s 45us/sample - loss: 5.2496e-04 - val_loss: 1.3012e-04\n",
      "Epoch 445/1000\n",
      "9000/9000 [==============================] - 0s 50us/sample - loss: 5.2101e-04 - val_loss: 1.5264e-04\n",
      "Epoch 446/1000\n",
      "9000/9000 [==============================] - 0s 46us/sample - loss: 5.1936e-04 - val_loss: 1.3281e-04\n",
      "Epoch 447/1000\n",
      "9000/9000 [==============================] - 0s 52us/sample - loss: 5.2287e-04 - val_loss: 1.1103e-04\n",
      "Epoch 448/1000\n",
      "9000/9000 [==============================] - 0s 48us/sample - loss: 5.2508e-04 - val_loss: 1.2802e-04\n",
      "Epoch 449/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.1858e-04 - val_loss: 1.5370e-04\n",
      "Epoch 450/1000\n",
      "9000/9000 [==============================] - 0s 47us/sample - loss: 5.2353e-04 - val_loss: 1.2372e-04\n",
      "Epoch 451/1000\n",
      "9000/9000 [==============================] - 0s 49us/sample - loss: 5.2078e-04 - val_loss: 1.2372e-04\n",
      "Epoch 452/1000\n",
      "9000/9000 [==============================] - 0s 53us/sample - loss: 5.2499e-04 - val_loss: 1.4034e-04\n",
      "Epoch 453/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.1922e-04 - val_loss: 1.2518e-04\n",
      "Epoch 454/1000\n",
      "9000/9000 [==============================] - 0s 51us/sample - loss: 5.2163e-04 - val_loss: 1.6649e-04\n",
      "Epoch 455/1000\n",
      "9000/9000 [==============================] - 1s 64us/sample - loss: 5.1916e-04 - val_loss: 1.2824e-04\n",
      "Epoch 456/1000\n",
      "9000/9000 [==============================] - 1s 69us/sample - loss: 5.2143e-04 - val_loss: 1.1224e-04\n",
      "Epoch 457/1000\n",
      "9000/9000 [==============================] - 1s 66us/sample - loss: 5.2216e-04 - val_loss: 1.1431e-04\n",
      "Epoch 458/1000\n",
      "9000/9000 [==============================] - 1s 67us/sample - loss: 5.2184e-04 - val_loss: 1.6997e-04\n",
      "Epoch 459/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2336e-04 - val_loss: 1.8188e-04\n",
      "Epoch 460/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2025e-04 - val_loss: 1.3051e-04\n",
      "Epoch 461/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2129e-04 - val_loss: 1.9456e-04\n",
      "Epoch 462/1000\n",
      "9000/9000 [==============================] - 1s 70us/sample - loss: 5.2423e-04 - val_loss: 1.2054e-04\n",
      "Epoch 463/1000\n",
      "9000/9000 [==============================] - 1s 73us/sample - loss: 5.2127e-04 - val_loss: 1.2658e-04\n",
      "Epoch 464/1000\n",
      "9000/9000 [==============================] - 1s 72us/sample - loss: 5.2346e-04 - val_loss: 1.8677e-04\n",
      "Epoch 465/1000\n",
      "9000/9000 [==============================] - 1s 70us/sample - loss: 5.2249e-04 - val_loss: 1.0873e-04\n",
      "Epoch 466/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2594e-04 - val_loss: 1.0303e-04\n",
      "Epoch 467/1000\n",
      "9000/9000 [==============================] - 1s 71us/sample - loss: 5.2306e-04 - val_loss: 1.1308e-04\n",
      "Epoch 468/1000\n",
      "9000/9000 [==============================] - 1s 72us/sample - loss: 5.2033e-04 - val_loss: 1.6085e-04\n",
      "Epoch 469/1000\n",
      "9000/9000 [==============================] - 1s 69us/sample - loss: 5.2117e-04 - val_loss: 1.2746e-04\n",
      "Epoch 470/1000\n",
      "9000/9000 [==============================] - 1s 69us/sample - loss: 5.2250e-04 - val_loss: 1.1616e-04\n",
      "Epoch 471/1000\n",
      "9000/9000 [==============================] - 1s 65us/sample - loss: 5.1816e-04 - val_loss: 1.5924e-04\n",
      "Epoch 472/1000\n",
      "9000/9000 [==============================] - 1s 67us/sample - loss: 5.2106e-04 - val_loss: 1.5933e-04\n",
      "Epoch 473/1000\n",
      "9000/9000 [==============================] - 1s 65us/sample - loss: 5.2122e-04 - val_loss: 1.4143e-04\n",
      "Epoch 474/1000\n",
      "9000/9000 [==============================] - 1s 63us/sample - loss: 5.2642e-04 - val_loss: 1.1862e-04\n",
      "Epoch 475/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2031e-04 - val_loss: 1.1569e-04\n",
      "Epoch 476/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2090e-04 - val_loss: 1.3726e-04\n",
      "Epoch 477/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2343e-04 - val_loss: 1.3008e-04\n",
      "Epoch 478/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.1945e-04 - val_loss: 1.5135e-04\n",
      "Epoch 479/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2466e-04 - val_loss: 1.1358e-04\n",
      "Epoch 480/1000\n",
      "9000/9000 [==============================] - 1s 59us/sample - loss: 5.2193e-04 - val_loss: 1.1005e-04\n",
      "Epoch 481/1000\n",
      "9000/9000 [==============================] - 1s 62us/sample - loss: 5.2251e-04 - val_loss: 1.3081e-04\n",
      "Epoch 482/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.1901e-04 - val_loss: 1.2947e-04\n",
      "Epoch 483/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.1996e-04 - val_loss: 1.3674e-04\n",
      "Epoch 484/1000\n",
      "9000/9000 [==============================] - 1s 61us/sample - loss: 5.2249e-04 - val_loss: 1.3352e-04\n",
      "Epoch 485/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2012e-04 - val_loss: 1.4781e-04\n",
      "Epoch 486/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.1984e-04 - val_loss: 1.3092e-04\n",
      "Epoch 487/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2016e-04 - val_loss: 1.7098e-04\n",
      "Epoch 488/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2243e-04 - val_loss: 1.1448e-04\n",
      "Epoch 489/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2363e-04 - val_loss: 1.2983e-04\n",
      "Epoch 490/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.1947e-04 - val_loss: 1.1368e-04\n",
      "Epoch 491/1000\n",
      "9000/9000 [==============================] - 1s 60us/sample - loss: 5.2100e-04 - val_loss: 1.6213e-04\n",
      "Epoch 492/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2114e-04 - val_loss: 1.1514e-04\n",
      "Epoch 493/1000\n",
      "9000/9000 [==============================] - 1s 57us/sample - loss: 5.1842e-04 - val_loss: 1.3200e-04\n",
      "Epoch 494/1000\n",
      "9000/9000 [==============================] - 1s 58us/sample - loss: 5.2205e-04 - val_loss: 1.1562e-04\n"
     ]
    }
   ],
   "source": [
    "history = mlp.fit(X_train_enc_, y_train, epochs=1000, callbacks=[early_stp], validation_data=[X_valid_enc_, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 44us/sample - loss: 9.1643e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.164288057945668e-05"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss value with no activation function in the output layer\n",
    "mlp.evaluate(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 44us/sample - loss: 1.0162e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00010161751531995834"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss value with tanh activation function and MCdropout in the output layer\n",
    "mlp.evaluate(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 44us/sample - loss: 1.3626e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001362584630260244"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss value with tanh activation function and MCdropout in every layer\n",
    "mlp.evaluate(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 41us/sample - loss: 1.5111e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00015111107873963192"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss value with tanh activation function and MCdropout in every layer\n",
    "mlp.evaluate(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 34us/sample - loss: 9.7823e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.782341483514756e-05"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss value with tanh in in output layer and no dropout, 5 layers\n",
    "mlp.evaluate(X_valid_enc_, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.predict(X_test_enc_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15158f908>]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_test)\n",
    "plt.plot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_change = pd.concat([pd.DataFrame(y_test.reshape((-1, 1))), \n",
    "                        pd.DataFrame(y_pred.reshape((-1, 1)))], axis=1)\n",
    "\n",
    "# Calculate the pct change    \n",
    "comparison_change.columns = [\"True\", \"Pred\"]\n",
    "\n",
    "# Tranform regression predictions to classification\n",
    "test = []\n",
    "for a in comparison_change[\"True\"]:\n",
    "    if a >= 0:\n",
    "        test.append(1)\n",
    "    else:\n",
    "        test.append(0)\n",
    "\n",
    "pred = []\n",
    "for a in comparison_change[\"Pred\"]:\n",
    "    if a >= 0:\n",
    "        pred.append(1)\n",
    "    else:\n",
    "        pred.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.18      0.28       133\n",
      "           1       0.58      0.90      0.70       167\n",
      "\n",
      "    accuracy                           0.58       300\n",
      "   macro avg       0.58      0.54      0.49       300\n",
      "weighted avg       0.58      0.58      0.51       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification Report\n",
    "print(classification_report(test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4-tf'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
